{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-06T01:38:00.641409Z",
     "start_time": "2022-05-06T01:37:52.107283Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOAD DURATION:  0:00:08.507525\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "start_time = datetime.now()\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import lightgbm as lgb\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "from catboost import CatBoostClassifier\n",
    "import xgboost as xgboost\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "print('LOAD DURATION: ', datetime.now() - start_time) # load time about 30 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-02T02:49:01.554334Z",
     "start_time": "2022-05-02T02:49:01.542637Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-02T02:49:18.301919Z",
     "start_time": "2022-05-02T02:49:01.559381Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.6 s, sys: 1.05 s, total: 15.7 s\n",
      "Wall time: 16.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "usecols_skb= ['Cardnum_total_0', 'Cardnum_total_1','Cardnum_total_3','Cardnum_total_7','Cardnum_total_14', \n",
    "              'card_state_total_1', 'card_state_total_3', 'card_state_total_7', 'card_state_total_14', \n",
    "              'card_zip_total_1','card_zip_total_3', 'card_zip_total_7', 'card_zip_total_14', 'card_zip_total_30', \n",
    "              'card_merch_total_1', 'card_merch_total_3', 'card_merch_total_7', 'card_merch_total_14', \n",
    "              'card_merch_total_30', 'merch_state_total_1', 'Recnum','Fraud']\n",
    "\n",
    "usecols_skb_lasso= ['Recnum', 'Fraud', 'card_state_total_3', 'card_zip_total_3', 'card_merch_total_3', \n",
    "                    'card_merch_total_7', 'card_merch_total_14']\n",
    "\n",
    "usecols_stepforward= ['card_zip_total_3','card_state_max_30','card_merch_total_14','card_merch_total_7','card_state_total_30',\n",
    "             'card_merch_avg_14','merch_state_avg_1','merch_zip_avg_0','Merchnum_avg_0','merch_zip_avg_1',\n",
    "             'Merchnum_avg_1','card_zip_avg_14','card_merch_avg_7','merch_state_avg_0','Cardnum_total_1',\n",
    "             'card_merch_total_3','card_merch_avg_3','card_merch_total_0','card_state_total_0','card_zip_total_0',\n",
    "             'Recnum','Fraud']\n",
    "\n",
    "usecols_stepforward_lasso= ['Recnum','Fraud', 'card_merch_total_7']\n",
    "\n",
    "usecols_backward= ['card_merch_total_7', 'card_zip_total_3', 'card_merch_total_14', 'card_zip_total_14', \n",
    "                   'card_state_total_7', 'card_state_total_14', 'card_state_total_1', 'card_zip_max_30', \n",
    "                   'card_state_total_30', 'card_state_max_14', 'card_zip_max_1', 'merch_state_total_3', \n",
    "                   'merch_state_total_1', 'Cardnum_total_3', 'Cardnum_total_7', 'card_state_max_30', \n",
    "                   'card_merch_avg_30', 'merch_state_max_1', 'Cardnum_total_1', 'merch_zip_total_0',\n",
    "                   'Recnum','Fraud']\n",
    "\n",
    "usecols_backward_lasso= ['Recnum', 'Cardnum_total_7', 'card_state_total_7', 'card_zip_max_1', \n",
    "                         'card_zip_max_30', 'merch_zip_total_0','Fraud']\n",
    "#Variable Created\n",
    "Day of Week target encoding: dow_risk\n",
    "State target encoding: state_risk\n",
    "2\n",
    "New Entities for Variables:\n",
    "{'card_state', 'card_zip', 'card_amount', 'merch_amount', 'card_merch', 'merch_zip', 'merch_state'}\n",
    "7\n",
    "Days-since: # days since a transaction with that entity has been seen.\n",
    "Entities are {'Cardnum', 'Merchnum', 'card_state', 'card_zip', 'card_amount', 'merch_amount', 'card_merch', 'merch_zip', 'merch_state'}\n",
    "9\n",
    "Frequency: # transactions at that entity over the past n days.\n",
    "Entities are {'Cardnum', 'Merchnum', 'card_state', 'card_zip', 'card_amount', 'merch_amount', 'card_merch', 'merch_zip', 'merch_state', ('Cardnum','Merchnum'), ('Merchnum','Cardnum'), ('Amount','Cardnum'), ('Amount','Merchnum')}, n is {0, 1, 3, 7, 14, 30}\n",
    "78\n",
    "Amount: {Average, Maximum, Median, Total, Actual/average, Actual/maximum, Actual/median, Actual/total} amount at that entity over the past n days.\n",
    "Entities are {'Cardnum',  'Merchnum', 'card_state', 'card_zip', 'card_merch', 'merch_zip', 'merch_state'}, n is {0, 1, 3, 7, 14, 30}\n",
    "336\n",
    "Relative velocity for number: # transactions at that entity seen in a short time window {0 or past 1 day} compared to # transactions at that entity over a longer time window {past 3, 7, 14, 30 days}\n",
    "Entities are {'Cardnum',  'Merchnum', 'card_state', 'card_zip', 'card_merch', 'merch_zip', 'merch_state'}\n",
    "72\n",
    "Relative velocity for amount: Amount of transactions at that {'Cardnum',  'Merchnum'} in a short time window {0 or past 1 day} compared to amount of transactions at that {'Cardnum',  'Merchnum'} over a longer time window {past 3, 7, 14, 30 days}\n",
    "16\n",
    "Benford’s Law: Use the two entities Cardnum and Merchnum to calculate the unusualness, U*\n",
    "2\n",
    "Total\n",
    "522\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "vars_skb = pd.read_csv('vars.csv', usecols = usecols_skb)\n",
    "vars_skb_lasso = pd.read_csv('vars.csv', usecols = usecols_skb_lasso)\n",
    "vars_stepforward = pd.read_csv('vars.csv', usecols = usecols_stepforward)\n",
    "vars_stepforward_lasso = pd.read_csv('vars.csv', usecols = usecols_stepforward_lasso)\n",
    "vars_backward = pd.read_csv('vars.csv', usecols = usecols_backward)\n",
    "vars_backward_lasso = pd.read_csv('vars.csv', usecols = usecols_backward_lasso)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T05:04:45.517890Z",
     "start_time": "2022-05-03T05:04:45.270534Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Recnum</th>\n",
       "      <th>Fraud</th>\n",
       "      <th>Cardnum_total_0</th>\n",
       "      <th>Cardnum_total_1</th>\n",
       "      <th>Cardnum_total_3</th>\n",
       "      <th>Cardnum_total_7</th>\n",
       "      <th>Cardnum_total_14</th>\n",
       "      <th>card_state_total_1</th>\n",
       "      <th>card_state_total_3</th>\n",
       "      <th>card_state_total_7</th>\n",
       "      <th>...</th>\n",
       "      <th>card_zip_total_3</th>\n",
       "      <th>card_zip_total_7</th>\n",
       "      <th>card_zip_total_14</th>\n",
       "      <th>card_zip_total_30</th>\n",
       "      <th>card_merch_total_1</th>\n",
       "      <th>card_merch_total_3</th>\n",
       "      <th>card_merch_total_7</th>\n",
       "      <th>card_merch_total_14</th>\n",
       "      <th>card_merch_total_30</th>\n",
       "      <th>merch_state_total_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>...</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>31.42</td>\n",
       "      <td>31.42</td>\n",
       "      <td>31.42</td>\n",
       "      <td>31.42</td>\n",
       "      <td>31.42</td>\n",
       "      <td>31.42</td>\n",
       "      <td>31.42</td>\n",
       "      <td>31.42</td>\n",
       "      <td>...</td>\n",
       "      <td>31.42</td>\n",
       "      <td>31.42</td>\n",
       "      <td>31.42</td>\n",
       "      <td>31.42</td>\n",
       "      <td>31.42</td>\n",
       "      <td>31.42</td>\n",
       "      <td>31.42</td>\n",
       "      <td>31.42</td>\n",
       "      <td>31.42</td>\n",
       "      <td>31.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>178.49</td>\n",
       "      <td>178.49</td>\n",
       "      <td>178.49</td>\n",
       "      <td>178.49</td>\n",
       "      <td>178.49</td>\n",
       "      <td>178.49</td>\n",
       "      <td>178.49</td>\n",
       "      <td>178.49</td>\n",
       "      <td>...</td>\n",
       "      <td>178.49</td>\n",
       "      <td>178.49</td>\n",
       "      <td>178.49</td>\n",
       "      <td>178.49</td>\n",
       "      <td>178.49</td>\n",
       "      <td>178.49</td>\n",
       "      <td>178.49</td>\n",
       "      <td>178.49</td>\n",
       "      <td>178.49</td>\n",
       "      <td>178.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>...</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>7.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>7.24</td>\n",
       "      <td>7.24</td>\n",
       "      <td>7.24</td>\n",
       "      <td>7.24</td>\n",
       "      <td>7.24</td>\n",
       "      <td>7.24</td>\n",
       "      <td>7.24</td>\n",
       "      <td>7.24</td>\n",
       "      <td>...</td>\n",
       "      <td>7.24</td>\n",
       "      <td>7.24</td>\n",
       "      <td>7.24</td>\n",
       "      <td>7.24</td>\n",
       "      <td>7.24</td>\n",
       "      <td>7.24</td>\n",
       "      <td>7.24</td>\n",
       "      <td>7.24</td>\n",
       "      <td>7.24</td>\n",
       "      <td>10.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3.67</td>\n",
       "      <td>3.67</td>\n",
       "      <td>3.67</td>\n",
       "      <td>3.67</td>\n",
       "      <td>3.67</td>\n",
       "      <td>3.67</td>\n",
       "      <td>3.67</td>\n",
       "      <td>3.67</td>\n",
       "      <td>...</td>\n",
       "      <td>3.67</td>\n",
       "      <td>3.67</td>\n",
       "      <td>3.67</td>\n",
       "      <td>3.67</td>\n",
       "      <td>3.67</td>\n",
       "      <td>3.67</td>\n",
       "      <td>3.67</td>\n",
       "      <td>3.67</td>\n",
       "      <td>3.67</td>\n",
       "      <td>14.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>...</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>18.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>230.32</td>\n",
       "      <td>230.32</td>\n",
       "      <td>230.32</td>\n",
       "      <td>230.32</td>\n",
       "      <td>230.32</td>\n",
       "      <td>230.32</td>\n",
       "      <td>230.32</td>\n",
       "      <td>230.32</td>\n",
       "      <td>...</td>\n",
       "      <td>230.32</td>\n",
       "      <td>230.32</td>\n",
       "      <td>230.32</td>\n",
       "      <td>230.32</td>\n",
       "      <td>230.32</td>\n",
       "      <td>230.32</td>\n",
       "      <td>230.32</td>\n",
       "      <td>230.32</td>\n",
       "      <td>230.32</td>\n",
       "      <td>230.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>62.11</td>\n",
       "      <td>62.11</td>\n",
       "      <td>62.11</td>\n",
       "      <td>62.11</td>\n",
       "      <td>62.11</td>\n",
       "      <td>62.11</td>\n",
       "      <td>62.11</td>\n",
       "      <td>62.11</td>\n",
       "      <td>...</td>\n",
       "      <td>62.11</td>\n",
       "      <td>62.11</td>\n",
       "      <td>62.11</td>\n",
       "      <td>62.11</td>\n",
       "      <td>62.11</td>\n",
       "      <td>62.11</td>\n",
       "      <td>62.11</td>\n",
       "      <td>62.11</td>\n",
       "      <td>62.11</td>\n",
       "      <td>62.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10.86</td>\n",
       "      <td>10.86</td>\n",
       "      <td>10.86</td>\n",
       "      <td>10.86</td>\n",
       "      <td>10.86</td>\n",
       "      <td>10.86</td>\n",
       "      <td>10.86</td>\n",
       "      <td>10.86</td>\n",
       "      <td>...</td>\n",
       "      <td>10.86</td>\n",
       "      <td>10.86</td>\n",
       "      <td>10.86</td>\n",
       "      <td>10.86</td>\n",
       "      <td>10.86</td>\n",
       "      <td>10.86</td>\n",
       "      <td>10.86</td>\n",
       "      <td>10.86</td>\n",
       "      <td>10.86</td>\n",
       "      <td>21.77</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Recnum  Fraud  Cardnum_total_0  Cardnum_total_1  Cardnum_total_3  \\\n",
       "0       1      0             3.62             3.62             3.62   \n",
       "1       2      0            31.42            31.42            31.42   \n",
       "2       3      0           178.49           178.49           178.49   \n",
       "3       4      0             3.62             3.62             3.62   \n",
       "4       5      0             7.24             7.24             7.24   \n",
       "5       6      0             3.67             3.67             3.67   \n",
       "6       7      0             3.62             3.62             3.62   \n",
       "7       8      0           230.32           230.32           230.32   \n",
       "8       9      0            62.11            62.11            62.11   \n",
       "9      10      0            10.86            10.86            10.86   \n",
       "\n",
       "   Cardnum_total_7  Cardnum_total_14  card_state_total_1  card_state_total_3  \\\n",
       "0             3.62              3.62                3.62                3.62   \n",
       "1            31.42             31.42               31.42               31.42   \n",
       "2           178.49            178.49              178.49              178.49   \n",
       "3             3.62              3.62                3.62                3.62   \n",
       "4             7.24              7.24                7.24                7.24   \n",
       "5             3.67              3.67                3.67                3.67   \n",
       "6             3.62              3.62                3.62                3.62   \n",
       "7           230.32            230.32              230.32              230.32   \n",
       "8            62.11             62.11               62.11               62.11   \n",
       "9            10.86             10.86               10.86               10.86   \n",
       "\n",
       "   card_state_total_7  ...  card_zip_total_3  card_zip_total_7  \\\n",
       "0                3.62  ...              3.62              3.62   \n",
       "1               31.42  ...             31.42             31.42   \n",
       "2              178.49  ...            178.49            178.49   \n",
       "3                3.62  ...              3.62              3.62   \n",
       "4                7.24  ...              7.24              7.24   \n",
       "5                3.67  ...              3.67              3.67   \n",
       "6                3.62  ...              3.62              3.62   \n",
       "7              230.32  ...            230.32            230.32   \n",
       "8               62.11  ...             62.11             62.11   \n",
       "9               10.86  ...             10.86             10.86   \n",
       "\n",
       "   card_zip_total_14  card_zip_total_30  card_merch_total_1  \\\n",
       "0               3.62               3.62                3.62   \n",
       "1              31.42              31.42               31.42   \n",
       "2             178.49             178.49              178.49   \n",
       "3               3.62               3.62                3.62   \n",
       "4               7.24               7.24                7.24   \n",
       "5               3.67               3.67                3.67   \n",
       "6               3.62               3.62                3.62   \n",
       "7             230.32             230.32              230.32   \n",
       "8              62.11              62.11               62.11   \n",
       "9              10.86              10.86               10.86   \n",
       "\n",
       "   card_merch_total_3  card_merch_total_7  card_merch_total_14  \\\n",
       "0                3.62                3.62                 3.62   \n",
       "1               31.42               31.42                31.42   \n",
       "2              178.49              178.49               178.49   \n",
       "3                3.62                3.62                 3.62   \n",
       "4                7.24                7.24                 7.24   \n",
       "5                3.67                3.67                 3.67   \n",
       "6                3.62                3.62                 3.62   \n",
       "7              230.32              230.32               230.32   \n",
       "8               62.11               62.11                62.11   \n",
       "9               10.86               10.86                10.86   \n",
       "\n",
       "   card_merch_total_30  merch_state_total_1  \n",
       "0                 3.62                 3.62  \n",
       "1                31.42                31.42  \n",
       "2               178.49               178.49  \n",
       "3                 3.62                 7.24  \n",
       "4                 7.24                10.86  \n",
       "5                 3.67                14.53  \n",
       "6                 3.62                18.15  \n",
       "7               230.32               230.32  \n",
       "8                62.11                62.11  \n",
       "9                10.86                21.77  \n",
       "\n",
       "[10 rows x 22 columns]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars = vars_backward\n",
    "vars.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T05:04:46.199088Z",
     "start_time": "2022-05-03T05:04:46.173939Z"
    }
   },
   "outputs": [],
   "source": [
    "# use this to cap variables. For some problems it helps\n",
    "Clip = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T05:04:46.687043Z",
     "start_time": "2022-05-03T05:04:46.655774Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1059"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars['Fraud'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T05:04:47.033234Z",
     "start_time": "2022-05-03T05:04:47.023590Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96397, 22)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T05:04:48.003802Z",
     "start_time": "2022-05-03T05:04:47.334716Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Recnum</th>\n",
       "      <th>Fraud</th>\n",
       "      <th>Cardnum_total_0</th>\n",
       "      <th>Cardnum_total_1</th>\n",
       "      <th>Cardnum_total_3</th>\n",
       "      <th>Cardnum_total_7</th>\n",
       "      <th>Cardnum_total_14</th>\n",
       "      <th>card_state_total_1</th>\n",
       "      <th>card_state_total_3</th>\n",
       "      <th>card_state_total_7</th>\n",
       "      <th>...</th>\n",
       "      <th>card_zip_total_3</th>\n",
       "      <th>card_zip_total_7</th>\n",
       "      <th>card_zip_total_14</th>\n",
       "      <th>card_zip_total_30</th>\n",
       "      <th>card_merch_total_1</th>\n",
       "      <th>card_merch_total_3</th>\n",
       "      <th>card_merch_total_7</th>\n",
       "      <th>card_merch_total_14</th>\n",
       "      <th>card_merch_total_30</th>\n",
       "      <th>merch_state_total_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>96397.000000</td>\n",
       "      <td>96397.000000</td>\n",
       "      <td>96397.000000</td>\n",
       "      <td>96397.000000</td>\n",
       "      <td>96397.000000</td>\n",
       "      <td>96397.000000</td>\n",
       "      <td>96397.000000</td>\n",
       "      <td>96397.000000</td>\n",
       "      <td>96397.000000</td>\n",
       "      <td>96397.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>96397.000000</td>\n",
       "      <td>96397.000000</td>\n",
       "      <td>96397.000000</td>\n",
       "      <td>96397.000000</td>\n",
       "      <td>96397.000000</td>\n",
       "      <td>96397.000000</td>\n",
       "      <td>96397.000000</td>\n",
       "      <td>96397.000000</td>\n",
       "      <td>96397.000000</td>\n",
       "      <td>96397.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>48365.481820</td>\n",
       "      <td>0.010986</td>\n",
       "      <td>741.645565</td>\n",
       "      <td>1110.045587</td>\n",
       "      <td>1512.932952</td>\n",
       "      <td>2384.036098</td>\n",
       "      <td>3768.183808</td>\n",
       "      <td>658.801075</td>\n",
       "      <td>737.869736</td>\n",
       "      <td>902.263993</td>\n",
       "      <td>...</td>\n",
       "      <td>641.696503</td>\n",
       "      <td>710.502552</td>\n",
       "      <td>805.398964</td>\n",
       "      <td>989.355309</td>\n",
       "      <td>599.824086</td>\n",
       "      <td>631.592947</td>\n",
       "      <td>691.248912</td>\n",
       "      <td>773.478689</td>\n",
       "      <td>930.102629</td>\n",
       "      <td>1124.903699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>27945.003883</td>\n",
       "      <td>0.104236</td>\n",
       "      <td>3431.446131</td>\n",
       "      <td>5669.434127</td>\n",
       "      <td>6115.505329</td>\n",
       "      <td>7158.500841</td>\n",
       "      <td>9421.917379</td>\n",
       "      <td>4054.415369</td>\n",
       "      <td>4120.864187</td>\n",
       "      <td>4246.471258</td>\n",
       "      <td>...</td>\n",
       "      <td>4066.606386</td>\n",
       "      <td>4112.186652</td>\n",
       "      <td>4186.589280</td>\n",
       "      <td>4345.734883</td>\n",
       "      <td>4020.452418</td>\n",
       "      <td>4063.110709</td>\n",
       "      <td>4104.238743</td>\n",
       "      <td>4170.834510</td>\n",
       "      <td>4306.518987</td>\n",
       "      <td>4362.985595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>24154.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>292.400000</td>\n",
       "      <td>556.340000</td>\n",
       "      <td>55.440000</td>\n",
       "      <td>65.480000</td>\n",
       "      <td>87.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>58.290000</td>\n",
       "      <td>70.400000</td>\n",
       "      <td>84.900000</td>\n",
       "      <td>105.240000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>55.250000</td>\n",
       "      <td>65.710000</td>\n",
       "      <td>77.350000</td>\n",
       "      <td>94.290000</td>\n",
       "      <td>118.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>48365.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>220.000000</td>\n",
       "      <td>339.160000</td>\n",
       "      <td>518.610000</td>\n",
       "      <td>986.140000</td>\n",
       "      <td>1723.640000</td>\n",
       "      <td>189.980000</td>\n",
       "      <td>216.500000</td>\n",
       "      <td>280.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>185.520000</td>\n",
       "      <td>215.000000</td>\n",
       "      <td>256.450000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>179.950000</td>\n",
       "      <td>203.470000</td>\n",
       "      <td>238.780000</td>\n",
       "      <td>291.750000</td>\n",
       "      <td>395.560000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>72578.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>685.650000</td>\n",
       "      <td>1043.110000</td>\n",
       "      <td>1550.000000</td>\n",
       "      <td>2566.000000</td>\n",
       "      <td>4093.250000</td>\n",
       "      <td>577.200000</td>\n",
       "      <td>659.000000</td>\n",
       "      <td>837.440000</td>\n",
       "      <td>...</td>\n",
       "      <td>556.430000</td>\n",
       "      <td>621.980000</td>\n",
       "      <td>716.560000</td>\n",
       "      <td>902.190000</td>\n",
       "      <td>512.550000</td>\n",
       "      <td>545.010000</td>\n",
       "      <td>600.000000</td>\n",
       "      <td>681.930000</td>\n",
       "      <td>834.640000</td>\n",
       "      <td>1135.840000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>96753.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>218301.830000</td>\n",
       "      <td>307468.060000</td>\n",
       "      <td>310843.060000</td>\n",
       "      <td>312616.060000</td>\n",
       "      <td>313995.060000</td>\n",
       "      <td>306633.410000</td>\n",
       "      <td>306633.410000</td>\n",
       "      <td>306633.410000</td>\n",
       "      <td>...</td>\n",
       "      <td>306633.410000</td>\n",
       "      <td>306633.410000</td>\n",
       "      <td>306633.410000</td>\n",
       "      <td>306633.410000</td>\n",
       "      <td>306633.410000</td>\n",
       "      <td>306633.410000</td>\n",
       "      <td>306633.410000</td>\n",
       "      <td>306633.410000</td>\n",
       "      <td>306633.410000</td>\n",
       "      <td>306633.410000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Recnum         Fraud  Cardnum_total_0  Cardnum_total_1  \\\n",
       "count  96397.000000  96397.000000     96397.000000     96397.000000   \n",
       "mean   48365.481820      0.010986       741.645565      1110.045587   \n",
       "std    27945.003883      0.104236      3431.446131      5669.434127   \n",
       "min        1.000000      0.000000         0.010000         0.010000   \n",
       "25%    24154.000000      0.000000        60.000000        90.000000   \n",
       "50%    48365.000000      0.000000       220.000000       339.160000   \n",
       "75%    72578.000000      0.000000       685.650000      1043.110000   \n",
       "max    96753.000000      1.000000    218301.830000    307468.060000   \n",
       "\n",
       "       Cardnum_total_3  Cardnum_total_7  Cardnum_total_14  card_state_total_1  \\\n",
       "count     96397.000000     96397.000000      96397.000000        96397.000000   \n",
       "mean       1512.932952      2384.036098       3768.183808          658.801075   \n",
       "std        6115.505329      7158.500841       9421.917379         4054.415369   \n",
       "min           0.010000         0.140000          0.140000            0.010000   \n",
       "25%         140.000000       292.400000        556.340000           55.440000   \n",
       "50%         518.610000       986.140000       1723.640000          189.980000   \n",
       "75%        1550.000000      2566.000000       4093.250000          577.200000   \n",
       "max      310843.060000    312616.060000     313995.060000       306633.410000   \n",
       "\n",
       "       card_state_total_3  card_state_total_7  ...  card_zip_total_3  \\\n",
       "count        96397.000000        96397.000000  ...      96397.000000   \n",
       "mean           737.869736          902.263993  ...        641.696503   \n",
       "std           4120.864187         4246.471258  ...       4066.606386   \n",
       "min              0.010000            0.010000  ...          0.010000   \n",
       "25%             65.480000           87.500000  ...         58.290000   \n",
       "50%            216.500000          280.000000  ...        185.520000   \n",
       "75%            659.000000          837.440000  ...        556.430000   \n",
       "max         306633.410000       306633.410000  ...     306633.410000   \n",
       "\n",
       "       card_zip_total_7  card_zip_total_14  card_zip_total_30  \\\n",
       "count      96397.000000       96397.000000       96397.000000   \n",
       "mean         710.502552         805.398964         989.355309   \n",
       "std         4112.186652        4186.589280        4345.734883   \n",
       "min            0.010000           0.010000           0.010000   \n",
       "25%           70.400000          84.900000         105.240000   \n",
       "50%          215.000000         256.450000         321.000000   \n",
       "75%          621.980000         716.560000         902.190000   \n",
       "max       306633.410000      306633.410000      306633.410000   \n",
       "\n",
       "       card_merch_total_1  card_merch_total_3  card_merch_total_7  \\\n",
       "count        96397.000000        96397.000000        96397.000000   \n",
       "mean           599.824086          631.592947          691.248912   \n",
       "std           4020.452418         4063.110709         4104.238743   \n",
       "min              0.010000            0.010000            0.010000   \n",
       "25%             50.000000           55.250000           65.710000   \n",
       "50%            168.000000          179.950000          203.470000   \n",
       "75%            512.550000          545.010000          600.000000   \n",
       "max         306633.410000       306633.410000       306633.410000   \n",
       "\n",
       "       card_merch_total_14  card_merch_total_30  merch_state_total_1  \n",
       "count         96397.000000         96397.000000         96397.000000  \n",
       "mean            773.478689           930.102629          1124.903699  \n",
       "std            4170.834510          4306.518987          4362.985595  \n",
       "min               0.010000             0.010000             0.010000  \n",
       "25%              77.350000            94.290000           118.000000  \n",
       "50%             238.780000           291.750000           395.560000  \n",
       "75%             681.930000           834.640000          1135.840000  \n",
       "max          306633.410000        306633.410000        306633.410000  \n",
       "\n",
       "[8 rows x 22 columns]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T05:04:48.031102Z",
     "start_time": "2022-05-03T05:04:48.009579Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Fraud\n",
       "0      0\n",
       "1      0\n",
       "2      0\n",
       "3      0\n",
       "4      0"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record_save = vars['Recnum']\n",
    "Y_save = pd.DataFrame(vars.loc[:,'Fraud'])\n",
    "Y_save.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale and truncate field values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T05:04:51.513720Z",
     "start_time": "2022-05-03T05:04:51.265756Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cardnum_total_0</th>\n",
       "      <th>Cardnum_total_1</th>\n",
       "      <th>Cardnum_total_3</th>\n",
       "      <th>Cardnum_total_7</th>\n",
       "      <th>Cardnum_total_14</th>\n",
       "      <th>card_state_total_1</th>\n",
       "      <th>card_state_total_3</th>\n",
       "      <th>card_state_total_7</th>\n",
       "      <th>card_state_total_14</th>\n",
       "      <th>card_zip_total_1</th>\n",
       "      <th>card_zip_total_3</th>\n",
       "      <th>card_zip_total_7</th>\n",
       "      <th>card_zip_total_14</th>\n",
       "      <th>card_zip_total_30</th>\n",
       "      <th>card_merch_total_1</th>\n",
       "      <th>card_merch_total_3</th>\n",
       "      <th>card_merch_total_7</th>\n",
       "      <th>card_merch_total_14</th>\n",
       "      <th>card_merch_total_30</th>\n",
       "      <th>merch_state_total_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>96397.000000</td>\n",
       "      <td>96397.000000</td>\n",
       "      <td>96397.000000</td>\n",
       "      <td>96397.000000</td>\n",
       "      <td>96397.000000</td>\n",
       "      <td>96397.000000</td>\n",
       "      <td>96397.000000</td>\n",
       "      <td>96397.000000</td>\n",
       "      <td>96397.000000</td>\n",
       "      <td>96397.000000</td>\n",
       "      <td>96397.000000</td>\n",
       "      <td>96397.000000</td>\n",
       "      <td>96397.000000</td>\n",
       "      <td>96397.000000</td>\n",
       "      <td>96397.000000</td>\n",
       "      <td>96397.000000</td>\n",
       "      <td>96397.000000</td>\n",
       "      <td>96397.000000</td>\n",
       "      <td>96397.000000</td>\n",
       "      <td>96397.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>741.645565</td>\n",
       "      <td>1110.045587</td>\n",
       "      <td>1512.932952</td>\n",
       "      <td>2384.036098</td>\n",
       "      <td>3768.183808</td>\n",
       "      <td>658.801075</td>\n",
       "      <td>737.869736</td>\n",
       "      <td>902.263993</td>\n",
       "      <td>1150.539713</td>\n",
       "      <td>605.584869</td>\n",
       "      <td>641.696503</td>\n",
       "      <td>710.502552</td>\n",
       "      <td>805.398964</td>\n",
       "      <td>989.355309</td>\n",
       "      <td>599.824086</td>\n",
       "      <td>631.592947</td>\n",
       "      <td>691.248912</td>\n",
       "      <td>773.478689</td>\n",
       "      <td>930.102629</td>\n",
       "      <td>1124.903699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>3431.446131</td>\n",
       "      <td>5669.434127</td>\n",
       "      <td>6115.505329</td>\n",
       "      <td>7158.500841</td>\n",
       "      <td>9421.917379</td>\n",
       "      <td>4054.415369</td>\n",
       "      <td>4120.864187</td>\n",
       "      <td>4246.471258</td>\n",
       "      <td>4507.600609</td>\n",
       "      <td>4022.504312</td>\n",
       "      <td>4066.606386</td>\n",
       "      <td>4112.186652</td>\n",
       "      <td>4186.589280</td>\n",
       "      <td>4345.734883</td>\n",
       "      <td>4020.452418</td>\n",
       "      <td>4063.110709</td>\n",
       "      <td>4104.238743</td>\n",
       "      <td>4170.834510</td>\n",
       "      <td>4306.518987</td>\n",
       "      <td>4362.985595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>292.400000</td>\n",
       "      <td>556.340000</td>\n",
       "      <td>55.440000</td>\n",
       "      <td>65.480000</td>\n",
       "      <td>87.500000</td>\n",
       "      <td>113.560000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>58.290000</td>\n",
       "      <td>70.400000</td>\n",
       "      <td>84.900000</td>\n",
       "      <td>105.240000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>55.250000</td>\n",
       "      <td>65.710000</td>\n",
       "      <td>77.350000</td>\n",
       "      <td>94.290000</td>\n",
       "      <td>118.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>220.000000</td>\n",
       "      <td>339.160000</td>\n",
       "      <td>518.610000</td>\n",
       "      <td>986.140000</td>\n",
       "      <td>1723.640000</td>\n",
       "      <td>189.980000</td>\n",
       "      <td>216.500000</td>\n",
       "      <td>280.000000</td>\n",
       "      <td>365.270000</td>\n",
       "      <td>171.600000</td>\n",
       "      <td>185.520000</td>\n",
       "      <td>215.000000</td>\n",
       "      <td>256.450000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>179.950000</td>\n",
       "      <td>203.470000</td>\n",
       "      <td>238.780000</td>\n",
       "      <td>291.750000</td>\n",
       "      <td>395.560000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>685.650000</td>\n",
       "      <td>1043.110000</td>\n",
       "      <td>1550.000000</td>\n",
       "      <td>2566.000000</td>\n",
       "      <td>4093.250000</td>\n",
       "      <td>577.200000</td>\n",
       "      <td>659.000000</td>\n",
       "      <td>837.440000</td>\n",
       "      <td>1091.360000</td>\n",
       "      <td>520.000000</td>\n",
       "      <td>556.430000</td>\n",
       "      <td>621.980000</td>\n",
       "      <td>716.560000</td>\n",
       "      <td>902.190000</td>\n",
       "      <td>512.550000</td>\n",
       "      <td>545.010000</td>\n",
       "      <td>600.000000</td>\n",
       "      <td>681.930000</td>\n",
       "      <td>834.640000</td>\n",
       "      <td>1135.840000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>218301.830000</td>\n",
       "      <td>307468.060000</td>\n",
       "      <td>310843.060000</td>\n",
       "      <td>312616.060000</td>\n",
       "      <td>313995.060000</td>\n",
       "      <td>306633.410000</td>\n",
       "      <td>306633.410000</td>\n",
       "      <td>306633.410000</td>\n",
       "      <td>306633.410000</td>\n",
       "      <td>306633.410000</td>\n",
       "      <td>306633.410000</td>\n",
       "      <td>306633.410000</td>\n",
       "      <td>306633.410000</td>\n",
       "      <td>306633.410000</td>\n",
       "      <td>306633.410000</td>\n",
       "      <td>306633.410000</td>\n",
       "      <td>306633.410000</td>\n",
       "      <td>306633.410000</td>\n",
       "      <td>306633.410000</td>\n",
       "      <td>306633.410000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Cardnum_total_0  Cardnum_total_1  Cardnum_total_3  Cardnum_total_7  \\\n",
       "count     96397.000000     96397.000000     96397.000000     96397.000000   \n",
       "mean        741.645565      1110.045587      1512.932952      2384.036098   \n",
       "std        3431.446131      5669.434127      6115.505329      7158.500841   \n",
       "min           0.010000         0.010000         0.010000         0.140000   \n",
       "25%          60.000000        90.000000       140.000000       292.400000   \n",
       "50%         220.000000       339.160000       518.610000       986.140000   \n",
       "75%         685.650000      1043.110000      1550.000000      2566.000000   \n",
       "max      218301.830000    307468.060000    310843.060000    312616.060000   \n",
       "\n",
       "       Cardnum_total_14  card_state_total_1  card_state_total_3  \\\n",
       "count      96397.000000        96397.000000        96397.000000   \n",
       "mean        3768.183808          658.801075          737.869736   \n",
       "std         9421.917379         4054.415369         4120.864187   \n",
       "min            0.140000            0.010000            0.010000   \n",
       "25%          556.340000           55.440000           65.480000   \n",
       "50%         1723.640000          189.980000          216.500000   \n",
       "75%         4093.250000          577.200000          659.000000   \n",
       "max       313995.060000       306633.410000       306633.410000   \n",
       "\n",
       "       card_state_total_7  card_state_total_14  card_zip_total_1  \\\n",
       "count        96397.000000         96397.000000      96397.000000   \n",
       "mean           902.263993          1150.539713        605.584869   \n",
       "std           4246.471258          4507.600609       4022.504312   \n",
       "min              0.010000             0.010000          0.010000   \n",
       "25%             87.500000           113.560000         51.000000   \n",
       "50%            280.000000           365.270000        171.600000   \n",
       "75%            837.440000          1091.360000        520.000000   \n",
       "max         306633.410000        306633.410000     306633.410000   \n",
       "\n",
       "       card_zip_total_3  card_zip_total_7  card_zip_total_14  \\\n",
       "count      96397.000000      96397.000000       96397.000000   \n",
       "mean         641.696503        710.502552         805.398964   \n",
       "std         4066.606386       4112.186652        4186.589280   \n",
       "min            0.010000          0.010000           0.010000   \n",
       "25%           58.290000         70.400000          84.900000   \n",
       "50%          185.520000        215.000000         256.450000   \n",
       "75%          556.430000        621.980000         716.560000   \n",
       "max       306633.410000     306633.410000      306633.410000   \n",
       "\n",
       "       card_zip_total_30  card_merch_total_1  card_merch_total_3  \\\n",
       "count       96397.000000        96397.000000        96397.000000   \n",
       "mean          989.355309          599.824086          631.592947   \n",
       "std          4345.734883         4020.452418         4063.110709   \n",
       "min             0.010000            0.010000            0.010000   \n",
       "25%           105.240000           50.000000           55.250000   \n",
       "50%           321.000000          168.000000          179.950000   \n",
       "75%           902.190000          512.550000          545.010000   \n",
       "max        306633.410000       306633.410000       306633.410000   \n",
       "\n",
       "       card_merch_total_7  card_merch_total_14  card_merch_total_30  \\\n",
       "count        96397.000000         96397.000000         96397.000000   \n",
       "mean           691.248912           773.478689           930.102629   \n",
       "std           4104.238743          4170.834510          4306.518987   \n",
       "min              0.010000             0.010000             0.010000   \n",
       "25%             65.710000            77.350000            94.290000   \n",
       "50%            203.470000           238.780000           291.750000   \n",
       "75%            600.000000           681.930000           834.640000   \n",
       "max         306633.410000        306633.410000        306633.410000   \n",
       "\n",
       "       merch_state_total_1  \n",
       "count         96397.000000  \n",
       "mean           1124.903699  \n",
       "std            4362.985595  \n",
       "min               0.010000  \n",
       "25%             118.000000  \n",
       "50%             395.560000  \n",
       "75%            1135.840000  \n",
       "max          306633.410000  "
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_no_scaling = vars.drop(columns = ['Recnum','Fraud'])\n",
    "X_no_scaling.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T05:04:51.710327Z",
     "start_time": "2022-05-03T05:04:51.613271Z"
    }
   },
   "outputs": [],
   "source": [
    "X = (X_no_scaling - X_no_scaling.mean()) / X_no_scaling.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T05:04:52.380777Z",
     "start_time": "2022-05-03T05:04:51.946821Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cardnum_total_0</th>\n",
       "      <th>Cardnum_total_1</th>\n",
       "      <th>Cardnum_total_3</th>\n",
       "      <th>Cardnum_total_7</th>\n",
       "      <th>Cardnum_total_14</th>\n",
       "      <th>card_state_total_1</th>\n",
       "      <th>card_state_total_3</th>\n",
       "      <th>card_state_total_7</th>\n",
       "      <th>card_state_total_14</th>\n",
       "      <th>card_zip_total_1</th>\n",
       "      <th>card_zip_total_3</th>\n",
       "      <th>card_zip_total_7</th>\n",
       "      <th>card_zip_total_14</th>\n",
       "      <th>card_zip_total_30</th>\n",
       "      <th>card_merch_total_1</th>\n",
       "      <th>card_merch_total_3</th>\n",
       "      <th>card_merch_total_7</th>\n",
       "      <th>card_merch_total_14</th>\n",
       "      <th>card_merch_total_30</th>\n",
       "      <th>merch_state_total_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>96397.000000</td>\n",
       "      <td>96397.000000</td>\n",
       "      <td>96397.000000</td>\n",
       "      <td>96397.000000</td>\n",
       "      <td>96397.000000</td>\n",
       "      <td>96397.000000</td>\n",
       "      <td>96397.000000</td>\n",
       "      <td>96397.000000</td>\n",
       "      <td>96397.000000</td>\n",
       "      <td>96397.000000</td>\n",
       "      <td>96397.000000</td>\n",
       "      <td>96397.000000</td>\n",
       "      <td>96397.000000</td>\n",
       "      <td>96397.000000</td>\n",
       "      <td>96397.000000</td>\n",
       "      <td>96397.000000</td>\n",
       "      <td>96397.000000</td>\n",
       "      <td>96397.000000</td>\n",
       "      <td>96397.000000</td>\n",
       "      <td>96397.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>-0.015303</td>\n",
       "      <td>-0.016958</td>\n",
       "      <td>-0.016033</td>\n",
       "      <td>-0.014465</td>\n",
       "      <td>-0.013169</td>\n",
       "      <td>-0.012398</td>\n",
       "      <td>-0.012330</td>\n",
       "      <td>-0.012043</td>\n",
       "      <td>-0.011538</td>\n",
       "      <td>-0.012554</td>\n",
       "      <td>-0.012608</td>\n",
       "      <td>-0.012515</td>\n",
       "      <td>-0.012247</td>\n",
       "      <td>-0.011520</td>\n",
       "      <td>-0.012565</td>\n",
       "      <td>-0.012627</td>\n",
       "      <td>-0.012558</td>\n",
       "      <td>-0.012329</td>\n",
       "      <td>-0.011710</td>\n",
       "      <td>-0.011512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>0.563192</td>\n",
       "      <td>0.541879</td>\n",
       "      <td>0.603512</td>\n",
       "      <td>0.694553</td>\n",
       "      <td>0.771249</td>\n",
       "      <td>0.449155</td>\n",
       "      <td>0.473886</td>\n",
       "      <td>0.517611</td>\n",
       "      <td>0.586084</td>\n",
       "      <td>0.433062</td>\n",
       "      <td>0.448165</td>\n",
       "      <td>0.466324</td>\n",
       "      <td>0.496054</td>\n",
       "      <td>0.553797</td>\n",
       "      <td>0.431967</td>\n",
       "      <td>0.446373</td>\n",
       "      <td>0.462533</td>\n",
       "      <td>0.489352</td>\n",
       "      <td>0.540337</td>\n",
       "      <td>0.558051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>-0.216129</td>\n",
       "      <td>-0.195793</td>\n",
       "      <td>-0.247391</td>\n",
       "      <td>-0.333016</td>\n",
       "      <td>-0.399923</td>\n",
       "      <td>-0.162487</td>\n",
       "      <td>-0.179055</td>\n",
       "      <td>-0.212471</td>\n",
       "      <td>-0.255242</td>\n",
       "      <td>-0.150547</td>\n",
       "      <td>-0.157794</td>\n",
       "      <td>-0.172777</td>\n",
       "      <td>-0.192374</td>\n",
       "      <td>-0.227659</td>\n",
       "      <td>-0.149191</td>\n",
       "      <td>-0.155443</td>\n",
       "      <td>-0.168421</td>\n",
       "      <td>-0.185447</td>\n",
       "      <td>-0.215973</td>\n",
       "      <td>-0.257827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>-0.198647</td>\n",
       "      <td>-0.179920</td>\n",
       "      <td>-0.224500</td>\n",
       "      <td>-0.292189</td>\n",
       "      <td>-0.340891</td>\n",
       "      <td>-0.148816</td>\n",
       "      <td>-0.163167</td>\n",
       "      <td>-0.191868</td>\n",
       "      <td>-0.230051</td>\n",
       "      <td>-0.137871</td>\n",
       "      <td>-0.143463</td>\n",
       "      <td>-0.155660</td>\n",
       "      <td>-0.172097</td>\n",
       "      <td>-0.203444</td>\n",
       "      <td>-0.136757</td>\n",
       "      <td>-0.141848</td>\n",
       "      <td>-0.152413</td>\n",
       "      <td>-0.166904</td>\n",
       "      <td>-0.194081</td>\n",
       "      <td>-0.230783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>-0.152019</td>\n",
       "      <td>-0.135972</td>\n",
       "      <td>-0.162590</td>\n",
       "      <td>-0.195278</td>\n",
       "      <td>-0.216999</td>\n",
       "      <td>-0.115632</td>\n",
       "      <td>-0.126520</td>\n",
       "      <td>-0.146537</td>\n",
       "      <td>-0.174210</td>\n",
       "      <td>-0.107889</td>\n",
       "      <td>-0.112176</td>\n",
       "      <td>-0.120496</td>\n",
       "      <td>-0.131121</td>\n",
       "      <td>-0.153796</td>\n",
       "      <td>-0.107407</td>\n",
       "      <td>-0.111157</td>\n",
       "      <td>-0.118848</td>\n",
       "      <td>-0.128199</td>\n",
       "      <td>-0.148229</td>\n",
       "      <td>-0.167166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>-0.016318</td>\n",
       "      <td>-0.011806</td>\n",
       "      <td>0.006061</td>\n",
       "      <td>0.025419</td>\n",
       "      <td>0.034501</td>\n",
       "      <td>-0.020126</td>\n",
       "      <td>-0.019139</td>\n",
       "      <td>-0.015265</td>\n",
       "      <td>-0.013129</td>\n",
       "      <td>-0.021277</td>\n",
       "      <td>-0.020967</td>\n",
       "      <td>-0.021527</td>\n",
       "      <td>-0.021220</td>\n",
       "      <td>-0.020058</td>\n",
       "      <td>-0.021708</td>\n",
       "      <td>-0.021310</td>\n",
       "      <td>-0.022233</td>\n",
       "      <td>-0.021950</td>\n",
       "      <td>-0.022167</td>\n",
       "      <td>0.002507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Cardnum_total_0  Cardnum_total_1  Cardnum_total_3  Cardnum_total_7  \\\n",
       "count     96397.000000     96397.000000     96397.000000     96397.000000   \n",
       "mean         -0.015303        -0.016958        -0.016033        -0.014465   \n",
       "std           0.563192         0.541879         0.603512         0.694553   \n",
       "min          -0.216129        -0.195793        -0.247391        -0.333016   \n",
       "25%          -0.198647        -0.179920        -0.224500        -0.292189   \n",
       "50%          -0.152019        -0.135972        -0.162590        -0.195278   \n",
       "75%          -0.016318        -0.011806         0.006061         0.025419   \n",
       "max          10.000000        10.000000        10.000000        10.000000   \n",
       "\n",
       "       Cardnum_total_14  card_state_total_1  card_state_total_3  \\\n",
       "count      96397.000000        96397.000000        96397.000000   \n",
       "mean          -0.013169           -0.012398           -0.012330   \n",
       "std            0.771249            0.449155            0.473886   \n",
       "min           -0.399923           -0.162487           -0.179055   \n",
       "25%           -0.340891           -0.148816           -0.163167   \n",
       "50%           -0.216999           -0.115632           -0.126520   \n",
       "75%            0.034501           -0.020126           -0.019139   \n",
       "max           10.000000           10.000000           10.000000   \n",
       "\n",
       "       card_state_total_7  card_state_total_14  card_zip_total_1  \\\n",
       "count        96397.000000         96397.000000      96397.000000   \n",
       "mean            -0.012043            -0.011538         -0.012554   \n",
       "std              0.517611             0.586084          0.433062   \n",
       "min             -0.212471            -0.255242         -0.150547   \n",
       "25%             -0.191868            -0.230051         -0.137871   \n",
       "50%             -0.146537            -0.174210         -0.107889   \n",
       "75%             -0.015265            -0.013129         -0.021277   \n",
       "max             10.000000            10.000000         10.000000   \n",
       "\n",
       "       card_zip_total_3  card_zip_total_7  card_zip_total_14  \\\n",
       "count      96397.000000      96397.000000       96397.000000   \n",
       "mean          -0.012608         -0.012515          -0.012247   \n",
       "std            0.448165          0.466324           0.496054   \n",
       "min           -0.157794         -0.172777          -0.192374   \n",
       "25%           -0.143463         -0.155660          -0.172097   \n",
       "50%           -0.112176         -0.120496          -0.131121   \n",
       "75%           -0.020967         -0.021527          -0.021220   \n",
       "max           10.000000         10.000000          10.000000   \n",
       "\n",
       "       card_zip_total_30  card_merch_total_1  card_merch_total_3  \\\n",
       "count       96397.000000        96397.000000        96397.000000   \n",
       "mean           -0.011520           -0.012565           -0.012627   \n",
       "std             0.553797            0.431967            0.446373   \n",
       "min            -0.227659           -0.149191           -0.155443   \n",
       "25%            -0.203444           -0.136757           -0.141848   \n",
       "50%            -0.153796           -0.107407           -0.111157   \n",
       "75%            -0.020058           -0.021708           -0.021310   \n",
       "max            10.000000           10.000000           10.000000   \n",
       "\n",
       "       card_merch_total_7  card_merch_total_14  card_merch_total_30  \\\n",
       "count        96397.000000         96397.000000         96397.000000   \n",
       "mean            -0.012558            -0.012329            -0.011710   \n",
       "std              0.462533             0.489352             0.540337   \n",
       "min             -0.168421            -0.185447            -0.215973   \n",
       "25%             -0.152413            -0.166904            -0.194081   \n",
       "50%             -0.118848            -0.128199            -0.148229   \n",
       "75%             -0.022233            -0.021950            -0.022167   \n",
       "max             10.000000            10.000000            10.000000   \n",
       "\n",
       "       merch_state_total_1  \n",
       "count         96397.000000  \n",
       "mean             -0.011512  \n",
       "std               0.558051  \n",
       "min              -0.257827  \n",
       "25%              -0.230783  \n",
       "50%              -0.167166  \n",
       "75%               0.002507  \n",
       "max              10.000000  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# push in any outlier values\n",
    "cols = X.columns\n",
    "X.loc[:,cols] = X[cols].clip(upper=Clip)\n",
    "X.loc[:,cols] = X[cols].clip(lower=-1*Clip)\n",
    "\n",
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T05:04:52.485139Z",
     "start_time": "2022-05-03T05:04:52.475562Z"
    }
   },
   "outputs": [],
   "source": [
    "# separate data into modeling (traintest) and out of time\n",
    "oot_recnum=84300\n",
    "X_trntst = X[0:oot_recnum]\n",
    "Y_trntst = Y_save[0:oot_recnum]\n",
    "X_oot = X[oot_recnum:]\n",
    "Y_oot = Y_save[oot_recnum:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T05:04:52.860787Z",
     "start_time": "2022-05-03T05:04:52.840953Z"
    }
   },
   "outputs": [],
   "source": [
    "niter = 0\n",
    "nitermax = 5\n",
    "FDR3 = pd.DataFrame(np.zeros((nitermax,3)), columns=('trn', 'tst', 'oot'))\n",
    "X_oot_orig = X_oot.copy()\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T06:06:52.080229Z",
     "start_time": "2022-04-27T06:06:52.067111Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from sklearn.linear_model import Ridge, RidgeCV, Lasso, LassoCV \n",
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T06:06:52.090999Z",
     "start_time": "2022-04-27T06:06:52.084337Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "alphas = 10**np.linspace(10,-2,100)*0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T06:06:52.120906Z",
     "start_time": "2022-04-27T06:06:52.096738Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_trn, X_tst, Y_trn, Y_tst = train_test_split(X_trntst, Y_trntst, test_size = .3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T06:06:52.544511Z",
     "start_time": "2022-04-27T06:06:52.124030Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 5)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge = Ridge()\n",
    "coefs = []\n",
    "for a in alphas: \n",
    "    ridge.set_params(alpha=a) \n",
    "    ridge.fit(X_trn,Y_trn.values.ravel()) \n",
    "    coefs.append(ridge.coef_) \n",
    "np.shape(coefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T06:06:53.259466Z",
     "start_time": "2022-04-27T06:06:52.552024Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Ridge')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "ax = plt.gca() # Get the current Axes instance\n",
    "ax.plot(alphas, coefs)\n",
    "ax.set_xscale('log')\n",
    "plt.xlabel('alpha') \n",
    "plt.ylabel('standadized coef') \n",
    "plt.title('Ridge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T06:06:54.078839Z",
     "start_time": "2022-04-27T06:06:53.263345Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features: ['Recnum', 'card_merch_total_3']\n"
     ]
    }
   ],
   "source": [
    "lasso = Lasso(max_iter=10000) \n",
    "coefs = [] \n",
    "for a in alphas: \n",
    "    lasso.set_params(alpha=a) \n",
    "    lasso.fit(X_trn,Y_trn.values.ravel()) \n",
    "    coefs.append(lasso.coef_) \n",
    "# print('Shape:',np.shape(coefs)\n",
    "print('Selected Features:', list(vars.columns[np.where(lasso.coef_!=0)[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T06:06:54.567903Z",
     "start_time": "2022-04-27T06:06:54.082717Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fe8049b80d0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "ax = plt.gca() # Get the current Axes instance \n",
    "ax.plot(alphas, coefs)\n",
    "ax.set_xscale('log')\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('standerdized coef') \n",
    "plt.title('Lasso')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T06:06:54.584636Z",
     "start_time": "2022-04-27T06:06:54.575448Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# param_grid = {\n",
    "#     'num_leaves': [31, 127],\n",
    "#     'reg_alpha': [0.1, 0.5],\n",
    "#     'min_data_in_leaf': [30, 50, 100, 300, 400],\n",
    "#     'lambda_l1': [0, 1, 1.5],\n",
    "#     'lambda_l2': [0, 1]\n",
    "#     }\n",
    "\n",
    "# bt = lgb.LGBMClassifier()\n",
    "# grid_search = GridSearchCV(bt, param_grid,cv=5,\n",
    "#                           scoring='neg_mean_squared_error')\n",
    " \n",
    "# grid_search.fit(trn.iloc[:, 1:5],trn.iloc[:, 5])\n",
    "# grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T06:06:57.123416Z",
     "start_time": "2022-04-27T06:06:54.597965Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.6367924528301887 0.6147540983606558 0.33519553072625696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.6489533011272142 0.5907335907335908 0.3743016759776536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 0.6272 0.6431372549019608 0.35195530726256985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 0.6333333333333333 0.636 0.30726256983240224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 0.6345177664974619 0.6055363321799307 0.36312849162011174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 0.6272577996715928 0.6494464944649446 0.30726256983240224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 0.6432078559738135 0.6171003717472119 0.31843575418994413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 0.6395348837209303 0.6187050359712231 0.3240223463687151\n",
      "8 0.6111111111111112 0.6529850746268657 0.4134078212290503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 0.6103059581320451 0.6640926640926641 0.329608938547486\n",
      "trn    0.631221\n",
      "tst    0.629249\n",
      "oot    0.342458\n",
      "dtype: float64\n",
      "CPU times: user 2.97 s, sys: 198 ms, total: 3.17 s\n",
      "Wall time: 2.51 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Logistic regression\n",
    "\n",
    "for niter in range(nitermax):    \n",
    "    X_trn, X_tst, Y_trn, Y_tst = train_test_split(X_trntst, Y_trntst, test_size = .3)\n",
    "\n",
    "    model = LogisticRegression()\n",
    "#     penalty='l1', C=1, solver='saga'\n",
    " \n",
    "    X_oot = X_oot_orig.copy()\n",
    "    X_trn_save = X_trn.copy()\n",
    "    Y_trn_save = Y_trn.copy()\n",
    "\n",
    "    model.fit(X_trn, Y_trn.values.ravel())   \n",
    "\n",
    "    predictions = model.predict_proba(X_trn_save)[:,1]\n",
    "    X_trn['predicted'] = predictions\n",
    "    X_trn['Fraud'] = Y_trn_save['Fraud']\n",
    "    topRows = int(round(X_trn.shape[0]*0.03))\n",
    "    temp = X_trn.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR3.loc[niter, 'trn'] = sum(needed)/sum(X_trn.loc[:,'Fraud'])\n",
    "\n",
    "    predictions = model.predict_proba(X_tst)[:,1]\n",
    "    X_tst['predicted']=predictions\n",
    "    X_tst['Fraud'] = Y_tst['Fraud']\n",
    "    topRows = int(round(X_tst.shape[0]*0.03))\n",
    "    temp = X_tst.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR3.loc[niter, 'tst'] = sum(needed)/sum(X_tst.loc[:,'Fraud'])\n",
    "\n",
    "    predictions = model.predict_proba(X_oot)[:,1]\n",
    "    X_oot['predicted']=predictions\n",
    "    X_oot['Fraud'] = Y_oot['Fraud']\n",
    "    topRows = int(round(X_oot.shape[0]*0.03))\n",
    "    temp = X_oot.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR3.loc[niter, 'oot'] = sum(needed)/sum(X_oot.loc[:,'Fraud'])\n",
    "    print(niter, FDR3.loc[niter, 'trn'],FDR3.loc[niter, 'tst'],FDR3.loc[niter, 'oot'])\n",
    "    \n",
    "print(FDR3.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T06:07:05.713568Z",
     "start_time": "2022-04-27T06:06:57.326423Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.0 0.42424242424242425 0.2122905027932961\n",
      "1 1.0 0.3957597173144876 0.2346368715083799\n",
      "2 1.0 0.38223938223938225 0.19553072625698323\n",
      "3 1.0 0.3192307692307692 0.2011173184357542\n",
      "4 1.0 0.36531365313653136 0.24581005586592178\n",
      "5 1.0 0.3345323741007194 0.2011173184357542\n",
      "6 1.0 0.37777777777777777 0.19553072625698323\n",
      "7 1.0 0.40344827586206894 0.21787709497206703\n",
      "8 1.0 0.37410071942446044 0.2346368715083799\n",
      "9 1.0 0.38661710037174724 0.2122905027932961\n",
      "trn    1.000000\n",
      "tst    0.376326\n",
      "oot    0.215084\n",
      "dtype: float64\n",
      "CPU times: user 7.48 s, sys: 205 ms, total: 7.69 s\n",
      "Wall time: 8.38 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Single DT\n",
    "\n",
    "for niter in range(nitermax):    \n",
    "    X_trn, X_tst, Y_trn, Y_tst = train_test_split(X_trntst, Y_trntst, test_size = .3)\n",
    "\n",
    "    model = DecisionTreeClassifier()\n",
    "#     criterion='gini', splitter='best', max_depth=20, min_samples_split=300,\n",
    "#                             min_samples_leaf=60, max_features=None  \n",
    " \n",
    "    X_oot = X_oot_orig.copy()\n",
    "    X_trn_save = X_trn.copy()\n",
    "    Y_trn_save = Y_trn.copy()\n",
    "\n",
    "    model.fit(X_trn, Y_trn.values.ravel())   \n",
    "\n",
    "    predictions = model.predict_proba(X_trn_save)[:,1]\n",
    "    X_trn['predicted'] = predictions\n",
    "    X_trn['Fraud'] = Y_trn_save['Fraud']\n",
    "    topRows = int(round(X_trn.shape[0]*0.03))\n",
    "    temp = X_trn.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR3.loc[niter, 'trn'] = sum(needed)/sum(X_trn.loc[:,'Fraud'])\n",
    "\n",
    "    predictions = model.predict_proba(X_tst)[:,1]\n",
    "    X_tst['predicted']=predictions\n",
    "    X_tst['Fraud'] = Y_tst['Fraud']\n",
    "    topRows = int(round(X_tst.shape[0]*0.03))\n",
    "    temp = X_tst.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR3.loc[niter, 'tst'] = sum(needed)/sum(X_tst.loc[:,'Fraud'])\n",
    "\n",
    "    predictions = model.predict_proba(X_oot)[:,1]\n",
    "    X_oot['predicted']=predictions\n",
    "    X_oot['Fraud'] = Y_oot['Fraud']\n",
    "    topRows = int(round(X_oot.shape[0]*0.03))\n",
    "    temp = X_oot.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR3.loc[niter, 'oot'] = sum(needed)/sum(X_oot.loc[:,'Fraud'])\n",
    "    print(niter, FDR3.loc[niter, 'trn'],FDR3.loc[niter, 'tst'],FDR3.loc[niter, 'oot'])\n",
    "    \n",
    "print(FDR3.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-01T20:46:21.264371Z",
     "start_time": "2022-05-01T20:46:15.825861Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7429519071310116 0.631768953068592 0.39106145251396646\n",
      "1 0.7194127243066885 0.7265917602996255 0.4022346368715084\n",
      "2 0.7018633540372671 0.6694915254237288 0.5027932960893855\n",
      "3 0.7272727272727273 0.6284584980237155 0.4245810055865922\n",
      "4 0.7243066884176182 0.6779026217228464 0.4022346368715084\n",
      "trn    0.723161\n",
      "tst    0.666843\n",
      "oot    0.424581\n",
      "dtype: float64\n",
      "CPU times: user 6.5 s, sys: 139 ms, total: 6.64 s\n",
      "Wall time: 5.43 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Single DT\n",
    "\n",
    "for niter in range(nitermax):    \n",
    "    X_trn, X_tst, Y_trn, Y_tst = train_test_split(X_trntst, Y_trntst, test_size = .3)\n",
    "\n",
    "    model = DecisionTreeClassifier(\n",
    "        criterion='gini', \n",
    "        splitter='best', \n",
    "        max_depth=8, \n",
    "#         min_samples_leaf=60, \n",
    "        min_samples_split=100,\n",
    "#         max_features=None\n",
    "    )\n",
    "#     \n",
    " \n",
    "    X_oot = X_oot_orig.copy()\n",
    "    X_trn_save = X_trn.copy()\n",
    "    Y_trn_save = Y_trn.copy()\n",
    "\n",
    "    model.fit(X_trn, Y_trn.values.ravel())   \n",
    "\n",
    "    predictions = model.predict_proba(X_trn_save)[:,1]\n",
    "    X_trn['predicted'] = predictions\n",
    "    X_trn['Fraud'] = Y_trn_save['Fraud']\n",
    "    topRows = int(round(X_trn.shape[0]*0.03))\n",
    "    temp = X_trn.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR3.loc[niter, 'trn'] = sum(needed)/sum(X_trn.loc[:,'Fraud'])\n",
    "\n",
    "    predictions = model.predict_proba(X_tst)[:,1]\n",
    "    X_tst['predicted']=predictions\n",
    "    X_tst['Fraud'] = Y_tst['Fraud']\n",
    "    topRows = int(round(X_tst.shape[0]*0.03))\n",
    "    temp = X_tst.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR3.loc[niter, 'tst'] = sum(needed)/sum(X_tst.loc[:,'Fraud'])\n",
    "\n",
    "    predictions = model.predict_proba(X_oot)[:,1]\n",
    "    X_oot['predicted']=predictions\n",
    "    X_oot['Fraud'] = Y_oot['Fraud']\n",
    "    topRows = int(round(X_oot.shape[0]*0.03))\n",
    "    temp = X_oot.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR3.loc[niter, 'oot'] = sum(needed)/sum(X_oot.loc[:,'Fraud'])\n",
    "    print(niter, FDR3.loc[niter, 'trn'],FDR3.loc[niter, 'tst'],FDR3.loc[niter, 'oot'])\n",
    "    \n",
    "print(FDR3.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-01T18:26:47.215704Z",
     "start_time": "2022-05-01T18:26:47.106064Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid parameter min_sample_split for estimator RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n                       max_depth=1, max_features='auto', max_leaf_nodes=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=1, min_samples_split=2,\n                       min_weight_fraction_leaf=0.0, n_estimators='warn',\n                       n_jobs=None, oob_score=False, random_state=None,\n                       verbose=0, warm_start=False). Check the list of available parameters with `estimator.get_params().keys()`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-d2a5b83b7fb8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mgrid_search\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_save\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    686\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1147\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    665\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 667\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    757\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[0mtrain_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mset_params\u001b[0;34m(self, **params)\u001b[0m\n\u001b[1;32m    222\u001b[0m                                  \u001b[0;34m'Check the list of available parameters '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m                                  \u001b[0;34m'with `estimator.get_params().keys()`.'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m                                  (key, self))\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdelim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid parameter min_sample_split for estimator RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n                       max_depth=1, max_features='auto', max_leaf_nodes=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=1, min_samples_split=2,\n                       min_weight_fraction_leaf=0.0, n_estimators='warn',\n                       n_jobs=None, oob_score=False, random_state=None,\n                       verbose=0, warm_start=False). Check the list of available parameters with `estimator.get_params().keys()`."
     ]
    }
   ],
   "source": [
    "\n",
    "param_grid = {\n",
    "    'max_depth': np.arange(1,20),\n",
    "    'min_sample_split': np.arange(5,50,5),\n",
    "    'min_samples_leaf': np.arange(10,100,10),\n",
    "    'n_estimators': np.arange(30,200,10)\n",
    "    }\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "grid_search = GridSearchCV(rf, param_grid,cv=5)\n",
    " \n",
    "grid_search.fit(X,Y_save)\n",
    "grid_search.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-01T20:05:30.202204Z",
     "start_time": "2022-05-01T20:04:42.480028Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7672552166934189 0.7003891050583657 0.5083798882681564\n",
      "1 0.7619047619047619 0.7232472324723247 0.5083798882681564\n",
      "2 0.7622047244094489 0.7428571428571429 0.5307262569832403\n",
      "3 0.7527910685805422 0.7391304347826086 0.5083798882681564\n",
      "4 0.768 0.7176470588235294 0.5083798882681564\n",
      "trn    0.762431\n",
      "tst    0.724654\n",
      "oot    0.512849\n",
      "dtype: float64\n",
      "CPU times: user 45 s, sys: 654 ms, total: 45.7 s\n",
      "Wall time: 47.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# RF\n",
    "\n",
    "for niter in range(nitermax):    \n",
    "    X_trn, X_tst, Y_trn, Y_tst = train_test_split(X_trntst, Y_trntst, test_size = .3)\n",
    "    \n",
    "    model = RandomForestClassifier(\n",
    "        criterion='gini',\n",
    "        max_depth=8,\n",
    "        min_samples_leaf=60, \n",
    "        min_samples_split=10,\n",
    "        n_estimators=56)\n",
    "    \n",
    "    #keep tree simple but # of tree increase\n",
    "    \n",
    "    X_oot = X_oot_orig.copy()\n",
    "    X_trn_save = X_trn.copy()\n",
    "    Y_trn_save = Y_trn.copy()\n",
    "\n",
    "    model.fit(X_trn, Y_trn.values.ravel())   \n",
    "\n",
    "    predictions = model.predict_proba(X_trn_save)[:,1]\n",
    "    X_trn['predicted'] = predictions\n",
    "    X_trn['Fraud'] = Y_trn_save['Fraud']\n",
    "    topRows = int(round(X_trn.shape[0]*0.03))\n",
    "    temp = X_trn.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR3.loc[niter, 'trn'] = sum(needed)/sum(X_trn.loc[:,'Fraud'])\n",
    "\n",
    "    predictions = model.predict_proba(X_tst)[:,1]\n",
    "    X_tst['predicted']=predictions\n",
    "    X_tst['Fraud'] = Y_tst['Fraud']\n",
    "    topRows = int(round(X_tst.shape[0]*0.03))\n",
    "    temp = X_tst.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR3.loc[niter, 'tst'] = sum(needed)/sum(X_tst.loc[:,'Fraud'])\n",
    "\n",
    "    predictions = model.predict_proba(X_oot)[:,1]\n",
    "    X_oot['predicted']=predictions\n",
    "    X_oot['Fraud'] = Y_oot['Fraud']\n",
    "    topRows = int(round(X_oot.shape[0]*0.03))\n",
    "    temp = X_oot.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR3.loc[niter, 'oot'] = sum(needed)/sum(X_oot.loc[:,'Fraud'])\n",
    "    print(niter, FDR3.loc[niter, 'trn'],FDR3.loc[niter, 'tst'],FDR3.loc[niter, 'oot'])\n",
    "\n",
    "print(FDR3.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-01T20:09:55.264977Z",
     "start_time": "2022-05-01T20:09:13.884851Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7651757188498403 0.6889763779527559 0.4972067039106145\n",
      "1 0.7432216905901117 0.7628458498023716 0.48044692737430167\n",
      "2 0.7654941373534339 0.7102473498233216 0.5195530726256983\n",
      "3 0.7409733124018838 0.7654320987654321 0.4860335195530726\n",
      "4 0.750788643533123 0.7357723577235772 0.48044692737430167\n",
      "trn    0.753131\n",
      "tst    0.732655\n",
      "oot    0.492737\n",
      "dtype: float64\n",
      "CPU times: user 38.1 s, sys: 636 ms, total: 38.8 s\n",
      "Wall time: 41.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# RF\n",
    "\n",
    "for niter in range(nitermax):    \n",
    "    X_trn, X_tst, Y_trn, Y_tst = train_test_split(X_trntst, Y_trntst, test_size = .3)\n",
    "    \n",
    "    model = RandomForestClassifier(\n",
    "        criterion='gini',\n",
    "        max_depth=7,\n",
    "        min_samples_leaf=60, \n",
    "        min_samples_split=10,\n",
    "        n_estimators=50)\n",
    "    \n",
    "    #keep tree simple but # of tree increase\n",
    "    \n",
    "    X_oot = X_oot_orig.copy()\n",
    "    X_trn_save = X_trn.copy()\n",
    "    Y_trn_save = Y_trn.copy()\n",
    "\n",
    "    model.fit(X_trn, Y_trn.values.ravel())   \n",
    "\n",
    "    predictions = model.predict_proba(X_trn_save)[:,1]\n",
    "    X_trn['predicted'] = predictions\n",
    "    X_trn['Fraud'] = Y_trn_save['Fraud']\n",
    "    topRows = int(round(X_trn.shape[0]*0.03))\n",
    "    temp = X_trn.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR3.loc[niter, 'trn'] = sum(needed)/sum(X_trn.loc[:,'Fraud'])\n",
    "\n",
    "    predictions = model.predict_proba(X_tst)[:,1]\n",
    "    X_tst['predicted']=predictions\n",
    "    X_tst['Fraud'] = Y_tst['Fraud']\n",
    "    topRows = int(round(X_tst.shape[0]*0.03))\n",
    "    temp = X_tst.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR3.loc[niter, 'tst'] = sum(needed)/sum(X_tst.loc[:,'Fraud'])\n",
    "\n",
    "    predictions = model.predict_proba(X_oot)[:,1]\n",
    "    X_oot['predicted']=predictions\n",
    "    X_oot['Fraud'] = Y_oot['Fraud']\n",
    "    topRows = int(round(X_oot.shape[0]*0.03))\n",
    "    temp = X_oot.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR3.loc[niter, 'oot'] = sum(needed)/sum(X_oot.loc[:,'Fraud'])\n",
    "    print(niter, FDR3.loc[niter, 'trn'],FDR3.loc[niter, 'tst'],FDR3.loc[niter, 'oot'])\n",
    "\n",
    "print(FDR3.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-01T18:59:39.409485Z",
     "start_time": "2022-05-01T18:58:48.492362Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7671009771986971 0.7218045112781954 0.553072625698324\n",
      "1 0.7544426494345718 0.7586206896551724 0.547486033519553\n",
      "2 0.7634228187919463 0.7253521126760564 0.4860335195530726\n",
      "3 0.7431340872374798 0.7854406130268199 0.5865921787709497\n",
      "4 0.7423014586709886 0.7718631178707225 0.5307262569832403\n",
      "trn    0.754080\n",
      "tst    0.752616\n",
      "oot    0.540782\n",
      "dtype: float64\n",
      "CPU times: user 50.8 s, sys: 634 ms, total: 51.4 s\n",
      "Wall time: 50.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# RF\n",
    "\n",
    "for niter in range(nitermax):    \n",
    "    X_trn, X_tst, Y_trn, Y_tst = train_test_split(X_trntst, Y_trntst, test_size = .3)\n",
    "    \n",
    "    model = RandomForestClassifier(\n",
    "        criterion='gini',\n",
    "        max_depth=8,\n",
    "        min_samples_leaf=60, \n",
    "        min_samples_split=10,\n",
    "        n_estimators=65)\n",
    "\n",
    "    X_oot = X_oot_orig.copy()\n",
    "    X_trn_save = X_trn.copy()\n",
    "    Y_trn_save = Y_trn.copy()\n",
    "\n",
    "    model.fit(X_trn, Y_trn.values.ravel())   \n",
    "\n",
    "    predictions = model.predict_proba(X_trn_save)[:,1]\n",
    "    X_trn['predicted'] = predictions\n",
    "    X_trn['Fraud'] = Y_trn_save['Fraud']\n",
    "    topRows = int(round(X_trn.shape[0]*0.03))\n",
    "    temp = X_trn.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR3.loc[niter, 'trn'] = sum(needed)/sum(X_trn.loc[:,'Fraud'])\n",
    "\n",
    "    predictions = model.predict_proba(X_tst)[:,1]\n",
    "    X_tst['predicted']=predictions\n",
    "    X_tst['Fraud'] = Y_tst['Fraud']\n",
    "    topRows = int(round(X_tst.shape[0]*0.03))\n",
    "    temp = X_tst.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR3.loc[niter, 'tst'] = sum(needed)/sum(X_tst.loc[:,'Fraud'])\n",
    "\n",
    "    predictions = model.predict_proba(X_oot)[:,1]\n",
    "    X_oot['predicted']=predictions\n",
    "    X_oot['Fraud'] = Y_oot['Fraud']\n",
    "    topRows = int(round(X_oot.shape[0]*0.03))\n",
    "    temp = X_oot.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR3.loc[niter, 'oot'] = sum(needed)/sum(X_oot.loc[:,'Fraud'])\n",
    "    print(niter, FDR3.loc[niter, 'trn'],FDR3.loc[niter, 'tst'],FDR3.loc[niter, 'oot'])\n",
    "\n",
    "print(FDR3.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Boosted Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T01:04:11.554106Z",
     "start_time": "2022-05-03T01:04:02.055137Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.860062893081761 0.7049180327868853 0.4692737430167598\n",
      "1 0.8905472636815921 0.7472924187725631 0.547486033519553\n",
      "2 0.8898026315789473 0.7426470588235294 0.5139664804469274\n",
      "3 0.8536585365853658 0.7320754716981132 0.43575418994413406\n",
      "4 0.8661290322580645 0.7230769230769231 0.48044692737430167\n",
      "trn    0.872040\n",
      "tst    0.730002\n",
      "oot    0.489385\n",
      "dtype: float64\n",
      "CPU times: user 28.4 s, sys: 1.06 s, total: 29.5 s\n",
      "Wall time: 9.49 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# BT\n",
    "\n",
    "for niter in range(nitermax):    \n",
    "    X_trn, X_tst, Y_trn, Y_tst = train_test_split(X_trntst, Y_trntst, test_size = .3)\n",
    "\n",
    "    model = lgb.LGBMClassifier(\n",
    "        learning_rate=0.001, \n",
    "        max_depth=10, \n",
    "        num_leaves=100,\n",
    "        subsample=0.8,\n",
    "        boosting_type='gbdt'\n",
    "\n",
    "    \n",
    "    )\n",
    "#     \n",
    "\n",
    "    X_oot = X_oot_orig.copy()\n",
    "    X_trn_save = X_trn.copy()\n",
    "    Y_trn_save = Y_trn.copy()\n",
    "\n",
    "    model.fit(X_trn, Y_trn.values.ravel())   \n",
    "\n",
    "    predictions = model.predict_proba(X_trn_save)[:,1]\n",
    "    X_trn['predicted'] = predictions\n",
    "    X_trn['Fraud'] = Y_trn_save['Fraud']\n",
    "    topRows = int(round(X_trn.shape[0]*0.03))\n",
    "    temp = X_trn.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR3.loc[niter, 'trn'] = sum(needed)/sum(X_trn.loc[:,'Fraud'])\n",
    "\n",
    "    predictions = model.predict_proba(X_tst)[:,1]\n",
    "    X_tst['predicted']=predictions\n",
    "    X_tst['Fraud'] = Y_tst['Fraud']\n",
    "    topRows = int(round(X_tst.shape[0]*0.03))\n",
    "    temp = X_tst.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR3.loc[niter, 'tst'] = sum(needed)/sum(X_tst.loc[:,'Fraud'])\n",
    "\n",
    "    predictions = model.predict_proba(X_oot)[:,1]\n",
    "    X_oot['predicted']=predictions\n",
    "    X_oot['Fraud'] = Y_oot['Fraud']\n",
    "    topRows = int(round(X_oot.shape[0]*0.03))\n",
    "    temp = X_oot.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR3.loc[niter, 'oot'] = sum(needed)/sum(X_oot.loc[:,'Fraud'])\n",
    "    print(niter, FDR3.loc[niter, 'trn'],FDR3.loc[niter, 'tst'],FDR3.loc[niter, 'oot'])\n",
    "    \n",
    "print(FDR3.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-01T20:24:58.839259Z",
     "start_time": "2022-05-01T20:24:52.559698Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.819935691318328 0.7596899224806202 0.5195530726256983\n",
      "1 0.8089368258859785 0.7272727272727273 0.4581005586592179\n",
      "2 0.8391376451077943 0.7256317689530686 0.4692737430167598\n",
      "3 0.8201320132013201 0.7627737226277372 0.4581005586592179\n",
      "4 0.8210702341137124 0.7163120567375887 0.44692737430167595\n",
      "trn    0.821842\n",
      "tst    0.738336\n",
      "oot    0.470391\n",
      "dtype: float64\n",
      "CPU times: user 20.7 s, sys: 922 ms, total: 21.7 s\n",
      "Wall time: 6.27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# BT\n",
    "\n",
    "for niter in range(nitermax):    \n",
    "    X_trn, X_tst, Y_trn, Y_tst = train_test_split(X_trntst, Y_trntst, test_size = .3)\n",
    "\n",
    "    model = lgb.LGBMClassifier(\n",
    "        learning_rate=0.01, \n",
    "        max_depth=5, \n",
    "        num_leaves=15, \n",
    "        n_estimators=161)\n",
    "#     \n",
    "\n",
    "    X_oot = X_oot_orig.copy()\n",
    "    X_trn_save = X_trn.copy()\n",
    "    Y_trn_save = Y_trn.copy()\n",
    "\n",
    "    model.fit(X_trn, Y_trn.values.ravel())   \n",
    "\n",
    "    predictions = model.predict_proba(X_trn_save)[:,1]\n",
    "    X_trn['predicted'] = predictions\n",
    "    X_trn['Fraud'] = Y_trn_save['Fraud']\n",
    "    topRows = int(round(X_trn.shape[0]*0.03))\n",
    "    temp = X_trn.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR3.loc[niter, 'trn'] = sum(needed)/sum(X_trn.loc[:,'Fraud'])\n",
    "\n",
    "    predictions = model.predict_proba(X_tst)[:,1]\n",
    "    X_tst['predicted']=predictions\n",
    "    X_tst['Fraud'] = Y_tst['Fraud']\n",
    "    topRows = int(round(X_tst.shape[0]*0.03))\n",
    "    temp = X_tst.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR3.loc[niter, 'tst'] = sum(needed)/sum(X_tst.loc[:,'Fraud'])\n",
    "\n",
    "    predictions = model.predict_proba(X_oot)[:,1]\n",
    "    X_oot['predicted']=predictions\n",
    "    X_oot['Fraud'] = Y_oot['Fraud']\n",
    "    topRows = int(round(X_oot.shape[0]*0.03))\n",
    "    temp = X_oot.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR3.loc[niter, 'oot'] = sum(needed)/sum(X_oot.loc[:,'Fraud'])\n",
    "    print(niter, FDR3.loc[niter, 'trn'],FDR3.loc[niter, 'tst'],FDR3.loc[niter, 'oot'])\n",
    "    \n",
    "print(FDR3.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Gradient Boosting Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-02T05:23:21.112111Z",
     "start_time": "2022-05-02T05:19:12.073550Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.8025682182985554 0.7509727626459144 0.39664804469273746\n",
      "1 0.8118971061093248 0.7093023255813954 0.4692737430167598\n",
      "2 0.7936772046589018 0.7168458781362007 0.4860335195530726\n",
      "3 0.7774244833068362 0.7171314741035857 0.5083798882681564\n",
      "4 0.7891268533772653 0.6776556776556777 0.37988826815642457\n",
      "trn    0.794939\n",
      "tst    0.714382\n",
      "oot    0.448045\n",
      "dtype: float64\n",
      "CPU times: user 4min 8s, sys: 1.06 s, total: 4min 9s\n",
      "Wall time: 4min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# GBT\n",
    "\n",
    "for niter in range(nitermax):    \n",
    "    X_trn, X_tst, Y_trn, Y_tst = train_test_split(X_trntst, Y_trntst, test_size = .3)\n",
    "\n",
    "    model = GradientBoostingClassifier(\n",
    "        learning_rate=0.01,\n",
    "        max_depth= 8,\n",
    "#         min_samples_leaf= 60,\n",
    "#         min_samples_split= 10,\n",
    "#         n_estimators=50\n",
    "    )\n",
    "\n",
    "\n",
    "    X_oot = X_oot_orig.copy()\n",
    "    X_trn_save = X_trn.copy()\n",
    "    Y_trn_save = Y_trn.copy()\n",
    "\n",
    "    model.fit(X_trn, Y_trn.values.ravel())   \n",
    "\n",
    "    predictions = model.predict_proba(X_trn_save)[:,1]\n",
    "    X_trn['predicted'] = predictions\n",
    "    X_trn['Fraud'] = Y_trn_save['Fraud']\n",
    "    topRows = int(round(X_trn.shape[0]*0.03))\n",
    "    temp = X_trn.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR3.loc[niter, 'trn'] = sum(needed)/sum(X_trn.loc[:,'Fraud'])\n",
    "\n",
    "    predictions = model.predict_proba(X_tst)[:,1]\n",
    "    X_tst['predicted']=predictions\n",
    "    X_tst['Fraud'] = Y_tst['Fraud']\n",
    "    topRows = int(round(X_tst.shape[0]*0.03))\n",
    "    temp = X_tst.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR3.loc[niter, 'tst'] = sum(needed)/sum(X_tst.loc[:,'Fraud'])\n",
    "\n",
    "    predictions = model.predict_proba(X_oot)[:,1]\n",
    "    X_oot['predicted']=predictions\n",
    "    X_oot['Fraud'] = Y_oot['Fraud']\n",
    "    topRows = int(round(X_oot.shape[0]*0.03))\n",
    "    temp = X_oot.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR3.loc[niter, 'oot'] = sum(needed)/sum(X_oot.loc[:,'Fraud'])\n",
    "    print(niter, FDR3.loc[niter, 'trn'],FDR3.loc[niter, 'tst'],FDR3.loc[niter, 'oot'])\n",
    "    \n",
    "print(FDR3.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-02T18:06:40.529628Z",
     "start_time": "2022-05-02T18:02:57.631674Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7401960784313726 0.7052238805970149 0.43575418994413406\n",
      "1 0.7645161290322581 0.7346153846153847 0.441340782122905\n",
      "2 0.7483552631578947 0.7058823529411765 0.43575418994413406\n",
      "3 0.7405660377358491 0.7049180327868853 0.36312849162011174\n",
      "4 0.7336448598130841 0.7016806722689075 0.4581005586592179\n",
      "trn    0.745456\n",
      "tst    0.710464\n",
      "oot    0.426816\n",
      "dtype: float64\n",
      "CPU times: user 3min 12s, sys: 2.75 s, total: 3min 14s\n",
      "Wall time: 3min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# GBT\n",
    "\n",
    "for niter in range(nitermax):    \n",
    "    X_trn, X_tst, Y_trn, Y_tst = train_test_split(X_trntst, Y_trntst, test_size = .3)\n",
    "\n",
    "    model = GradientBoostingClassifier(\n",
    "        learning_rate=0.01,\n",
    "        max_depth= 5,\n",
    "#         min_samples_leaf= 50,\n",
    "#         min_samples_split= 10,\n",
    "        n_estimators=150\n",
    "    )\n",
    "\n",
    "\n",
    "    X_oot = X_oot_orig.copy()\n",
    "    X_trn_save = X_trn.copy()\n",
    "    Y_trn_save = Y_trn.copy()\n",
    "\n",
    "    model.fit(X_trn, Y_trn.values.ravel())   \n",
    "\n",
    "    predictions = model.predict_proba(X_trn_save)[:,1]\n",
    "    X_trn['predicted'] = predictions\n",
    "    X_trn['Fraud'] = Y_trn_save['Fraud']\n",
    "    topRows = int(round(X_trn.shape[0]*0.03))\n",
    "    temp = X_trn.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR3.loc[niter, 'trn'] = sum(needed)/sum(X_trn.loc[:,'Fraud'])\n",
    "\n",
    "    predictions = model.predict_proba(X_tst)[:,1]\n",
    "    X_tst['predicted']=predictions\n",
    "    X_tst['Fraud'] = Y_tst['Fraud']\n",
    "    topRows = int(round(X_tst.shape[0]*0.03))\n",
    "    temp = X_tst.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR3.loc[niter, 'tst'] = sum(needed)/sum(X_tst.loc[:,'Fraud'])\n",
    "\n",
    "    predictions = model.predict_proba(X_oot)[:,1]\n",
    "    X_oot['predicted']=predictions\n",
    "    X_oot['Fraud'] = Y_oot['Fraud']\n",
    "    topRows = int(round(X_oot.shape[0]*0.03))\n",
    "    temp = X_oot.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR3.loc[niter, 'oot'] = sum(needed)/sum(X_oot.loc[:,'Fraud'])\n",
    "    print(niter, FDR3.loc[niter, 'trn'],FDR3.loc[niter, 'tst'],FDR3.loc[niter, 'oot'])\n",
    "    \n",
    "print(FDR3.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T06:09:44.034246Z",
     "start_time": "2022-04-27T06:08:18.594304Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.631578947368421 0.6580882352941176 0.3687150837988827\n",
      "1 0.6324503311258278 0.6630434782608695 0.3240223463687151\n",
      "2 0.6403225806451613 0.6384615384615384 0.3743016759776536\n",
      "3 0.6276422764227643 0.6641509433962264 0.3854748603351955\n",
      "4 0.6327503974562798 0.6533864541832669 0.3687150837988827\n",
      "5 0.6411960132890365 0.6438848920863309 0.3687150837988827\n",
      "6 0.6423948220064725 0.6221374045801527 0.3743016759776536\n",
      "7 0.6502463054187192 0.6162361623616236 0.3463687150837989\n",
      "8 0.64375 0.6375 0.329608938547486\n",
      "9 0.6283048211508554 0.6582278481012658 0.441340782122905\n",
      "trn    0.637064\n",
      "tst    0.645512\n",
      "oot    0.368156\n",
      "dtype: float64\n",
      "CPU times: user 1min 49s, sys: 3.49 s, total: 1min 52s\n",
      "Wall time: 1min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# NN\n",
    "\n",
    "for niter in range(nitermax):    \n",
    "    X_trn, X_tst, Y_trn, Y_tst = train_test_split(X_trntst, Y_trntst, test_size = .3)\n",
    "\n",
    "    model = MLPClassifier()\n",
    "#     hidden_layer_sizes=(100,),activation='relu',solver='adam',alpha=0.001,learning_rate='adaptive',\n",
    "#     learning_rate_init=0.001,max_iter=200\n",
    "    \n",
    "    X_oot = X_oot_orig.copy()\n",
    "    X_trn_save = X_trn.copy()\n",
    "    Y_trn_save = Y_trn.copy()\n",
    "\n",
    "    model.fit(X_trn, Y_trn.values.ravel())   \n",
    "\n",
    "    predictions = model.predict_proba(X_trn_save)[:,1]\n",
    "    X_trn['predicted'] = predictions\n",
    "    X_trn['Fraud'] = Y_trn_save['Fraud']\n",
    "    topRows = int(round(X_trn.shape[0]*0.03))\n",
    "    temp = X_trn.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR3.loc[niter, 'trn'] = sum(needed)/sum(X_trn.loc[:,'Fraud'])\n",
    "\n",
    "    predictions = model.predict_proba(X_tst)[:,1]\n",
    "    X_tst['predicted']=predictions\n",
    "    X_tst['Fraud'] = Y_tst['Fraud']\n",
    "    topRows = int(round(X_tst.shape[0]*0.03))\n",
    "    temp = X_tst.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR3.loc[niter, 'tst'] = sum(needed)/sum(X_tst.loc[:,'Fraud'])\n",
    "\n",
    "    predictions = model.predict_proba(X_oot)[:,1]\n",
    "    X_oot['predicted']=predictions\n",
    "    X_oot['Fraud'] = Y_oot['Fraud']\n",
    "    topRows = int(round(X_oot.shape[0]*0.03))\n",
    "    temp = X_oot.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR3.loc[niter, 'oot'] = sum(needed)/sum(X_oot.loc[:,'Fraud'])\n",
    "    print(niter, FDR3.loc[niter, 'trn'],FDR3.loc[niter, 'tst'],FDR3.loc[niter, 'oot'])\n",
    "    \n",
    "print(FDR3.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-02T18:34:37.738989Z",
     "start_time": "2022-05-02T18:33:20.641178Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7721311475409836 0.7074074074074074 0.5810055865921788\n",
      "1 0.7869127516778524 0.6866197183098591 0.5977653631284916\n",
      "2 0.7644151565074135 0.7472527472527473 0.5921787709497207\n",
      "3 0.7608346709470305 0.7315175097276264 0.5977653631284916\n",
      "4 0.7661691542288557 0.740072202166065 0.6033519553072626\n",
      "trn    0.770093\n",
      "tst    0.722574\n",
      "oot    0.594413\n",
      "dtype: float64\n",
      "CPU times: user 2min 10s, sys: 3.79 s, total: 2min 14s\n",
      "Wall time: 1min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# NN\n",
    "\n",
    "for niter in range(nitermax):    \n",
    "    X_trn, X_tst, Y_trn, Y_tst = train_test_split(X_trntst, Y_trntst, test_size = .3)\n",
    "\n",
    "    model = MLPClassifier(\n",
    "        hidden_layer_sizes=(50,),\n",
    "#         max_iter=200,\n",
    "#         activation='relu',\n",
    "#         solver='adam',\n",
    "        alpha=0.1,\n",
    "#         learning_rate_init=0.001,\n",
    "        learning_rate='adaptive'\n",
    "        )\n",
    "    \n",
    "    X_oot = X_oot_orig.copy()\n",
    "    X_trn_save = X_trn.copy()\n",
    "    Y_trn_save = Y_trn.copy()\n",
    "\n",
    "    model.fit(X_trn, Y_trn.values.ravel())   \n",
    "\n",
    "    predictions = model.predict_proba(X_trn_save)[:,1]\n",
    "    X_trn['predicted'] = predictions\n",
    "    X_trn['Fraud'] = Y_trn_save['Fraud']\n",
    "    topRows = int(round(X_trn.shape[0]*0.03))\n",
    "    temp = X_trn.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR3.loc[niter, 'trn'] = sum(needed)/sum(X_trn.loc[:,'Fraud'])\n",
    "\n",
    "    predictions = model.predict_proba(X_tst)[:,1]\n",
    "    X_tst['predicted']=predictions\n",
    "    X_tst['Fraud'] = Y_tst['Fraud']\n",
    "    topRows = int(round(X_tst.shape[0]*0.03))\n",
    "    temp = X_tst.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR3.loc[niter, 'tst'] = sum(needed)/sum(X_tst.loc[:,'Fraud'])\n",
    "\n",
    "    predictions = model.predict_proba(X_oot)[:,1]\n",
    "    X_oot['predicted']=predictions\n",
    "    X_oot['Fraud'] = Y_oot['Fraud']\n",
    "    topRows = int(round(X_oot.shape[0]*0.03))\n",
    "    temp = X_oot.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR3.loc[niter, 'oot'] = sum(needed)/sum(X_oot.loc[:,'Fraud'])\n",
    "    print(niter, FDR3.loc[niter, 'trn'],FDR3.loc[niter, 'tst'],FDR3.loc[niter, 'oot'])\n",
    "    \n",
    "print(FDR3.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T01:28:56.702153Z",
     "start_time": "2022-05-03T01:23:23.457654Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.8484848484848485 0.8023715415019763 0.5418994413407822\n",
      "1 0.8512 0.7803921568627451 0.5642458100558659\n",
      "2 0.853225806451613 0.7807692307692308 0.5363128491620112\n",
      "3 0.8503184713375797 0.7936507936507936 0.6424581005586593\n",
      "4 0.8827361563517915 0.7781954887218046 0.5418994413407822\n",
      "trn    0.857193\n",
      "tst    0.787076\n",
      "oot    0.565363\n",
      "dtype: float64\n",
      "CPU times: user 9min 30s, sys: 11.5 s, total: 9min 41s\n",
      "Wall time: 5min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# NN\n",
    "\n",
    "for niter in range(nitermax):    \n",
    "    X_trn, X_tst, Y_trn, Y_tst = train_test_split(X_trntst, Y_trntst, test_size = .3)\n",
    "\n",
    "    model = MLPClassifier(\n",
    "        hidden_layer_sizes=(60,30,15,),\n",
    "#         max_iter=200,\n",
    "        activation='relu',\n",
    "#         solver='adam',\n",
    "        alpha=0.01,\n",
    "#         learning_rate_init=0.001,\n",
    "        learning_rate='adaptive'\n",
    "        )\n",
    "    \n",
    "    X_oot = X_oot_orig.copy()\n",
    "    X_trn_save = X_trn.copy()\n",
    "    Y_trn_save = Y_trn.copy()\n",
    "\n",
    "    model.fit(X_trn, Y_trn.values.ravel())   \n",
    "\n",
    "    predictions = model.predict_proba(X_trn_save)[:,1]\n",
    "    X_trn['predicted'] = predictions\n",
    "    X_trn['Fraud'] = Y_trn_save['Fraud']\n",
    "    topRows = int(round(X_trn.shape[0]*0.03))\n",
    "    temp = X_trn.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR3.loc[niter, 'trn'] = sum(needed)/sum(X_trn.loc[:,'Fraud'])\n",
    "\n",
    "    predictions = model.predict_proba(X_tst)[:,1]\n",
    "    X_tst['predicted']=predictions\n",
    "    X_tst['Fraud'] = Y_tst['Fraud']\n",
    "    topRows = int(round(X_tst.shape[0]*0.03))\n",
    "    temp = X_tst.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR3.loc[niter, 'tst'] = sum(needed)/sum(X_tst.loc[:,'Fraud'])\n",
    "\n",
    "    predictions = model.predict_proba(X_oot)[:,1]\n",
    "    X_oot['predicted']=predictions\n",
    "    X_oot['Fraud'] = Y_oot['Fraud']\n",
    "    topRows = int(round(X_oot.shape[0]*0.03))\n",
    "    temp = X_oot.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR3.loc[niter, 'oot'] = sum(needed)/sum(X_oot.loc[:,'Fraud'])\n",
    "    print(niter, FDR3.loc[niter, 'trn'],FDR3.loc[niter, 'tst'],FDR3.loc[niter, 'oot'])\n",
    "    \n",
    "print(FDR3.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-02T18:59:54.072551Z",
     "start_time": "2022-05-02T18:57:23.028571Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7680412371134021 0.7449664429530202 0.6145251396648045\n",
      "1 0.7653910149750416 0.7132616487455197 0.5642458100558659\n",
      "2 0.7691082802547771 0.6984126984126984 0.5642458100558659\n",
      "3 0.7732463295269169 0.7191011235955056 0.6089385474860335\n",
      "4 0.7644230769230769 0.73046875 0.6368715083798883\n",
      "5 0.7648 0.7294117647058823 0.5977653631284916\n",
      "6 0.7557755775577558 0.7627737226277372 0.6089385474860335\n",
      "7 0.7782874617737003 0.7035398230088495 0.6368715083798883\n",
      "8 0.7611202635914333 0.7435897435897436 0.6089385474860335\n",
      "9 0.7562604340567612 0.7224199288256228 0.6033519553072626\n",
      "trn    0.765645\n",
      "tst    0.726795\n",
      "oot    0.604469\n",
      "dtype: float64\n",
      "CPU times: user 4min 28s, sys: 6.8 s, total: 4min 35s\n",
      "Wall time: 2min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# NN\n",
    "\n",
    "for niter in range(10):    \n",
    "    X_trn, X_tst, Y_trn, Y_tst = train_test_split(X_trntst, Y_trntst, test_size = .3)\n",
    "\n",
    "    model = MLPClassifier(\n",
    "        hidden_layer_sizes=(60),\n",
    "#         max_iter=200,\n",
    "#         activation='relu',\n",
    "#         solver='adam',\n",
    "        alpha=0.01,\n",
    "#         learning_rate_init=0.001,\n",
    "        learning_rate='adaptive'\n",
    "        )\n",
    "    \n",
    "    X_oot = X_oot_orig.copy()\n",
    "    X_trn_save = X_trn.copy()\n",
    "    Y_trn_save = Y_trn.copy()\n",
    "\n",
    "    model.fit(X_trn, Y_trn.values.ravel())   \n",
    "\n",
    "    predictions = model.predict_proba(X_trn_save)[:,1]\n",
    "    X_trn['predicted'] = predictions\n",
    "    X_trn['Fraud'] = Y_trn_save['Fraud']\n",
    "    topRows = int(round(X_trn.shape[0]*0.03))\n",
    "    temp = X_trn.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR3.loc[niter, 'trn'] = sum(needed)/sum(X_trn.loc[:,'Fraud'])\n",
    "\n",
    "    predictions = model.predict_proba(X_tst)[:,1]\n",
    "    X_tst['predicted']=predictions\n",
    "    X_tst['Fraud'] = Y_tst['Fraud']\n",
    "    topRows = int(round(X_tst.shape[0]*0.03))\n",
    "    temp = X_tst.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR3.loc[niter, 'tst'] = sum(needed)/sum(X_tst.loc[:,'Fraud'])\n",
    "\n",
    "    predictions = model.predict_proba(X_oot)[:,1]\n",
    "    X_oot['predicted']=predictions\n",
    "    X_oot['Fraud'] = Y_oot['Fraud']\n",
    "    topRows = int(round(X_oot.shape[0]*0.03))\n",
    "    temp = X_oot.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR3.loc[niter, 'oot'] = sum(needed)/sum(X_oot.loc[:,'Fraud'])\n",
    "    print(niter, FDR3.loc[niter, 'trn'],FDR3.loc[niter, 'tst'],FDR3.loc[niter, 'oot'])\n",
    "    \n",
    "print(FDR3.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-02T18:43:35.863401Z",
     "start_time": "2022-05-02T18:41:51.155319Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7797716150081566 0.7078651685393258 0.5754189944134078\n",
      "1 0.7540453074433657 0.7557251908396947 0.5921787709497207\n",
      "2 0.7609427609427609 0.7342657342657343 0.5865921787709497\n",
      "3 0.7541528239202658 0.7517985611510791 0.5865921787709497\n",
      "4 0.7711038961038961 0.7272727272727273 0.6368715083798883\n",
      "trn    0.763438\n",
      "tst    0.732498\n",
      "oot    0.596648\n",
      "dtype: float64\n",
      "CPU times: user 2min 40s, sys: 5.29 s, total: 2min 45s\n",
      "Wall time: 1min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# NN\n",
    "\n",
    "for niter in range(nitermax):    \n",
    "    X_trn, X_tst, Y_trn, Y_tst = train_test_split(X_trntst, Y_trntst, test_size = .3)\n",
    "\n",
    "    model = MLPClassifier(\n",
    "        hidden_layer_sizes=(65),\n",
    "#         max_iter=200,\n",
    "#         activation='relu',\n",
    "#         solver='adam',\n",
    "        alpha=0.01,\n",
    "#         learning_rate_init=0.001,\n",
    "        learning_rate='adaptive'\n",
    "        )\n",
    "    \n",
    "    X_oot = X_oot_orig.copy()\n",
    "    X_trn_save = X_trn.copy()\n",
    "    Y_trn_save = Y_trn.copy()\n",
    "\n",
    "    model.fit(X_trn, Y_trn.values.ravel())   \n",
    "\n",
    "    predictions = model.predict_proba(X_trn_save)[:,1]\n",
    "    X_trn['predicted'] = predictions\n",
    "    X_trn['Fraud'] = Y_trn_save['Fraud']\n",
    "    topRows = int(round(X_trn.shape[0]*0.03))\n",
    "    temp = X_trn.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR3.loc[niter, 'trn'] = sum(needed)/sum(X_trn.loc[:,'Fraud'])\n",
    "\n",
    "    predictions = model.predict_proba(X_tst)[:,1]\n",
    "    X_tst['predicted']=predictions\n",
    "    X_tst['Fraud'] = Y_tst['Fraud']\n",
    "    topRows = int(round(X_tst.shape[0]*0.03))\n",
    "    temp = X_tst.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR3.loc[niter, 'tst'] = sum(needed)/sum(X_tst.loc[:,'Fraud'])\n",
    "\n",
    "    predictions = model.predict_proba(X_oot)[:,1]\n",
    "    X_oot['predicted']=predictions\n",
    "    X_oot['Fraud'] = Y_oot['Fraud']\n",
    "    topRows = int(round(X_oot.shape[0]*0.03))\n",
    "    temp = X_oot.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR3.loc[niter, 'oot'] = sum(needed)/sum(X_oot.loc[:,'Fraud'])\n",
    "    print(niter, FDR3.loc[niter, 'trn'],FDR3.loc[niter, 'tst'],FDR3.loc[niter, 'oot'])\n",
    "    \n",
    "print(FDR3.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-02T19:01:29.012152Z",
     "start_time": "2022-05-02T19:01:15.202770Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.485422\n",
      "0:\tlearn: 0.0903270\ttotal: 17.9ms\tremaining: 1.77s\n",
      "1:\tlearn: 0.0411562\ttotal: 34.4ms\tremaining: 1.68s\n",
      "2:\tlearn: 0.0334225\ttotal: 45.7ms\tremaining: 1.48s\n",
      "3:\tlearn: 0.0294381\ttotal: 59.6ms\tremaining: 1.43s\n",
      "4:\tlearn: 0.0277436\ttotal: 73.2ms\tremaining: 1.39s\n",
      "5:\tlearn: 0.0269169\ttotal: 85.9ms\tremaining: 1.35s\n",
      "6:\tlearn: 0.0264037\ttotal: 99ms\tremaining: 1.31s\n",
      "7:\tlearn: 0.0256345\ttotal: 113ms\tremaining: 1.29s\n",
      "8:\tlearn: 0.0250722\ttotal: 130ms\tremaining: 1.32s\n",
      "9:\tlearn: 0.0245042\ttotal: 146ms\tremaining: 1.31s\n",
      "10:\tlearn: 0.0242957\ttotal: 163ms\tremaining: 1.32s\n",
      "11:\tlearn: 0.0239468\ttotal: 181ms\tremaining: 1.32s\n",
      "12:\tlearn: 0.0236865\ttotal: 197ms\tremaining: 1.32s\n",
      "13:\tlearn: 0.0234565\ttotal: 235ms\tremaining: 1.44s\n",
      "14:\tlearn: 0.0230986\ttotal: 254ms\tremaining: 1.44s\n",
      "15:\tlearn: 0.0228143\ttotal: 271ms\tremaining: 1.42s\n",
      "16:\tlearn: 0.0225277\ttotal: 289ms\tremaining: 1.41s\n",
      "17:\tlearn: 0.0221383\ttotal: 308ms\tremaining: 1.4s\n",
      "18:\tlearn: 0.0219070\ttotal: 324ms\tremaining: 1.38s\n",
      "19:\tlearn: 0.0217380\ttotal: 474ms\tremaining: 1.9s\n",
      "20:\tlearn: 0.0214975\ttotal: 521ms\tremaining: 1.96s\n",
      "21:\tlearn: 0.0213663\ttotal: 551ms\tremaining: 1.96s\n",
      "22:\tlearn: 0.0205046\ttotal: 573ms\tremaining: 1.92s\n",
      "23:\tlearn: 0.0204276\ttotal: 604ms\tremaining: 1.91s\n",
      "24:\tlearn: 0.0202196\ttotal: 633ms\tremaining: 1.9s\n",
      "25:\tlearn: 0.0200954\ttotal: 658ms\tremaining: 1.87s\n",
      "26:\tlearn: 0.0199126\ttotal: 714ms\tremaining: 1.93s\n",
      "27:\tlearn: 0.0198038\ttotal: 737ms\tremaining: 1.89s\n",
      "28:\tlearn: 0.0196292\ttotal: 754ms\tremaining: 1.85s\n",
      "29:\tlearn: 0.0191682\ttotal: 773ms\tremaining: 1.8s\n",
      "30:\tlearn: 0.0188940\ttotal: 806ms\tremaining: 1.79s\n",
      "31:\tlearn: 0.0187300\ttotal: 821ms\tremaining: 1.74s\n",
      "32:\tlearn: 0.0186129\ttotal: 835ms\tremaining: 1.7s\n",
      "33:\tlearn: 0.0184523\ttotal: 850ms\tremaining: 1.65s\n",
      "34:\tlearn: 0.0181873\ttotal: 867ms\tremaining: 1.61s\n",
      "35:\tlearn: 0.0181370\ttotal: 883ms\tremaining: 1.57s\n",
      "36:\tlearn: 0.0180110\ttotal: 915ms\tremaining: 1.56s\n",
      "37:\tlearn: 0.0178809\ttotal: 944ms\tremaining: 1.54s\n",
      "38:\tlearn: 0.0177953\ttotal: 959ms\tremaining: 1.5s\n",
      "39:\tlearn: 0.0177195\ttotal: 976ms\tremaining: 1.46s\n",
      "40:\tlearn: 0.0175570\ttotal: 990ms\tremaining: 1.43s\n",
      "41:\tlearn: 0.0174864\ttotal: 1s\tremaining: 1.39s\n",
      "42:\tlearn: 0.0173691\ttotal: 1.02s\tremaining: 1.35s\n",
      "43:\tlearn: 0.0171520\ttotal: 1.05s\tremaining: 1.33s\n",
      "44:\tlearn: 0.0170884\ttotal: 1.07s\tremaining: 1.31s\n",
      "45:\tlearn: 0.0169727\ttotal: 1.14s\tremaining: 1.34s\n",
      "46:\tlearn: 0.0168225\ttotal: 1.17s\tremaining: 1.32s\n",
      "47:\tlearn: 0.0166641\ttotal: 1.19s\tremaining: 1.28s\n",
      "48:\tlearn: 0.0162512\ttotal: 1.2s\tremaining: 1.25s\n",
      "49:\tlearn: 0.0161785\ttotal: 1.22s\tremaining: 1.22s\n",
      "50:\tlearn: 0.0159215\ttotal: 1.23s\tremaining: 1.19s\n",
      "51:\tlearn: 0.0157901\ttotal: 1.25s\tremaining: 1.15s\n",
      "52:\tlearn: 0.0156786\ttotal: 1.26s\tremaining: 1.12s\n",
      "53:\tlearn: 0.0155806\ttotal: 1.28s\tremaining: 1.09s\n",
      "54:\tlearn: 0.0154184\ttotal: 1.3s\tremaining: 1.06s\n",
      "55:\tlearn: 0.0153735\ttotal: 1.31s\tremaining: 1.03s\n",
      "56:\tlearn: 0.0152726\ttotal: 1.33s\tremaining: 1s\n",
      "57:\tlearn: 0.0151409\ttotal: 1.35s\tremaining: 975ms\n",
      "58:\tlearn: 0.0150361\ttotal: 1.37s\tremaining: 952ms\n",
      "59:\tlearn: 0.0149764\ttotal: 1.39s\tremaining: 924ms\n",
      "60:\tlearn: 0.0149150\ttotal: 1.4s\tremaining: 895ms\n",
      "61:\tlearn: 0.0148347\ttotal: 1.42s\tremaining: 868ms\n",
      "62:\tlearn: 0.0148082\ttotal: 1.43s\tremaining: 841ms\n",
      "63:\tlearn: 0.0146394\ttotal: 1.45s\tremaining: 815ms\n",
      "64:\tlearn: 0.0145678\ttotal: 1.46s\tremaining: 788ms\n",
      "65:\tlearn: 0.0145217\ttotal: 1.48s\tremaining: 761ms\n",
      "66:\tlearn: 0.0144183\ttotal: 1.49s\tremaining: 735ms\n",
      "67:\tlearn: 0.0143552\ttotal: 1.51s\tremaining: 709ms\n",
      "68:\tlearn: 0.0143189\ttotal: 1.52s\tremaining: 683ms\n",
      "69:\tlearn: 0.0141744\ttotal: 1.53s\tremaining: 657ms\n",
      "70:\tlearn: 0.0140900\ttotal: 1.55s\tremaining: 633ms\n",
      "71:\tlearn: 0.0139824\ttotal: 1.57s\tremaining: 612ms\n",
      "72:\tlearn: 0.0139387\ttotal: 1.59s\tremaining: 587ms\n",
      "73:\tlearn: 0.0138633\ttotal: 1.6s\tremaining: 564ms\n",
      "74:\tlearn: 0.0138142\ttotal: 1.62s\tremaining: 540ms\n",
      "75:\tlearn: 0.0137607\ttotal: 1.63s\tremaining: 516ms\n",
      "76:\tlearn: 0.0137059\ttotal: 1.65s\tremaining: 492ms\n",
      "77:\tlearn: 0.0136143\ttotal: 1.66s\tremaining: 469ms\n",
      "78:\tlearn: 0.0135579\ttotal: 1.68s\tremaining: 446ms\n",
      "79:\tlearn: 0.0134050\ttotal: 1.69s\tremaining: 423ms\n",
      "80:\tlearn: 0.0133418\ttotal: 1.71s\tremaining: 401ms\n",
      "81:\tlearn: 0.0131779\ttotal: 1.73s\tremaining: 379ms\n",
      "82:\tlearn: 0.0131533\ttotal: 1.74s\tremaining: 357ms\n",
      "83:\tlearn: 0.0131034\ttotal: 1.76s\tremaining: 336ms\n",
      "84:\tlearn: 0.0130317\ttotal: 1.79s\tremaining: 316ms\n",
      "85:\tlearn: 0.0129286\ttotal: 1.81s\tremaining: 294ms\n",
      "86:\tlearn: 0.0128184\ttotal: 1.82s\tremaining: 272ms\n",
      "87:\tlearn: 0.0127336\ttotal: 1.83s\tremaining: 250ms\n",
      "88:\tlearn: 0.0126947\ttotal: 1.85s\tremaining: 229ms\n",
      "89:\tlearn: 0.0126561\ttotal: 1.87s\tremaining: 207ms\n",
      "90:\tlearn: 0.0126095\ttotal: 1.88s\tremaining: 186ms\n",
      "91:\tlearn: 0.0125145\ttotal: 1.9s\tremaining: 165ms\n",
      "92:\tlearn: 0.0123580\ttotal: 1.91s\tremaining: 144ms\n",
      "93:\tlearn: 0.0123113\ttotal: 1.93s\tremaining: 123ms\n",
      "94:\tlearn: 0.0122766\ttotal: 2.02s\tremaining: 107ms\n",
      "95:\tlearn: 0.0122456\ttotal: 2.06s\tremaining: 85.7ms\n",
      "96:\tlearn: 0.0121513\ttotal: 2.08s\tremaining: 64.4ms\n",
      "97:\tlearn: 0.0121002\ttotal: 2.1s\tremaining: 42.8ms\n",
      "98:\tlearn: 0.0120061\ttotal: 2.11s\tremaining: 21.3ms\n",
      "99:\tlearn: 0.0119664\ttotal: 2.13s\tremaining: 0us\n",
      "0 0.9076175040518638 0.7642585551330798 0.5810055865921788\n",
      "Learning rate set to 0.485422\n",
      "0:\tlearn: 0.0931438\ttotal: 12.7ms\tremaining: 1.26s\n",
      "1:\tlearn: 0.0427866\ttotal: 25.1ms\tremaining: 1.23s\n",
      "2:\tlearn: 0.0341395\ttotal: 36.7ms\tremaining: 1.19s\n",
      "3:\tlearn: 0.0310359\ttotal: 49.3ms\tremaining: 1.18s\n",
      "4:\tlearn: 0.0298134\ttotal: 61.5ms\tremaining: 1.17s\n",
      "5:\tlearn: 0.0292012\ttotal: 76.5ms\tremaining: 1.2s\n",
      "6:\tlearn: 0.0286439\ttotal: 106ms\tremaining: 1.4s\n",
      "7:\tlearn: 0.0280261\ttotal: 132ms\tremaining: 1.52s\n",
      "8:\tlearn: 0.0271450\ttotal: 152ms\tremaining: 1.53s\n",
      "9:\tlearn: 0.0267181\ttotal: 172ms\tremaining: 1.55s\n",
      "10:\tlearn: 0.0260686\ttotal: 194ms\tremaining: 1.57s\n",
      "11:\tlearn: 0.0257313\ttotal: 213ms\tremaining: 1.56s\n",
      "12:\tlearn: 0.0252356\ttotal: 239ms\tremaining: 1.6s\n",
      "13:\tlearn: 0.0249337\ttotal: 287ms\tremaining: 1.76s\n",
      "14:\tlearn: 0.0244793\ttotal: 386ms\tremaining: 2.19s\n",
      "15:\tlearn: 0.0241531\ttotal: 431ms\tremaining: 2.26s\n",
      "16:\tlearn: 0.0237553\ttotal: 462ms\tremaining: 2.26s\n",
      "17:\tlearn: 0.0234791\ttotal: 490ms\tremaining: 2.23s\n",
      "18:\tlearn: 0.0232450\ttotal: 512ms\tremaining: 2.18s\n",
      "19:\tlearn: 0.0229918\ttotal: 530ms\tremaining: 2.12s\n",
      "20:\tlearn: 0.0227846\ttotal: 548ms\tremaining: 2.06s\n",
      "21:\tlearn: 0.0225836\ttotal: 562ms\tremaining: 1.99s\n",
      "22:\tlearn: 0.0221592\ttotal: 577ms\tremaining: 1.93s\n",
      "23:\tlearn: 0.0218390\ttotal: 592ms\tremaining: 1.88s\n",
      "24:\tlearn: 0.0216567\ttotal: 607ms\tremaining: 1.82s\n",
      "25:\tlearn: 0.0214333\ttotal: 621ms\tremaining: 1.77s\n",
      "26:\tlearn: 0.0211829\ttotal: 636ms\tremaining: 1.72s\n",
      "27:\tlearn: 0.0210412\ttotal: 649ms\tremaining: 1.67s\n",
      "28:\tlearn: 0.0206570\ttotal: 667ms\tremaining: 1.63s\n",
      "29:\tlearn: 0.0204803\ttotal: 692ms\tremaining: 1.61s\n",
      "30:\tlearn: 0.0202665\ttotal: 722ms\tremaining: 1.61s\n",
      "31:\tlearn: 0.0200964\ttotal: 742ms\tremaining: 1.58s\n",
      "32:\tlearn: 0.0199773\ttotal: 762ms\tremaining: 1.55s\n",
      "33:\tlearn: 0.0198556\ttotal: 791ms\tremaining: 1.53s\n",
      "34:\tlearn: 0.0192440\ttotal: 823ms\tremaining: 1.53s\n",
      "35:\tlearn: 0.0191466\ttotal: 843ms\tremaining: 1.5s\n",
      "36:\tlearn: 0.0189788\ttotal: 870ms\tremaining: 1.48s\n",
      "37:\tlearn: 0.0187859\ttotal: 902ms\tremaining: 1.47s\n",
      "38:\tlearn: 0.0183054\ttotal: 937ms\tremaining: 1.47s\n",
      "39:\tlearn: 0.0182247\ttotal: 965ms\tremaining: 1.45s\n",
      "40:\tlearn: 0.0180448\ttotal: 986ms\tremaining: 1.42s\n",
      "41:\tlearn: 0.0179702\ttotal: 1.02s\tremaining: 1.41s\n",
      "42:\tlearn: 0.0178510\ttotal: 1.05s\tremaining: 1.39s\n",
      "43:\tlearn: 0.0177364\ttotal: 1.09s\tremaining: 1.38s\n",
      "44:\tlearn: 0.0176634\ttotal: 1.11s\tremaining: 1.35s\n",
      "45:\tlearn: 0.0174449\ttotal: 1.14s\tremaining: 1.33s\n",
      "46:\tlearn: 0.0173382\ttotal: 1.16s\tremaining: 1.31s\n",
      "47:\tlearn: 0.0172727\ttotal: 1.18s\tremaining: 1.28s\n",
      "48:\tlearn: 0.0170953\ttotal: 1.2s\tremaining: 1.25s\n",
      "49:\tlearn: 0.0170117\ttotal: 1.22s\tremaining: 1.22s\n",
      "50:\tlearn: 0.0169225\ttotal: 1.23s\tremaining: 1.19s\n",
      "51:\tlearn: 0.0167330\ttotal: 1.25s\tremaining: 1.15s\n",
      "52:\tlearn: 0.0166511\ttotal: 1.26s\tremaining: 1.12s\n",
      "53:\tlearn: 0.0166257\ttotal: 1.28s\tremaining: 1.09s\n",
      "54:\tlearn: 0.0162813\ttotal: 1.31s\tremaining: 1.07s\n",
      "55:\tlearn: 0.0159586\ttotal: 1.35s\tremaining: 1.06s\n",
      "56:\tlearn: 0.0159130\ttotal: 1.37s\tremaining: 1.04s\n",
      "57:\tlearn: 0.0153857\ttotal: 1.39s\tremaining: 1.01s\n",
      "58:\tlearn: 0.0153523\ttotal: 1.41s\tremaining: 978ms\n",
      "59:\tlearn: 0.0153050\ttotal: 1.43s\tremaining: 950ms\n",
      "60:\tlearn: 0.0152235\ttotal: 1.44s\tremaining: 923ms\n",
      "61:\tlearn: 0.0151846\ttotal: 1.46s\tremaining: 895ms\n",
      "62:\tlearn: 0.0150292\ttotal: 1.48s\tremaining: 869ms\n",
      "63:\tlearn: 0.0149648\ttotal: 1.5s\tremaining: 844ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64:\tlearn: 0.0148892\ttotal: 1.53s\tremaining: 822ms\n",
      "65:\tlearn: 0.0148306\ttotal: 1.56s\tremaining: 802ms\n",
      "66:\tlearn: 0.0147261\ttotal: 1.58s\tremaining: 776ms\n",
      "67:\tlearn: 0.0146149\ttotal: 1.6s\tremaining: 751ms\n",
      "68:\tlearn: 0.0144795\ttotal: 1.62s\tremaining: 727ms\n",
      "69:\tlearn: 0.0144291\ttotal: 1.64s\tremaining: 702ms\n",
      "70:\tlearn: 0.0143365\ttotal: 1.66s\tremaining: 677ms\n",
      "71:\tlearn: 0.0142473\ttotal: 1.67s\tremaining: 651ms\n",
      "72:\tlearn: 0.0142008\ttotal: 1.69s\tremaining: 624ms\n",
      "73:\tlearn: 0.0141567\ttotal: 1.7s\tremaining: 599ms\n",
      "74:\tlearn: 0.0141372\ttotal: 1.73s\tremaining: 576ms\n",
      "75:\tlearn: 0.0140220\ttotal: 1.75s\tremaining: 553ms\n",
      "76:\tlearn: 0.0139798\ttotal: 1.78s\tremaining: 532ms\n",
      "77:\tlearn: 0.0139566\ttotal: 1.8s\tremaining: 508ms\n",
      "78:\tlearn: 0.0136721\ttotal: 1.82s\tremaining: 484ms\n",
      "79:\tlearn: 0.0135549\ttotal: 1.84s\tremaining: 460ms\n",
      "80:\tlearn: 0.0134225\ttotal: 1.88s\tremaining: 440ms\n",
      "81:\tlearn: 0.0133879\ttotal: 1.9s\tremaining: 418ms\n",
      "82:\tlearn: 0.0132761\ttotal: 1.94s\tremaining: 397ms\n",
      "83:\tlearn: 0.0132206\ttotal: 1.97s\tremaining: 374ms\n",
      "84:\tlearn: 0.0131583\ttotal: 1.99s\tremaining: 351ms\n",
      "85:\tlearn: 0.0131209\ttotal: 2.01s\tremaining: 327ms\n",
      "86:\tlearn: 0.0130641\ttotal: 2.03s\tremaining: 303ms\n",
      "87:\tlearn: 0.0130167\ttotal: 2.05s\tremaining: 279ms\n",
      "88:\tlearn: 0.0129702\ttotal: 2.07s\tremaining: 256ms\n",
      "89:\tlearn: 0.0129513\ttotal: 2.09s\tremaining: 232ms\n",
      "90:\tlearn: 0.0129143\ttotal: 2.11s\tremaining: 209ms\n",
      "91:\tlearn: 0.0128365\ttotal: 2.13s\tremaining: 185ms\n",
      "92:\tlearn: 0.0127668\ttotal: 2.15s\tremaining: 162ms\n",
      "93:\tlearn: 0.0126967\ttotal: 2.19s\tremaining: 140ms\n",
      "94:\tlearn: 0.0125060\ttotal: 2.22s\tremaining: 117ms\n",
      "95:\tlearn: 0.0124695\ttotal: 2.24s\tremaining: 93.4ms\n",
      "96:\tlearn: 0.0123259\ttotal: 2.29s\tremaining: 70.8ms\n",
      "97:\tlearn: 0.0122910\ttotal: 2.31s\tremaining: 47.1ms\n",
      "98:\tlearn: 0.0122598\ttotal: 2.33s\tremaining: 23.5ms\n",
      "99:\tlearn: 0.0121591\ttotal: 2.35s\tremaining: 0us\n",
      "1 0.9207606973058637 0.7550200803212851 0.4245810055865922\n",
      "Learning rate set to 0.485422\n",
      "0:\tlearn: 0.0941797\ttotal: 17.8ms\tremaining: 1.76s\n",
      "1:\tlearn: 0.0443682\ttotal: 35.6ms\tremaining: 1.75s\n",
      "2:\tlearn: 0.0361595\ttotal: 55.5ms\tremaining: 1.79s\n",
      "3:\tlearn: 0.0327907\ttotal: 79.2ms\tremaining: 1.9s\n",
      "4:\tlearn: 0.0312643\ttotal: 120ms\tremaining: 2.28s\n",
      "5:\tlearn: 0.0304904\ttotal: 160ms\tremaining: 2.5s\n",
      "6:\tlearn: 0.0299252\ttotal: 198ms\tremaining: 2.63s\n",
      "7:\tlearn: 0.0289042\ttotal: 230ms\tremaining: 2.65s\n",
      "8:\tlearn: 0.0279739\ttotal: 248ms\tremaining: 2.51s\n",
      "9:\tlearn: 0.0271471\ttotal: 270ms\tremaining: 2.43s\n",
      "10:\tlearn: 0.0266451\ttotal: 304ms\tremaining: 2.46s\n",
      "11:\tlearn: 0.0260802\ttotal: 337ms\tremaining: 2.47s\n",
      "12:\tlearn: 0.0256748\ttotal: 368ms\tremaining: 2.46s\n",
      "13:\tlearn: 0.0247779\ttotal: 391ms\tremaining: 2.4s\n",
      "14:\tlearn: 0.0244821\ttotal: 414ms\tremaining: 2.35s\n",
      "15:\tlearn: 0.0242628\ttotal: 437ms\tremaining: 2.29s\n",
      "16:\tlearn: 0.0240156\ttotal: 458ms\tremaining: 2.23s\n",
      "17:\tlearn: 0.0237518\ttotal: 476ms\tremaining: 2.17s\n",
      "18:\tlearn: 0.0235930\ttotal: 492ms\tremaining: 2.1s\n",
      "19:\tlearn: 0.0233515\ttotal: 510ms\tremaining: 2.04s\n",
      "20:\tlearn: 0.0231289\ttotal: 531ms\tremaining: 2s\n",
      "21:\tlearn: 0.0229126\ttotal: 574ms\tremaining: 2.03s\n",
      "22:\tlearn: 0.0226106\ttotal: 600ms\tremaining: 2.01s\n",
      "23:\tlearn: 0.0223116\ttotal: 628ms\tremaining: 1.99s\n",
      "24:\tlearn: 0.0219351\ttotal: 670ms\tremaining: 2.01s\n",
      "25:\tlearn: 0.0218132\ttotal: 697ms\tremaining: 1.99s\n",
      "26:\tlearn: 0.0216743\ttotal: 718ms\tremaining: 1.94s\n",
      "27:\tlearn: 0.0214556\ttotal: 747ms\tremaining: 1.92s\n",
      "28:\tlearn: 0.0208434\ttotal: 767ms\tremaining: 1.88s\n",
      "29:\tlearn: 0.0207218\ttotal: 784ms\tremaining: 1.83s\n",
      "30:\tlearn: 0.0205739\ttotal: 813ms\tremaining: 1.81s\n",
      "31:\tlearn: 0.0203000\ttotal: 831ms\tremaining: 1.76s\n",
      "32:\tlearn: 0.0200129\ttotal: 849ms\tremaining: 1.72s\n",
      "33:\tlearn: 0.0198730\ttotal: 865ms\tremaining: 1.68s\n",
      "34:\tlearn: 0.0197629\ttotal: 882ms\tremaining: 1.64s\n",
      "35:\tlearn: 0.0194277\ttotal: 902ms\tremaining: 1.6s\n",
      "36:\tlearn: 0.0193165\ttotal: 920ms\tremaining: 1.57s\n",
      "37:\tlearn: 0.0190004\ttotal: 938ms\tremaining: 1.53s\n",
      "38:\tlearn: 0.0189090\ttotal: 957ms\tremaining: 1.5s\n",
      "39:\tlearn: 0.0188133\ttotal: 988ms\tremaining: 1.48s\n",
      "40:\tlearn: 0.0185342\ttotal: 1.01s\tremaining: 1.46s\n",
      "41:\tlearn: 0.0182484\ttotal: 1.03s\tremaining: 1.43s\n",
      "42:\tlearn: 0.0181093\ttotal: 1.05s\tremaining: 1.4s\n",
      "43:\tlearn: 0.0180218\ttotal: 1.07s\tremaining: 1.36s\n",
      "44:\tlearn: 0.0178659\ttotal: 1.09s\tremaining: 1.33s\n",
      "45:\tlearn: 0.0177054\ttotal: 1.11s\tremaining: 1.3s\n",
      "46:\tlearn: 0.0175708\ttotal: 1.12s\tremaining: 1.27s\n",
      "47:\tlearn: 0.0173721\ttotal: 1.14s\tremaining: 1.23s\n",
      "48:\tlearn: 0.0172631\ttotal: 1.16s\tremaining: 1.2s\n",
      "49:\tlearn: 0.0171719\ttotal: 1.17s\tremaining: 1.17s\n",
      "50:\tlearn: 0.0171007\ttotal: 1.19s\tremaining: 1.15s\n",
      "51:\tlearn: 0.0169817\ttotal: 1.22s\tremaining: 1.12s\n",
      "52:\tlearn: 0.0169118\ttotal: 1.24s\tremaining: 1.1s\n",
      "53:\tlearn: 0.0168351\ttotal: 1.25s\tremaining: 1.06s\n",
      "54:\tlearn: 0.0166634\ttotal: 1.27s\tremaining: 1.04s\n",
      "55:\tlearn: 0.0166222\ttotal: 1.28s\tremaining: 1.01s\n",
      "56:\tlearn: 0.0163131\ttotal: 1.3s\tremaining: 982ms\n",
      "57:\tlearn: 0.0162258\ttotal: 1.33s\tremaining: 961ms\n",
      "58:\tlearn: 0.0159680\ttotal: 1.36s\tremaining: 944ms\n",
      "59:\tlearn: 0.0158453\ttotal: 1.39s\tremaining: 924ms\n",
      "60:\tlearn: 0.0157090\ttotal: 1.4s\tremaining: 898ms\n",
      "61:\tlearn: 0.0156710\ttotal: 1.43s\tremaining: 875ms\n",
      "62:\tlearn: 0.0156063\ttotal: 1.45s\tremaining: 850ms\n",
      "63:\tlearn: 0.0155397\ttotal: 1.46s\tremaining: 824ms\n",
      "64:\tlearn: 0.0154008\ttotal: 1.48s\tremaining: 796ms\n",
      "65:\tlearn: 0.0152750\ttotal: 1.49s\tremaining: 770ms\n",
      "66:\tlearn: 0.0151946\ttotal: 1.51s\tremaining: 743ms\n",
      "67:\tlearn: 0.0151513\ttotal: 1.53s\tremaining: 721ms\n",
      "68:\tlearn: 0.0150061\ttotal: 1.55s\tremaining: 697ms\n",
      "69:\tlearn: 0.0149441\ttotal: 1.57s\tremaining: 672ms\n",
      "70:\tlearn: 0.0148316\ttotal: 1.59s\tremaining: 648ms\n",
      "71:\tlearn: 0.0148092\ttotal: 1.62s\tremaining: 631ms\n",
      "72:\tlearn: 0.0146788\ttotal: 1.66s\tremaining: 613ms\n",
      "73:\tlearn: 0.0146177\ttotal: 1.68s\tremaining: 590ms\n",
      "74:\tlearn: 0.0145269\ttotal: 1.7s\tremaining: 566ms\n",
      "75:\tlearn: 0.0144567\ttotal: 1.72s\tremaining: 542ms\n",
      "76:\tlearn: 0.0144000\ttotal: 1.73s\tremaining: 517ms\n",
      "77:\tlearn: 0.0142660\ttotal: 1.75s\tremaining: 493ms\n",
      "78:\tlearn: 0.0141901\ttotal: 1.76s\tremaining: 469ms\n",
      "79:\tlearn: 0.0141346\ttotal: 1.78s\tremaining: 446ms\n",
      "80:\tlearn: 0.0140960\ttotal: 1.8s\tremaining: 422ms\n",
      "81:\tlearn: 0.0139664\ttotal: 1.81s\tremaining: 398ms\n",
      "82:\tlearn: 0.0138959\ttotal: 1.83s\tremaining: 375ms\n",
      "83:\tlearn: 0.0137754\ttotal: 1.85s\tremaining: 353ms\n",
      "84:\tlearn: 0.0137078\ttotal: 1.87s\tremaining: 330ms\n",
      "85:\tlearn: 0.0136315\ttotal: 1.89s\tremaining: 307ms\n",
      "86:\tlearn: 0.0135467\ttotal: 1.9s\tremaining: 284ms\n",
      "87:\tlearn: 0.0134426\ttotal: 1.93s\tremaining: 264ms\n",
      "88:\tlearn: 0.0133352\ttotal: 1.96s\tremaining: 242ms\n",
      "89:\tlearn: 0.0132786\ttotal: 1.99s\tremaining: 221ms\n",
      "90:\tlearn: 0.0132372\ttotal: 2.02s\tremaining: 200ms\n",
      "91:\tlearn: 0.0131758\ttotal: 2.06s\tremaining: 180ms\n",
      "92:\tlearn: 0.0131106\ttotal: 2.08s\tremaining: 157ms\n",
      "93:\tlearn: 0.0130276\ttotal: 2.11s\tremaining: 134ms\n",
      "94:\tlearn: 0.0129245\ttotal: 2.12s\tremaining: 112ms\n",
      "95:\tlearn: 0.0128820\ttotal: 2.14s\tremaining: 89.1ms\n",
      "96:\tlearn: 0.0128107\ttotal: 2.15s\tremaining: 66.6ms\n",
      "97:\tlearn: 0.0127777\ttotal: 2.17s\tremaining: 44.2ms\n",
      "98:\tlearn: 0.0127386\ttotal: 2.18s\tremaining: 22ms\n",
      "99:\tlearn: 0.0127018\ttotal: 2.19s\tremaining: 0us\n",
      "2 0.9126984126984127 0.776 0.5363128491620112\n",
      "Learning rate set to 0.485422\n",
      "0:\tlearn: 0.0904584\ttotal: 17.1ms\tremaining: 1.69s\n",
      "1:\tlearn: 0.0416073\ttotal: 34.1ms\tremaining: 1.67s\n",
      "2:\tlearn: 0.0329187\ttotal: 49.7ms\tremaining: 1.61s\n",
      "3:\tlearn: 0.0305624\ttotal: 114ms\tremaining: 2.73s\n",
      "4:\tlearn: 0.0293834\ttotal: 143ms\tremaining: 2.71s\n",
      "5:\tlearn: 0.0278633\ttotal: 165ms\tremaining: 2.59s\n",
      "6:\tlearn: 0.0273196\ttotal: 188ms\tremaining: 2.49s\n",
      "7:\tlearn: 0.0262706\ttotal: 209ms\tremaining: 2.4s\n",
      "8:\tlearn: 0.0259525\ttotal: 228ms\tremaining: 2.3s\n",
      "9:\tlearn: 0.0253650\ttotal: 251ms\tremaining: 2.26s\n",
      "10:\tlearn: 0.0249163\ttotal: 277ms\tremaining: 2.24s\n",
      "11:\tlearn: 0.0246381\ttotal: 298ms\tremaining: 2.18s\n",
      "12:\tlearn: 0.0242345\ttotal: 317ms\tremaining: 2.12s\n",
      "13:\tlearn: 0.0239137\ttotal: 335ms\tremaining: 2.06s\n",
      "14:\tlearn: 0.0233804\ttotal: 354ms\tremaining: 2.01s\n",
      "15:\tlearn: 0.0230799\ttotal: 373ms\tremaining: 1.96s\n",
      "16:\tlearn: 0.0227477\ttotal: 392ms\tremaining: 1.92s\n",
      "17:\tlearn: 0.0225341\ttotal: 410ms\tremaining: 1.87s\n",
      "18:\tlearn: 0.0223794\ttotal: 429ms\tremaining: 1.83s\n",
      "19:\tlearn: 0.0212173\ttotal: 451ms\tremaining: 1.8s\n",
      "20:\tlearn: 0.0208531\ttotal: 466ms\tremaining: 1.75s\n",
      "21:\tlearn: 0.0205989\ttotal: 482ms\tremaining: 1.71s\n",
      "22:\tlearn: 0.0204274\ttotal: 498ms\tremaining: 1.67s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23:\tlearn: 0.0200263\ttotal: 521ms\tremaining: 1.65s\n",
      "24:\tlearn: 0.0198533\ttotal: 541ms\tremaining: 1.62s\n",
      "25:\tlearn: 0.0195906\ttotal: 567ms\tremaining: 1.61s\n",
      "26:\tlearn: 0.0193535\ttotal: 590ms\tremaining: 1.6s\n",
      "27:\tlearn: 0.0191668\ttotal: 608ms\tremaining: 1.56s\n",
      "28:\tlearn: 0.0190329\ttotal: 623ms\tremaining: 1.52s\n",
      "29:\tlearn: 0.0189421\ttotal: 641ms\tremaining: 1.49s\n",
      "30:\tlearn: 0.0186161\ttotal: 662ms\tremaining: 1.47s\n",
      "31:\tlearn: 0.0182174\ttotal: 680ms\tremaining: 1.45s\n",
      "32:\tlearn: 0.0179313\ttotal: 715ms\tremaining: 1.45s\n",
      "33:\tlearn: 0.0177401\ttotal: 753ms\tremaining: 1.46s\n",
      "34:\tlearn: 0.0176199\ttotal: 783ms\tremaining: 1.45s\n",
      "35:\tlearn: 0.0175509\ttotal: 808ms\tremaining: 1.44s\n",
      "36:\tlearn: 0.0173934\ttotal: 829ms\tremaining: 1.41s\n",
      "37:\tlearn: 0.0172743\ttotal: 851ms\tremaining: 1.39s\n",
      "38:\tlearn: 0.0171354\ttotal: 883ms\tremaining: 1.38s\n",
      "39:\tlearn: 0.0170017\ttotal: 910ms\tremaining: 1.36s\n",
      "40:\tlearn: 0.0169494\ttotal: 926ms\tremaining: 1.33s\n",
      "41:\tlearn: 0.0168712\ttotal: 944ms\tremaining: 1.3s\n",
      "42:\tlearn: 0.0166507\ttotal: 977ms\tremaining: 1.29s\n",
      "43:\tlearn: 0.0166043\ttotal: 1.01s\tremaining: 1.29s\n",
      "44:\tlearn: 0.0164043\ttotal: 1.03s\tremaining: 1.26s\n",
      "45:\tlearn: 0.0162610\ttotal: 1.05s\tremaining: 1.23s\n",
      "46:\tlearn: 0.0161664\ttotal: 1.07s\tremaining: 1.2s\n",
      "47:\tlearn: 0.0159076\ttotal: 1.08s\tremaining: 1.17s\n",
      "48:\tlearn: 0.0157600\ttotal: 1.1s\tremaining: 1.14s\n",
      "49:\tlearn: 0.0156626\ttotal: 1.11s\tremaining: 1.11s\n",
      "50:\tlearn: 0.0155447\ttotal: 1.14s\tremaining: 1.1s\n",
      "51:\tlearn: 0.0154775\ttotal: 1.16s\tremaining: 1.07s\n",
      "52:\tlearn: 0.0153762\ttotal: 1.18s\tremaining: 1.04s\n",
      "53:\tlearn: 0.0151863\ttotal: 1.21s\tremaining: 1.03s\n",
      "54:\tlearn: 0.0151202\ttotal: 1.23s\tremaining: 1s\n",
      "55:\tlearn: 0.0149743\ttotal: 1.25s\tremaining: 986ms\n",
      "56:\tlearn: 0.0149250\ttotal: 1.27s\tremaining: 961ms\n",
      "57:\tlearn: 0.0148731\ttotal: 1.29s\tremaining: 935ms\n",
      "58:\tlearn: 0.0147527\ttotal: 1.31s\tremaining: 910ms\n",
      "59:\tlearn: 0.0146737\ttotal: 1.32s\tremaining: 884ms\n",
      "60:\tlearn: 0.0145837\ttotal: 1.34s\tremaining: 857ms\n",
      "61:\tlearn: 0.0144917\ttotal: 1.35s\tremaining: 830ms\n",
      "62:\tlearn: 0.0143307\ttotal: 1.37s\tremaining: 804ms\n",
      "63:\tlearn: 0.0142957\ttotal: 1.39s\tremaining: 779ms\n",
      "64:\tlearn: 0.0141991\ttotal: 1.4s\tremaining: 756ms\n",
      "65:\tlearn: 0.0140860\ttotal: 1.42s\tremaining: 732ms\n",
      "66:\tlearn: 0.0140110\ttotal: 1.44s\tremaining: 707ms\n",
      "67:\tlearn: 0.0139386\ttotal: 1.45s\tremaining: 682ms\n",
      "68:\tlearn: 0.0139078\ttotal: 1.46s\tremaining: 658ms\n",
      "69:\tlearn: 0.0135278\ttotal: 1.48s\tremaining: 635ms\n",
      "70:\tlearn: 0.0134689\ttotal: 1.5s\tremaining: 612ms\n",
      "71:\tlearn: 0.0133792\ttotal: 1.51s\tremaining: 589ms\n",
      "72:\tlearn: 0.0133015\ttotal: 1.53s\tremaining: 566ms\n",
      "73:\tlearn: 0.0132133\ttotal: 1.54s\tremaining: 543ms\n",
      "74:\tlearn: 0.0131079\ttotal: 1.56s\tremaining: 520ms\n",
      "75:\tlearn: 0.0130445\ttotal: 1.57s\tremaining: 497ms\n",
      "76:\tlearn: 0.0129553\ttotal: 1.6s\tremaining: 478ms\n",
      "77:\tlearn: 0.0129235\ttotal: 1.61s\tremaining: 456ms\n",
      "78:\tlearn: 0.0127911\ttotal: 1.63s\tremaining: 433ms\n",
      "79:\tlearn: 0.0127136\ttotal: 1.64s\tremaining: 411ms\n",
      "80:\tlearn: 0.0126435\ttotal: 1.66s\tremaining: 389ms\n",
      "81:\tlearn: 0.0126061\ttotal: 1.67s\tremaining: 367ms\n",
      "82:\tlearn: 0.0125344\ttotal: 1.69s\tremaining: 345ms\n",
      "83:\tlearn: 0.0124581\ttotal: 1.7s\tremaining: 324ms\n",
      "84:\tlearn: 0.0124301\ttotal: 1.71s\tremaining: 302ms\n",
      "85:\tlearn: 0.0123926\ttotal: 1.73s\tremaining: 281ms\n",
      "86:\tlearn: 0.0123413\ttotal: 1.74s\tremaining: 260ms\n",
      "87:\tlearn: 0.0122106\ttotal: 1.76s\tremaining: 241ms\n",
      "88:\tlearn: 0.0121596\ttotal: 1.78s\tremaining: 220ms\n",
      "89:\tlearn: 0.0119715\ttotal: 1.8s\tremaining: 200ms\n",
      "90:\tlearn: 0.0119323\ttotal: 1.83s\tremaining: 181ms\n",
      "91:\tlearn: 0.0118954\ttotal: 1.84s\tremaining: 160ms\n",
      "92:\tlearn: 0.0118656\ttotal: 1.86s\tremaining: 140ms\n",
      "93:\tlearn: 0.0118046\ttotal: 1.88s\tremaining: 120ms\n",
      "94:\tlearn: 0.0117826\ttotal: 1.9s\tremaining: 99.8ms\n",
      "95:\tlearn: 0.0117534\ttotal: 1.91s\tremaining: 79.8ms\n",
      "96:\tlearn: 0.0116624\ttotal: 1.94s\tremaining: 60.1ms\n",
      "97:\tlearn: 0.0115979\ttotal: 1.96s\tremaining: 40ms\n",
      "98:\tlearn: 0.0115658\ttotal: 1.98s\tremaining: 20ms\n",
      "99:\tlearn: 0.0114565\ttotal: 1.99s\tremaining: 0us\n",
      "3 0.9158415841584159 0.7664233576642335 0.5027932960893855\n",
      "Learning rate set to 0.485422\n",
      "0:\tlearn: 0.0902220\ttotal: 12.7ms\tremaining: 1.26s\n",
      "1:\tlearn: 0.0402993\ttotal: 25ms\tremaining: 1.22s\n",
      "2:\tlearn: 0.0328358\ttotal: 37.5ms\tremaining: 1.21s\n",
      "3:\tlearn: 0.0303388\ttotal: 50.7ms\tremaining: 1.22s\n",
      "4:\tlearn: 0.0280099\ttotal: 72.4ms\tremaining: 1.38s\n",
      "5:\tlearn: 0.0269598\ttotal: 101ms\tremaining: 1.58s\n",
      "6:\tlearn: 0.0264450\ttotal: 118ms\tremaining: 1.56s\n",
      "7:\tlearn: 0.0255318\ttotal: 133ms\tremaining: 1.53s\n",
      "8:\tlearn: 0.0249250\ttotal: 148ms\tremaining: 1.5s\n",
      "9:\tlearn: 0.0243340\ttotal: 164ms\tremaining: 1.47s\n",
      "10:\tlearn: 0.0239529\ttotal: 181ms\tremaining: 1.47s\n",
      "11:\tlearn: 0.0232530\ttotal: 200ms\tremaining: 1.47s\n",
      "12:\tlearn: 0.0230097\ttotal: 217ms\tremaining: 1.45s\n",
      "13:\tlearn: 0.0227544\ttotal: 236ms\tremaining: 1.45s\n",
      "14:\tlearn: 0.0225927\ttotal: 255ms\tremaining: 1.45s\n",
      "15:\tlearn: 0.0223310\ttotal: 281ms\tremaining: 1.47s\n",
      "16:\tlearn: 0.0221634\ttotal: 312ms\tremaining: 1.52s\n",
      "17:\tlearn: 0.0218520\ttotal: 355ms\tremaining: 1.62s\n",
      "18:\tlearn: 0.0216677\ttotal: 385ms\tremaining: 1.64s\n",
      "19:\tlearn: 0.0213832\ttotal: 403ms\tremaining: 1.61s\n",
      "20:\tlearn: 0.0211002\ttotal: 431ms\tremaining: 1.62s\n",
      "21:\tlearn: 0.0210121\ttotal: 472ms\tremaining: 1.68s\n",
      "22:\tlearn: 0.0206264\ttotal: 615ms\tremaining: 2.06s\n",
      "23:\tlearn: 0.0203266\ttotal: 651ms\tremaining: 2.06s\n",
      "24:\tlearn: 0.0202156\ttotal: 684ms\tremaining: 2.05s\n",
      "25:\tlearn: 0.0199884\ttotal: 737ms\tremaining: 2.1s\n",
      "26:\tlearn: 0.0198505\ttotal: 768ms\tremaining: 2.08s\n",
      "27:\tlearn: 0.0196965\ttotal: 795ms\tremaining: 2.04s\n",
      "28:\tlearn: 0.0194005\ttotal: 857ms\tremaining: 2.1s\n",
      "29:\tlearn: 0.0191788\ttotal: 891ms\tremaining: 2.08s\n",
      "30:\tlearn: 0.0190086\ttotal: 910ms\tremaining: 2.02s\n",
      "31:\tlearn: 0.0188961\ttotal: 927ms\tremaining: 1.97s\n",
      "32:\tlearn: 0.0187772\ttotal: 949ms\tremaining: 1.93s\n",
      "33:\tlearn: 0.0186350\ttotal: 1.26s\tremaining: 2.44s\n",
      "34:\tlearn: 0.0183091\ttotal: 1.28s\tremaining: 2.39s\n",
      "35:\tlearn: 0.0181742\ttotal: 1.32s\tremaining: 2.34s\n",
      "36:\tlearn: 0.0180656\ttotal: 1.37s\tremaining: 2.33s\n",
      "37:\tlearn: 0.0179678\ttotal: 1.43s\tremaining: 2.33s\n",
      "38:\tlearn: 0.0177772\ttotal: 1.55s\tremaining: 2.42s\n",
      "39:\tlearn: 0.0177005\ttotal: 1.6s\tremaining: 2.4s\n",
      "40:\tlearn: 0.0174387\ttotal: 1.63s\tremaining: 2.34s\n",
      "41:\tlearn: 0.0172715\ttotal: 1.65s\tremaining: 2.27s\n",
      "42:\tlearn: 0.0170805\ttotal: 1.66s\tremaining: 2.21s\n",
      "43:\tlearn: 0.0170208\ttotal: 1.71s\tremaining: 2.18s\n",
      "44:\tlearn: 0.0169433\ttotal: 1.81s\tremaining: 2.21s\n",
      "45:\tlearn: 0.0168152\ttotal: 1.85s\tremaining: 2.17s\n",
      "46:\tlearn: 0.0167515\ttotal: 1.89s\tremaining: 2.13s\n",
      "47:\tlearn: 0.0166305\ttotal: 1.92s\tremaining: 2.08s\n",
      "48:\tlearn: 0.0162407\ttotal: 1.96s\tremaining: 2.04s\n",
      "49:\tlearn: 0.0161559\ttotal: 1.99s\tremaining: 1.99s\n",
      "50:\tlearn: 0.0160089\ttotal: 2.31s\tremaining: 2.22s\n",
      "51:\tlearn: 0.0157765\ttotal: 2.37s\tremaining: 2.19s\n",
      "52:\tlearn: 0.0156825\ttotal: 2.44s\tremaining: 2.16s\n",
      "53:\tlearn: 0.0156282\ttotal: 2.52s\tremaining: 2.15s\n",
      "54:\tlearn: 0.0155659\ttotal: 2.55s\tremaining: 2.09s\n",
      "55:\tlearn: 0.0154943\ttotal: 2.57s\tremaining: 2.02s\n",
      "56:\tlearn: 0.0154072\ttotal: 2.6s\tremaining: 1.96s\n",
      "57:\tlearn: 0.0151907\ttotal: 2.61s\tremaining: 1.89s\n",
      "58:\tlearn: 0.0151215\ttotal: 2.63s\tremaining: 1.83s\n",
      "59:\tlearn: 0.0150358\ttotal: 2.65s\tremaining: 1.76s\n",
      "60:\tlearn: 0.0149950\ttotal: 2.67s\tremaining: 1.71s\n",
      "61:\tlearn: 0.0147844\ttotal: 2.69s\tremaining: 1.65s\n",
      "62:\tlearn: 0.0147375\ttotal: 2.71s\tremaining: 1.59s\n",
      "63:\tlearn: 0.0146897\ttotal: 2.73s\tremaining: 1.54s\n",
      "64:\tlearn: 0.0146730\ttotal: 2.75s\tremaining: 1.48s\n",
      "65:\tlearn: 0.0145276\ttotal: 2.78s\tremaining: 1.43s\n",
      "66:\tlearn: 0.0143252\ttotal: 2.8s\tremaining: 1.38s\n",
      "67:\tlearn: 0.0142413\ttotal: 2.81s\tremaining: 1.32s\n",
      "68:\tlearn: 0.0141567\ttotal: 2.83s\tremaining: 1.27s\n",
      "69:\tlearn: 0.0138572\ttotal: 2.85s\tremaining: 1.22s\n",
      "70:\tlearn: 0.0138018\ttotal: 2.86s\tremaining: 1.17s\n",
      "71:\tlearn: 0.0137590\ttotal: 2.88s\tremaining: 1.12s\n",
      "72:\tlearn: 0.0137323\ttotal: 2.9s\tremaining: 1.07s\n",
      "73:\tlearn: 0.0136841\ttotal: 2.92s\tremaining: 1.03s\n",
      "74:\tlearn: 0.0135761\ttotal: 2.94s\tremaining: 980ms\n",
      "75:\tlearn: 0.0135178\ttotal: 2.96s\tremaining: 936ms\n",
      "76:\tlearn: 0.0134632\ttotal: 2.99s\tremaining: 892ms\n",
      "77:\tlearn: 0.0133991\ttotal: 3s\tremaining: 847ms\n",
      "78:\tlearn: 0.0133739\ttotal: 3.02s\tremaining: 802ms\n",
      "79:\tlearn: 0.0133287\ttotal: 3.03s\tremaining: 758ms\n",
      "80:\tlearn: 0.0132540\ttotal: 3.05s\tremaining: 715ms\n",
      "81:\tlearn: 0.0131807\ttotal: 3.06s\tremaining: 672ms\n",
      "82:\tlearn: 0.0131458\ttotal: 3.07s\tremaining: 630ms\n",
      "83:\tlearn: 0.0130923\ttotal: 3.09s\tremaining: 588ms\n",
      "84:\tlearn: 0.0130406\ttotal: 3.1s\tremaining: 548ms\n",
      "85:\tlearn: 0.0127436\ttotal: 3.13s\tremaining: 510ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86:\tlearn: 0.0126604\ttotal: 3.15s\tremaining: 472ms\n",
      "87:\tlearn: 0.0126290\ttotal: 3.19s\tremaining: 435ms\n",
      "88:\tlearn: 0.0125561\ttotal: 3.21s\tremaining: 397ms\n",
      "89:\tlearn: 0.0125163\ttotal: 3.22s\tremaining: 358ms\n",
      "90:\tlearn: 0.0124567\ttotal: 3.24s\tremaining: 321ms\n",
      "91:\tlearn: 0.0124359\ttotal: 3.26s\tremaining: 283ms\n",
      "92:\tlearn: 0.0123771\ttotal: 3.27s\tremaining: 246ms\n",
      "93:\tlearn: 0.0122860\ttotal: 3.29s\tremaining: 210ms\n",
      "94:\tlearn: 0.0121490\ttotal: 3.31s\tremaining: 174ms\n",
      "95:\tlearn: 0.0121164\ttotal: 3.32s\tremaining: 138ms\n",
      "96:\tlearn: 0.0120579\ttotal: 3.34s\tremaining: 103ms\n",
      "97:\tlearn: 0.0120123\ttotal: 3.35s\tremaining: 68.4ms\n",
      "98:\tlearn: 0.0119606\ttotal: 3.37s\tremaining: 34ms\n",
      "99:\tlearn: 0.0119046\ttotal: 3.39s\tremaining: 0us\n",
      "4 0.9223140495867769 0.7563636363636363 0.547486033519553\n",
      "trn    0.915846\n",
      "tst    0.763613\n",
      "oot    0.518436\n",
      "dtype: float64\n",
      "CPU times: user 20.8 s, sys: 1.84 s, total: 22.6 s\n",
      "Wall time: 13.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# CatBoost\n",
    "\n",
    "for niter in range(nitermax):    \n",
    "    X_trn, X_tst, Y_trn, Y_tst = train_test_split(X_trntst, Y_trntst, test_size = .3)\n",
    "\n",
    "    model = CatBoostClassifier(\n",
    "            iterations=100,\n",
    "#             learning_rate=0.03,\n",
    "#             l2_leaf_reg=5\n",
    "    \n",
    "    )\n",
    "#     \n",
    "\n",
    "    X_oot = X_oot_orig.copy()\n",
    "    X_trn_save = X_trn.copy()\n",
    "    Y_trn_save = Y_trn.copy()\n",
    "\n",
    "    model.fit(X_trn, Y_trn.values.ravel())   \n",
    "\n",
    "    predictions = model.predict_proba(X_trn_save)[:,1]\n",
    "    X_trn['predicted'] = predictions\n",
    "    X_trn['Fraud'] = Y_trn_save['Fraud']\n",
    "    topRows = int(round(X_trn.shape[0]*0.03))\n",
    "    temp = X_trn.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR3.loc[niter, 'trn'] = sum(needed)/sum(X_trn.loc[:,'Fraud'])\n",
    "\n",
    "    predictions = model.predict_proba(X_tst)[:,1]\n",
    "    X_tst['predicted']=predictions\n",
    "    X_tst['Fraud'] = Y_tst['Fraud']\n",
    "    topRows = int(round(X_tst.shape[0]*0.03))\n",
    "    temp = X_tst.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR3.loc[niter, 'tst'] = sum(needed)/sum(X_tst.loc[:,'Fraud'])\n",
    "\n",
    "    predictions = model.predict_proba(X_oot)[:,1]\n",
    "    X_oot['predicted']=predictions\n",
    "    X_oot['Fraud'] = Y_oot['Fraud']\n",
    "    topRows = int(round(X_oot.shape[0]*0.03))\n",
    "    temp = X_oot.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR3.loc[niter, 'oot'] = sum(needed)/sum(X_oot.loc[:,'Fraud'])\n",
    "    print(niter, FDR3.loc[niter, 'trn'],FDR3.loc[niter, 'tst'],FDR3.loc[niter, 'oot'])\n",
    "    \n",
    "print(FDR3.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T05:08:29.025325Z",
     "start_time": "2022-05-03T05:07:28.403452Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:07:28] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:576: \n",
      "Parameters: { \"sub_sample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:07:28] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0 0.8041074249605056 0.7327935222672065 0.547486033519553\n",
      "[22:07:39] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:576: \n",
      "Parameters: { \"sub_sample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:07:40] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "1 0.8126009693053312 0.7432950191570882 0.553072625698324\n",
      "[22:07:59] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:576: \n",
      "Parameters: { \"sub_sample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:07:59] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "2 0.7906976744186046 0.7338129496402878 0.5586592178770949\n",
      "[22:08:09] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:576: \n",
      "Parameters: { \"sub_sample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:08:10] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "3 0.8216039279869067 0.7286245353159851 0.5363128491620112\n",
      "[22:08:19] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:576: \n",
      "Parameters: { \"sub_sample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:08:19] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "4 0.8029556650246306 0.7527675276752768 0.5363128491620112\n",
      "trn    0.806393\n",
      "tst    0.738259\n",
      "oot    0.546369\n",
      "dtype: float64\n",
      "CPU times: user 1min 44s, sys: 2.81 s, total: 1min 47s\n",
      "Wall time: 1min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# XGBoost\n",
    "\n",
    "for niter in range(nitermax):    \n",
    "    X_trn, X_tst, Y_trn, Y_tst = train_test_split(X_trntst, Y_trntst, test_size = .3)\n",
    "\n",
    "    model = XGBClassifier(\n",
    "        booster='gbtree',\n",
    "        max_depth=5, \n",
    "        min_child_weight=75,\n",
    "        sub_sample=75,\n",
    "        gamma=0.01, \n",
    "    )\n",
    "    \n",
    "    X_oot = X_oot_orig.copy()\n",
    "    X_trn_save = X_trn.copy()\n",
    "    Y_trn_save = Y_trn.copy()\n",
    "\n",
    "    model.fit(X_trn, Y_trn.values.ravel())   \n",
    "\n",
    "    predictions = model.predict_proba(X_trn_save)[:,1]\n",
    "    X_trn['predicted'] = predictions\n",
    "    X_trn['Fraud'] = Y_trn_save['Fraud']\n",
    "    topRows = int(round(X_trn.shape[0]*0.03))\n",
    "    temp = X_trn.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR3.loc[niter, 'trn'] = sum(needed)/sum(X_trn.loc[:,'Fraud'])\n",
    "\n",
    "    predictions = model.predict_proba(X_tst)[:,1]\n",
    "    X_tst['predicted']=predictions\n",
    "    X_tst['Fraud'] = Y_tst['Fraud']\n",
    "    topRows = int(round(X_tst.shape[0]*0.03))\n",
    "    temp = X_tst.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR3.loc[niter, 'tst'] = sum(needed)/sum(X_tst.loc[:,'Fraud'])\n",
    "\n",
    "    predictions = model.predict_proba(X_oot)[:,1]\n",
    "    X_oot['predicted']=predictions\n",
    "    X_oot['Fraud'] = Y_oot['Fraud']\n",
    "    topRows = int(round(X_oot.shape[0]*0.03))\n",
    "    temp = X_oot.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR3.loc[niter, 'oot'] = sum(needed)/sum(X_oot.loc[:,'Fraud'])\n",
    "    print(niter, FDR3.loc[niter, 'trn'],FDR3.loc[niter, 'tst'],FDR3.loc[niter, 'oot'])\n",
    "    \n",
    "print(FDR3.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-01T22:33:48.071639Z",
     "start_time": "2022-05-01T22:33:14.493552Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:33:14] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:576: \n",
      "Parameters: { \"sub_sample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:33:14] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0 0.7852564102564102 0.7734375 0.5586592178770949\n",
      "[15:33:20] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:576: \n",
      "Parameters: { \"sub_sample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:33:20] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "1 0.7911392405063291 0.7661290322580645 0.547486033519553\n",
      "[15:33:26] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:576: \n",
      "Parameters: { \"sub_sample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:33:26] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "2 0.803921568627451 0.7126865671641791 0.5418994413407822\n",
      "[15:33:32] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:576: \n",
      "Parameters: { \"sub_sample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:33:32] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "3 0.7917329093799682 0.7689243027888446 0.553072625698324\n",
      "[15:33:40] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:576: \n",
      "Parameters: { \"sub_sample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:33:40] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "4 0.8022875816993464 0.7089552238805971 0.553072625698324\n",
      "trn    0.794868\n",
      "tst    0.746027\n",
      "oot    0.550838\n",
      "dtype: float64\n",
      "CPU times: user 1min 45s, sys: 1.51 s, total: 1min 47s\n",
      "Wall time: 33.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# XGBoost\n",
    "\n",
    "for niter in range(nitermax):    \n",
    "    X_trn, X_tst, Y_trn, Y_tst = train_test_split(X_trntst, Y_trntst, test_size = .3)\n",
    "\n",
    "    model = XGBClassifier(\n",
    "        booster='gbtree',\n",
    "        max_depth=8, \n",
    "        min_child_weight=80,\n",
    "        sub_sample=40,\n",
    "        gamma=0.03, \n",
    "    )\n",
    "    \n",
    "    X_oot = X_oot_orig.copy()\n",
    "    X_trn_save = X_trn.copy()\n",
    "    Y_trn_save = Y_trn.copy()\n",
    "\n",
    "    model.fit(X_trn, Y_trn.values.ravel())   \n",
    "\n",
    "    predictions = model.predict_proba(X_trn_save)[:,1]\n",
    "    X_trn['predicted'] = predictions\n",
    "    X_trn['Fraud'] = Y_trn_save['Fraud']\n",
    "    topRows = int(round(X_trn.shape[0]*0.03))\n",
    "    temp = X_trn.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR3.loc[niter, 'trn'] = sum(needed)/sum(X_trn.loc[:,'Fraud'])\n",
    "\n",
    "    predictions = model.predict_proba(X_tst)[:,1]\n",
    "    X_tst['predicted']=predictions\n",
    "    X_tst['Fraud'] = Y_tst['Fraud']\n",
    "    topRows = int(round(X_tst.shape[0]*0.03))\n",
    "    temp = X_tst.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR3.loc[niter, 'tst'] = sum(needed)/sum(X_tst.loc[:,'Fraud'])\n",
    "\n",
    "    predictions = model.predict_proba(X_oot)[:,1]\n",
    "    X_oot['predicted']=predictions\n",
    "    X_oot['Fraud'] = Y_oot['Fraud']\n",
    "    topRows = int(round(X_oot.shape[0]*0.03))\n",
    "    temp = X_oot.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR3.loc[niter, 'oot'] = sum(needed)/sum(X_oot.loc[:,'Fraud'])\n",
    "    print(niter, FDR3.loc[niter, 'trn'],FDR3.loc[niter, 'tst'],FDR3.loc[niter, 'oot'])\n",
    "    \n",
    "print(FDR3.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-01T23:03:24.515988Z",
     "start_time": "2022-05-01T22:59:26.815135Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.8054662379421221 0.7635658914728682 0.5418994413407822\n",
      "1 0.8132045088566827 0.7760617760617761 0.5642458100558659\n",
      "2 0.8018867924528302 0.7745901639344263 0.5698324022346368\n",
      "3 0.8186195826645265 0.7237354085603113 0.5418994413407822\n",
      "4 0.8096774193548387 0.7807692307692308 0.547486033519553\n",
      "trn    0.809771\n",
      "tst    0.763744\n",
      "oot    0.553073\n",
      "dtype: float64\n",
      "CPU times: user 3min 36s, sys: 2.66 s, total: 3min 39s\n",
      "Wall time: 3min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# KNN\n",
    "\n",
    "for niter in range(nitermax):    \n",
    "    X_trn, X_tst, Y_trn, Y_tst = train_test_split(X_trntst, Y_trntst, test_size = .3)\n",
    "\n",
    "    model = KNeighborsClassifier(\n",
    "        n_neighbors=35, \n",
    "        leaf_size=50)\n",
    "\n",
    "    X_oot = X_oot_orig.copy()\n",
    "    X_trn_save = X_trn.copy()\n",
    "    Y_trn_save = Y_trn.copy()\n",
    "\n",
    "    model.fit(X_trn, Y_trn.values.ravel())   \n",
    "\n",
    "    predictions = model.predict_proba(X_trn_save)[:,1]\n",
    "    X_trn['predicted'] = predictions\n",
    "    X_trn['Fraud'] = Y_trn_save['Fraud']\n",
    "    topRows = int(round(X_trn.shape[0]*0.03))\n",
    "    temp = X_trn.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR3.loc[niter, 'trn'] = sum(needed)/sum(X_trn.loc[:,'Fraud'])\n",
    "\n",
    "    predictions = model.predict_proba(X_tst)[:,1]\n",
    "    X_tst['predicted']=predictions\n",
    "    X_tst['Fraud'] = Y_tst['Fraud']\n",
    "    topRows = int(round(X_tst.shape[0]*0.03))\n",
    "    temp = X_tst.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR3.loc[niter, 'tst'] = sum(needed)/sum(X_tst.loc[:,'Fraud'])\n",
    "\n",
    "    predictions = model.predict_proba(X_oot)[:,1]\n",
    "    X_oot['predicted']=predictions\n",
    "    X_oot['Fraud'] = Y_oot['Fraud']\n",
    "    topRows = int(round(X_oot.shape[0]*0.03))\n",
    "    temp = X_oot.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR3.loc[niter, 'oot'] = sum(needed)/sum(X_oot.loc[:,'Fraud'])\n",
    "    print(niter, FDR3.loc[niter, 'trn'],FDR3.loc[niter, 'tst'],FDR3.loc[niter, 'oot'])\n",
    "    \n",
    "print(FDR3.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-02T00:37:31.106489Z",
     "start_time": "2022-05-02T00:32:58.970720Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.8325123152709359 0.7490774907749077 0.5307262569832403\n",
      "1 0.8150572831423896 0.7620817843866171 0.5977653631284916\n",
      "2 0.8322475570032574 0.7255639097744361 0.5921787709497207\n",
      "3 0.8141891891891891 0.7395833333333334 0.5307262569832403\n",
      "4 0.819376026272578 0.7343173431734318 0.547486033519553\n",
      "trn    0.822676\n",
      "tst    0.742125\n",
      "oot    0.559777\n",
      "dtype: float64\n",
      "CPU times: user 3min 28s, sys: 3.42 s, total: 3min 31s\n",
      "Wall time: 4min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# KNN\n",
    "\n",
    "for niter in range(nitermax):    \n",
    "    X_trn, X_tst, Y_trn, Y_tst = train_test_split(X_trntst, Y_trntst, test_size = .3)\n",
    "\n",
    "    model = KNeighborsClassifier(\n",
    "        n_neighbors=29, \n",
    "        leaf_size=50)\n",
    "    \n",
    "\n",
    "    X_oot = X_oot_orig.copy()\n",
    "    X_trn_save = X_trn.copy()\n",
    "    Y_trn_save = Y_trn.copy()\n",
    "\n",
    "    model.fit(X_trn, Y_trn.values.ravel())   \n",
    "\n",
    "    predictions = model.predict_proba(X_trn_save)[:,1]\n",
    "    X_trn['predicted'] = predictions\n",
    "    X_trn['Fraud'] = Y_trn_save['Fraud']\n",
    "    topRows = int(round(X_trn.shape[0]*0.03))\n",
    "    temp = X_trn.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR3.loc[niter, 'trn'] = sum(needed)/sum(X_trn.loc[:,'Fraud'])\n",
    "\n",
    "    predictions = model.predict_proba(X_tst)[:,1]\n",
    "    X_tst['predicted']=predictions\n",
    "    X_tst['Fraud'] = Y_tst['Fraud']\n",
    "    topRows = int(round(X_tst.shape[0]*0.03))\n",
    "    temp = X_tst.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR3.loc[niter, 'tst'] = sum(needed)/sum(X_tst.loc[:,'Fraud'])\n",
    "\n",
    "    predictions = model.predict_proba(X_oot)[:,1]\n",
    "    X_oot['predicted']=predictions\n",
    "    X_oot['Fraud'] = Y_oot['Fraud']\n",
    "    topRows = int(round(X_oot.shape[0]*0.03))\n",
    "    temp = X_oot.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR3.loc[niter, 'oot'] = sum(needed)/sum(X_oot.loc[:,'Fraud'])\n",
    "    print(niter, FDR3.loc[niter, 'trn'],FDR3.loc[niter, 'tst'],FDR3.loc[niter, 'oot'])\n",
    "    \n",
    "print(FDR3.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-02T21:26:07.750180Z",
     "start_time": "2022-05-02T21:20:18.411811Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.6331058020477816 0.6122448979591837 0.30726256983240224\n",
      "1 0.6293245469522241 0.6593406593406593 0.33519553072625696\n",
      "2 0.6276083467094703 0.6303501945525292 0.31843575418994413\n",
      "3 0.6480891719745223 0.5873015873015873 0.30726256983240224\n",
      "4 0.6434494195688225 0.6137184115523465 0.3240223463687151\n",
      "trn    0.636315\n",
      "tst    0.620591\n",
      "oot    0.318436\n",
      "dtype: float64\n",
      "CPU times: user 5min 15s, sys: 9.42 s, total: 5min 25s\n",
      "Wall time: 5min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# SVM\n",
    "\n",
    "for niter in range(nitermax):    \n",
    "    X_trn, X_tst, Y_trn, Y_tst = train_test_split(X_trntst, Y_trntst, test_size = .3)\n",
    "\n",
    "    model = svm.SVC(\n",
    "#         C=0.1, \n",
    "#         gamma=100,\n",
    "        kernel='linear',\n",
    "        probability=True)\n",
    "  \n",
    "\n",
    "    X_oot = X_oot_orig.copy()\n",
    "    X_trn_save = X_trn.copy()\n",
    "    Y_trn_save = Y_trn.copy()\n",
    "\n",
    "    model.fit(X_trn, Y_trn.values.ravel())   \n",
    "\n",
    "    predictions = model.predict_proba(X_trn_save)[:,1]\n",
    "    X_trn['predicted'] = predictions\n",
    "    X_trn['Fraud'] = Y_trn_save['Fraud']\n",
    "    topRows = int(round(X_trn.shape[0]*0.03))\n",
    "    temp = X_trn.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR3.loc[niter, 'trn'] = sum(needed)/sum(X_trn.loc[:,'Fraud'])\n",
    "\n",
    "    predictions = model.predict_proba(X_tst)[:,1]\n",
    "    X_tst['predicted']=predictions\n",
    "    X_tst['Fraud'] = Y_tst['Fraud']\n",
    "    topRows = int(round(X_tst.shape[0]*0.03))\n",
    "    temp = X_tst.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR3.loc[niter, 'tst'] = sum(needed)/sum(X_tst.loc[:,'Fraud'])\n",
    "\n",
    "    predictions = model.predict_proba(X_oot)[:,1]\n",
    "    X_oot['predicted']=predictions\n",
    "    X_oot['Fraud'] = Y_oot['Fraud']\n",
    "    topRows = int(round(X_oot.shape[0]*0.03))\n",
    "    temp = X_oot.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR3.loc[niter, 'oot'] = sum(needed)/sum(X_oot.loc[:,'Fraud'])\n",
    "    print(niter, FDR3.loc[niter, 'trn'],FDR3.loc[niter, 'tst'],FDR3.loc[niter, 'oot'])\n",
    "    \n",
    "print(FDR3.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-02T21:57:20.844287Z",
     "start_time": "2022-05-02T21:26:07.757441Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7329093799682035 0.7211155378486056 0.6089385474860335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.7121212121212122 0.7552447552447552 0.5195530726256983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 0.7553719008264462 0.6836363636363636 0.4972067039106145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 0.753125 0.6875 0.4860335195530726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 0.7586776859504132 0.6981818181818182 0.5139664804469274\n",
      "trn    0.742441\n",
      "tst    0.709136\n",
      "oot    0.525140\n",
      "dtype: float64\n",
      "CPU times: user 26min 21s, sys: 26.9 s, total: 26min 48s\n",
      "Wall time: 31min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# SVM\n",
    "\n",
    "for niter in range(nitermax):    \n",
    "    X_trn, X_tst, Y_trn, Y_tst = train_test_split(X_trntst, Y_trntst, test_size = .3)\n",
    "\n",
    "    model = svm.SVC(\n",
    "#         C=0.1, \n",
    "#         gamma=100,\n",
    "        kernel='poly',\n",
    "        probability=True)\n",
    "\n",
    "    X_oot = X_oot_orig.copy()\n",
    "    X_trn_save = X_trn.copy()\n",
    "    Y_trn_save = Y_trn.copy()\n",
    "\n",
    "    model.fit(X_trn, Y_trn.values.ravel())   \n",
    "\n",
    "    predictions = model.predict_proba(X_trn_save)[:,1]\n",
    "    X_trn['predicted'] = predictions\n",
    "    X_trn['Fraud'] = Y_trn_save['Fraud']\n",
    "    topRows = int(round(X_trn.shape[0]*0.03))\n",
    "    temp = X_trn.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR3.loc[niter, 'trn'] = sum(needed)/sum(X_trn.loc[:,'Fraud'])\n",
    "\n",
    "    predictions = model.predict_proba(X_tst)[:,1]\n",
    "    X_tst['predicted']=predictions\n",
    "    X_tst['Fraud'] = Y_tst['Fraud']\n",
    "    topRows = int(round(X_tst.shape[0]*0.03))\n",
    "    temp = X_tst.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR3.loc[niter, 'tst'] = sum(needed)/sum(X_tst.loc[:,'Fraud'])\n",
    "\n",
    "    predictions = model.predict_proba(X_oot)[:,1]\n",
    "    X_oot['predicted']=predictions\n",
    "    X_oot['Fraud'] = Y_oot['Fraud']\n",
    "    topRows = int(round(X_oot.shape[0]*0.03))\n",
    "    temp = X_oot.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR3.loc[niter, 'oot'] = sum(needed)/sum(X_oot.loc[:,'Fraud'])\n",
    "    print(niter, FDR3.loc[niter, 'trn'],FDR3.loc[niter, 'tst'],FDR3.loc[niter, 'oot'])\n",
    "    \n",
    "print(FDR3.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-02T23:51:56.150816Z",
     "start_time": "2022-05-02T22:33:59.345878Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7475728155339806 0.7061068702290076 0.5027932960893855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.7579831932773109 0.7052631578947368 0.441340782122905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 0.751937984496124 0.7148936170212766 0.5307262569832403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 0.7635782747603834 0.6850393700787402 0.5195530726256983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 0.7435483870967742 0.7 0.5363128491620112\n",
      "trn    0.752924\n",
      "tst    0.702261\n",
      "oot    0.506145\n",
      "dtype: float64\n",
      "CPU times: user 1h 3min 45s, sys: 1min 11s, total: 1h 4min 56s\n",
      "Wall time: 1h 17min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# SVM\n",
    "\n",
    "for niter in range(nitermax):    \n",
    "    X_trn, X_tst, Y_trn, Y_tst = train_test_split(X_trntst, Y_trntst, test_size = .3)\n",
    "\n",
    "    model = svm.SVC(\n",
    "        C=10, \n",
    "#         gamma=100,\n",
    "        kernel='poly',\n",
    "        probability=True)\n",
    "\n",
    "    X_oot = X_oot_orig.copy()\n",
    "    X_trn_save = X_trn.copy()\n",
    "    Y_trn_save = Y_trn.copy()\n",
    "\n",
    "    model.fit(X_trn, Y_trn.values.ravel())   \n",
    "\n",
    "    predictions = model.predict_proba(X_trn_save)[:,1]\n",
    "    X_trn['predicted'] = predictions\n",
    "    X_trn['Fraud'] = Y_trn_save['Fraud']\n",
    "    topRows = int(round(X_trn.shape[0]*0.03))\n",
    "    temp = X_trn.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR3.loc[niter, 'trn'] = sum(needed)/sum(X_trn.loc[:,'Fraud'])\n",
    "\n",
    "    predictions = model.predict_proba(X_tst)[:,1]\n",
    "    X_tst['predicted']=predictions\n",
    "    X_tst['Fraud'] = Y_tst['Fraud']\n",
    "    topRows = int(round(X_tst.shape[0]*0.03))\n",
    "    temp = X_tst.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR3.loc[niter, 'tst'] = sum(needed)/sum(X_tst.loc[:,'Fraud'])\n",
    "\n",
    "    predictions = model.predict_proba(X_oot)[:,1]\n",
    "    X_oot['predicted']=predictions\n",
    "    X_oot['Fraud'] = Y_oot['Fraud']\n",
    "    topRows = int(round(X_oot.shape[0]*0.03))\n",
    "    temp = X_oot.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR3.loc[niter, 'oot'] = sum(needed)/sum(X_oot.loc[:,'Fraud'])\n",
    "    print(niter, FDR3.loc[niter, 'trn'],FDR3.loc[niter, 'tst'],FDR3.loc[niter, 'oot'])\n",
    "    \n",
    "print(FDR3.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Comparison plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-06T01:38:06.580085Z",
     "start_time": "2022-05-06T01:38:06.265219Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel('Modeling output.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-06T01:38:06.846185Z",
     "start_time": "2022-05-06T01:38:06.801096Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Train</th>\n",
       "      <th>Test</th>\n",
       "      <th>OOT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.632543</td>\n",
       "      <td>0.622926</td>\n",
       "      <td>0.363128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.632454</td>\n",
       "      <td>0.623528</td>\n",
       "      <td>0.363128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.631134</td>\n",
       "      <td>0.625092</td>\n",
       "      <td>0.363128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.628194</td>\n",
       "      <td>0.634535</td>\n",
       "      <td>0.363128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.525071</td>\n",
       "      <td>0.284358</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model     Train      Test       OOT\n",
       "0  Logistic Regression  0.632543  0.622926  0.363128\n",
       "1  Logistic Regression  0.632454  0.623528  0.363128\n",
       "2  Logistic Regression  0.631134  0.625092  0.363128\n",
       "3  Logistic Regression  0.628194  0.634535  0.363128\n",
       "4        Decision Tree  1.000000  0.525071  0.284358"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-06T01:50:31.974332Z",
     "start_time": "2022-05-06T01:50:31.932175Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Type</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>Train</td>\n",
       "      <td>0.632543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>Train</td>\n",
       "      <td>0.632454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>Train</td>\n",
       "      <td>0.631134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>Train</td>\n",
       "      <td>0.628194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>Train</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model   Type     Value\n",
       "0  Logistic Regression  Train  0.632543\n",
       "1  Logistic Regression  Train  0.632454\n",
       "2  Logistic Regression  Train  0.631134\n",
       "3  Logistic Regression  Train  0.628194\n",
       "4        Decision Tree  Train  1.000000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_unpivot = df.melt( id_vars='Model', value_vars=['Train','Test','OOT'], var_name=['Type'], value_name='Value')\n",
    "df_unpivot.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-06T01:50:36.212081Z",
     "start_time": "2022-05-06T01:50:36.172078Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Type</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>Train</td>\n",
       "      <td>0.632543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>Train</td>\n",
       "      <td>0.632454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>Train</td>\n",
       "      <td>0.631134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>Train</td>\n",
       "      <td>0.628194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>Train</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model   Type     Value\n",
       "0  Logistic Regression  Train  0.632543\n",
       "1  Logistic Regression  Train  0.632454\n",
       "2  Logistic Regression  Train  0.631134\n",
       "3  Logistic Regression  Train  0.628194\n",
       "4        Decision Tree  Train  1.000000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_compare = df_unpivot[(df_unpivot['Type']=='Train') | (df_unpivot['Type']=='Test')]\n",
    "df_compare.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-05T04:14:04.614012Z",
     "start_time": "2022-05-05T04:14:04.483932Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">Train</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Test</th>\n",
       "      <th colspan=\"2\" halign=\"left\">OOT</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Boosted Tree</td>\n",
       "      <td>0.940775</td>\n",
       "      <td>0.054730</td>\n",
       "      <td>0.850075</td>\n",
       "      <td>0.022342</td>\n",
       "      <td>0.543700</td>\n",
       "      <td>0.034278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>CatBoost</td>\n",
       "      <td>0.895143</td>\n",
       "      <td>0.043272</td>\n",
       "      <td>0.774031</td>\n",
       "      <td>0.012913</td>\n",
       "      <td>0.535531</td>\n",
       "      <td>0.026639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.756899</td>\n",
       "      <td>0.090211</td>\n",
       "      <td>0.672854</td>\n",
       "      <td>0.054262</td>\n",
       "      <td>0.417375</td>\n",
       "      <td>0.053134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Gradient Boosting Tree</td>\n",
       "      <td>0.894623</td>\n",
       "      <td>0.070288</td>\n",
       "      <td>0.827655</td>\n",
       "      <td>0.041695</td>\n",
       "      <td>0.545140</td>\n",
       "      <td>0.030774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>KNN</td>\n",
       "      <td>0.843146</td>\n",
       "      <td>0.056663</td>\n",
       "      <td>0.755858</td>\n",
       "      <td>0.009286</td>\n",
       "      <td>0.542346</td>\n",
       "      <td>0.037423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.631081</td>\n",
       "      <td>0.002030</td>\n",
       "      <td>0.626520</td>\n",
       "      <td>0.005421</td>\n",
       "      <td>0.363128</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Neural Network</td>\n",
       "      <td>0.783998</td>\n",
       "      <td>0.027108</td>\n",
       "      <td>0.748745</td>\n",
       "      <td>0.016655</td>\n",
       "      <td>0.578317</td>\n",
       "      <td>0.018382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.916529</td>\n",
       "      <td>0.068448</td>\n",
       "      <td>0.826754</td>\n",
       "      <td>0.025539</td>\n",
       "      <td>0.578324</td>\n",
       "      <td>0.018683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>SVM</td>\n",
       "      <td>0.673673</td>\n",
       "      <td>0.212638</td>\n",
       "      <td>0.642178</td>\n",
       "      <td>0.191341</td>\n",
       "      <td>0.416015</td>\n",
       "      <td>0.151211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.885568</td>\n",
       "      <td>0.060092</td>\n",
       "      <td>0.833094</td>\n",
       "      <td>0.040486</td>\n",
       "      <td>0.549441</td>\n",
       "      <td>0.018674</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Train                Test                 OOT  \\\n",
       "                            mean       std      mean       std      mean   \n",
       "Model                                                                      \n",
       "Boosted Tree            0.940775  0.054730  0.850075  0.022342  0.543700   \n",
       "CatBoost                0.895143  0.043272  0.774031  0.012913  0.535531   \n",
       "Decision Tree           0.756899  0.090211  0.672854  0.054262  0.417375   \n",
       "Gradient Boosting Tree  0.894623  0.070288  0.827655  0.041695  0.545140   \n",
       "KNN                     0.843146  0.056663  0.755858  0.009286  0.542346   \n",
       "Logistic Regression     0.631081  0.002030  0.626520  0.005421  0.363128   \n",
       "Neural Network          0.783998  0.027108  0.748745  0.016655  0.578317   \n",
       "Random Forest           0.916529  0.068448  0.826754  0.025539  0.578324   \n",
       "SVM                     0.673673  0.212638  0.642178  0.191341  0.416015   \n",
       "XGBoost                 0.885568  0.060092  0.833094  0.040486  0.549441   \n",
       "\n",
       "                                  \n",
       "                             std  \n",
       "Model                             \n",
       "Boosted Tree            0.034278  \n",
       "CatBoost                0.026639  \n",
       "Decision Tree           0.053134  \n",
       "Gradient Boosting Tree  0.030774  \n",
       "KNN                     0.037423  \n",
       "Logistic Regression     0.000000  \n",
       "Neural Network          0.018382  \n",
       "Random Forest           0.018683  \n",
       "SVM                     0.151211  \n",
       "XGBoost                 0.018674  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = df.groupby('Model').agg({'Train':['mean','std'],'Test':['mean','std'],'OOT':['mean','std']})\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-05T04:14:05.687621Z",
     "start_time": "2022-05-05T04:14:04.625793Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(20,6))\n",
    "ax = sns.boxplot(x='Model',y='Train', data=df, color='navy')\n",
    "\n",
    "# Select which box you want to change    \n",
    "mybox = ax.artists[9]\n",
    "\n",
    "# Change the appearance of that box\n",
    "mybox.set_facecolor('red')\n",
    "# mybox.set_edgecolor('black')\n",
    "# mybox.set_linewidth(3)\n",
    "plt.xlabel('')\n",
    "plt.ylabel('Train Score (FDR3%)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-05T04:14:06.982529Z",
     "start_time": "2022-05-05T04:14:05.693497Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABI8AAAFlCAYAAABiEshBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nOzde5xdZXno8d+TSVCGi6mS9FguE1R0oOpBHdF6qVhRUUtswQsqqbS2HE3RatWWlhYpNdVK1WMrU4tWqeFQRKCa0GjwhhYlNcNFbhk0oiMRNfECEUJhknnOH2sN2Rn2mj2T7DV79uT3/XzyyVrvXvvdz6zbXvtZ7/uuyEwkSZIkSZKkZuZ1OgBJkiRJkiTNXiaPJEmSJEmSVMnkkSRJkiRJkiqZPJIkSZIkSVIlk0eSJEmSJEmqZPJIkiRJkiRJleZ3OoDpOuigg3LJkiWdDkOSJEmSJGnOuPbaa3+amYuavdZ1yaMlS5YwNDTU6TAkSZIkSZLmjIgYqXrNbmuSJEmSJEmqZPJIkiRJkiRJlUweSZIkSZIkqZLJI0mSJEmSJFUyeSRJkiRJkqRKJo8kSZIkSZJUyeSRJEmSJEmSKpk8kiRJkiRJUiWTR5IkSZIkSapUa/IoIo6PiNsiYmNEnNHk9b6I+FJE3BgRV0XEIXXGI0mSJEmSpOmpLXkUET3AecBLgKOA10TEURMW+wfgk5n5ZOAc4D11xSNJkiRJkqTpm19j3ccAGzPzdoCIuBh4OXBrwzJHAW8rp78CfKbGeCRJkvY6K1asYHh4uO31joyMANDX19f2uvv7+znzzDPbXq8kSdo9dXZbOxi4o2F+U1nW6FvASeX07wIHRMSjJlYUEadFxFBEDG3ZsqWWYCVJkjR127ZtY9u2bZ0OQ5IkzYA6Wx5Fk7KcMP8O4MMRcSrwNeCHwPaHvCnzfOB8gIGBgYl1SJIkqUJdLXiWLVsGwMqVK2upX5IkzR51Jo82AYc2zB8C3Nm4QGbeCZwIEBH7Aydl5t01xiRJkiRJkqRpqLPb2nrgiIg4PCL2AU4GVjUuEBEHRcR4DH8BfLzGeCRJkiRJkjRNtSWPMnM7cDqwFtgAXJKZt0TEORGxtFzsWOC2iPg28KvAirrikSRJkiRJ0vTV2W2NzFwDrJlQdlbD9KXApXXGIEmSJEmSpN1Xa/JIkiSo71Hh4OPCZ0I3Puod3H6SJEntYvJIktTVfFR493LbSZIkdQeTR5Kk2tXZ+sPHhdfPR71LkiTt3ep82pokSZIkSZK6nMkjSZIkSZIkVTJ5JEmSJEmSpEqOeSRJelCdT0Wry4YNG4Cd4+d0A58Cpok89maOx58kSdNn8kiS9KDh4WHWrbuO7dsP7HQoU9bT8wAAV1+9scORTM38+Vs7HYJmIY+9meHxJ0nS7jF5JKlr1HVnfmRkBIC+vr62192Nd7i3bz+Qu+56ZqfDmLMWLlzX6RA0S3ns1c/jT5Kk3WPySNJeb9u2bZ0OQWqLbuv6ZLcnSZKk7mDySFLXqOvH2vgP15UrV9ZSvzRTuq3rk92eJEmSuoPJI0mS5hC7PtXLbk+SJGlvNK/TAUiSJEmSJGn2suWRpLbqtjFXwHFXGo2MjDB//lZbV9Ro/vytDw7SLkmSJHUDk0eS2qrbxlwBx12RJEmSpMmYPJLUdo65Ur+6Wgb19fVxxx2jbr8aLVy4jr6+vk6HIUmSJE2ZYx5JkiRJkiSpkskjSZIkSZIkVTJ5JEmSJEmSpEqOeSRJ0hzh0/LqV9fT8tx2M8OnHUqStHtseSRJkiRJkqRKtjySJO2i21o/9PTcC8COHft1OJKpmT9/a211+7S8+tX1tDy33czwaYeSJO0ek0eSpAf19/d3OoRp27BhAwBHHvm4Dkcydd24niVJkrT3MnkkSXrQmWee2ekQpm3ZsmUArFy5ssORSJIkSXOTySNJbeWgrzOj2wZ9XbFiBcPDw7XUPd7yaDyJ1E79/f1dmVCTJEmS2snkkSSpq/X29nY6BEmSJGlOM3kkqa0c9HVmdNugr7bekSRJkrrXvE4HIEmSJEmSpNnL5JEkSZIkSZIqmTySJEmSJElSJcc8ktR23fa0tZ6eewHYsWO/DkcydfPnb+10CJIkSZL2ErUmjyLieOBDQA/wscx874TXDwP+DVhYLnNGZq6pMyZJ9erv7+90CNM2/qj3I498XIcjmZ5uXNeSJEmSuk9tyaOI6AHOA14IbALWR8SqzLy1YbG/Ai7JzH+OiKOANcCSumKSVL9ufKrWsmXLAFi5cmWHI5EkSZKk2afOlkfHABsz83aAiLgYeDnQmDxK4MBy+hHAnTXGI0mSNGvZ5bd+dvmVJGn31Jk8Ohi4o2F+E/CMCcucDVwZEW8G9gOOa1ZRRJwGnAZw2GGHtT1QSZKkTurGbqh2+ZUkae9RZ/IompTlhPnXABdk5vsj4jeAlRHxxMwc2+VNmecD5wMMDAxMrEPSXmLFihUMDw+3vd7xH0Dj3dfaqb+/vyu78kmaWd14nrDLb/3q+t4bGRkBoK+vr+11g9996n51HXtQ7/Hnsac61Zk82gQc2jB/CA/tlvYG4HiAzLwmIh4OHARsrjEuSdpFb29vp0OQJGnGbNu2rdMhSHstjz91qzqTR+uBIyLicOCHwMnAaycs8wPgBcAFEXEk8HBgS40xSepi3kmRWuumcXMcM0eaXF3fe7YakyZX5zWnx5+6VW3Jo8zcHhGnA2uBHuDjmXlLRJwDDGXmKuDtwEcj4m0UXdpOzUy7pUmStBu6bSwXx8yRJEnqDnW2PCIz1wBrJpSd1TB9K/DsOmOQJGlv0W2t87z7KkmS1B3mdToASZIkSZIkzV4mjyRJkiRJklSp1m5rkiRJ0lxX52O96zA+3th419Fu4WPIJalzTB5JkiTNYXUlNupMQHRbkmB4eJh1665j+/YDOx3KlPT0PADA1Vdv7HAkU+eTDiWps0weSZIkadp6e3s7HcKsMTIy0ukQpmXHjv06HcJu6bb1LKkz6rppMn4O6uvra3vdMPtvnJg8kiRJmsNm84WoJEndYtu2bZ0OoaNMHkmSJEl7oK+vjzvuGOWuu57Z6VDmrIUL19V2t1/S3FLXTZPxbtorV66spf7ZzuSRJEmSJLVZN3adme3dZiR1jskjSZIkaQ/Nn7+VhQvXtbXOnp57idjR1jpnQmZP28dVqmvA7DqflDcyMlJLN5fxOuuoe2RkpLb1YWJK6m4mjyRJkqQ90N/fX0u9dSUf6tbb21tbq5h2W7t2LT/+8U/I7J6fRRFjANx9931tr/vuu+/jRz/6WdvrjdjOyMiIySPtos7kbR3qfMpondqVuO2es6QkSZI0C/mDuLtlzmf79gM7HcacVlfLMXW34eFhNlxzDUfef3+nQ5mS3gULiomrrupoHNOx4WEPa1tdJo8kSZIk7ZUc7HxmOOC5qhx5//2s3LSp02HMWcsOOaRtdc1rW02SJEmSJEmac2x5JEmSJlXXmAR1jx3g4KySpqKOwc7r1NNzL0DbByWvk93WpO5n8kiSJHVEb29vp0OQtJera7DzOo0n3o888nEdjmR6unFdq14jIyNse9jD2tq1Srva8LCH0Tsy0pa6TB5JkqRJ2XpH0lzVjee38daaK1eu7HAkkvYmjnkkSZIkSZJmVLcNoj6yYAEj409c6yLtWs+2PJIkSZKkNuvG8eK6bay4utZxneoe768Ode0X3daVcVu57TjyyM4GMg1H0r71bPJIkiRJkrqE48XtNDw8zLp117F9+4GdDmXKenoeAODqqzd2OJKpqXOw825KVIJdRk0eSZIkSVKbddsP4261ffuB3HXXMzsdxpzVTU8iVL0c80iSJEmSJEmVTB5JkiRJkiSpkskjSZIkSZIkVTJ5JEmSJEmSpEoOmC1JkiRJkuaEFStWMDw83PZ6N2zYAOx86lq79ff3z+qB9k0eSZIkSZIkTaK3t7fTIXSUySNJkiRJkjQnzObWO93MMY8kSZIkSZImsXnzZk455RS2bNnS6VA6wuSRJEmSJEnSJAYHBxkaGmJwcLDToXSEySNJkiRJkqQKmzdv5vLLLyczueyyy/bK1keOeaS9Sl0j74+MjADQ19fX9rph9o+8L0mSJElz1eDgIGNjYwCMjY0xODjIu971rg5HNbNqbXkUEcdHxG0RsTEizmjy+gcj4oby37cj4q4645Hqsm3bNrZt29bpMCRJkiRJbbZ69WpGR0cBGB0dZdWqVR2OaObV1vIoInqA84AXApuA9RGxKjNvHV8mM9/WsPybgafUFY+6S10thLrV8PAwy5Yta3u9tmiSJEmSpMmdcMIJXHrppYyOjrJgwQKWLl3a6ZBmXJ3d1o4BNmbm7QARcTHwcuDWiuVfA+xd7b5UaXh4mHXrrmP79gM7HcqU9PQ8AMDVV2/scCRTN3/+1k6HIEmSJEmz3vLly7n88ssBmDdvHsuXL+9wRDOvzuTRwcAdDfObgGc0WzAi+oDDgS9XvH4acBrAYYcd1t4oNWtt334gd931zE6HMWctXLiu0yFIkiRJ0qy3ePFiTjzxRC6++GJOOukkFi1a1OmQZlydYx5Fk7KsWPZk4NLM3NHsxcw8PzMHMnNgb9xIkiRJkiSpc5YvX87AwMBe2eoI6m15tAk4tGH+EODOimVPBv64xlgkSZIkSZJ2y+LFi7nwwgs7HUbH1NnyaD1wREQcHhH7UCSIHjIkeUQ8AfgV4JoaY5EkSZIkSdJuqK3lUWZuj4jTgbVAD/DxzLwlIs4BhjJzPJH0GuDizKzq0qa90MjICPPnb3VcnhrNn7+VkZGRTochSZIkSZrl6uy2RmauAdZMKDtrwvzZdcYgSZIkSZKk3Vdr8kjaXX19fdxxx6hPW6vRwoXr6Ovr63QYkiRJkqRZzuSRJEmSJKnrONRF/RzqQuPqHDBbkiRJkiRJXc6WR5IkSZKkruNQF/VzqAuNs+WRJEmSJEmSKtnySLNWN/Vf7um5F4AdO/brcCRTN3/+1k6HIEmSJEnqAiaPNCv19/d3OoRp2bBhAwBHHvm4DkcyPd22niVJkiRJM8/kkWalM888s5Z6V6xYwfDwcC1116m/v7+2dSJJkiRJ0mRMHklt0Nvb2+kQJEmSJEmqhckj7VVsvSNJkiRJ0vT4tDVJkiRJkiRVMnkkSZIkSZKkSiaPJEmSJEmSVMnkkSRJkiRJkiqZPJIkSZIkSVKlKT1tLSIeBTwL+DXgPuBm4PrMzBpjkyRJkiRJUodN2vIoIp4bEWuALwC/CxwOPBV4N3BzRPx1ROxff5h7h82bN3PKKaewZcuWTociSZIkSZIEtO62diJwemY+NTP/IDPPyMy3ZuZLgacAG4Dja49yLzE4OMjQ0BCDg4OdDkWSJEmSJAlokTzKzLdl5u0Vrz2QmZdm5qX1hLZ32bx5M5dffjmZyWWXXWbrI0mSJEmSNCtMacyjcRHxdODvgIcD78vM1bVEtRcaHBxkbGwMgLGxMQYHB3nXu97V4agkSZIkafaaP38rCxeu63QYU9bTcy8AO3bs1+FIpmb+/K2dDkGzxKTJo4hYnJmbG4reCbyinL4aMHnUJqtXr2Z0dBSA0dFRVq1aZfJIkiRJkir09/d3OoRp27BhAwBHHvm4Dkcydd24ntV+rVoefSwirgE+kJn3A3cDJwFjwC/rDm5vcsIJJ3DppZcyOjrKggULWLp0aadDkiRJkqRZ68wzz+x0CNO2bNkyAFauXNnhSKTpmTR5lJlLI+J3gP+MiH8F3gosA3opBtOetVasWMHw8HAtdY+MjLBt27a21jk2NrZLy6PPfvazrF7dvoZdvb299PX1ta2+Rv39/V154pYkSZIkSa21HPMoMz8TEVcAbwYuBf42M79Re2R7aO3atfz4xz8hc1rDOk1JxA4g215vo3vuubet9W3deg8/+tHP2lonQMR2RkZGTB5JkiRJkjRHTfq0tYh4WUR8DbgSuBZ4HfDqiLgwIpbUH54kSZIkSZI6qVWznPcAzwb2Ba7IzGOAP4mIfuBc4JU1x7fbXvziF3dVt7W61d1tTZIkSZIkzU2tkkdbKcY22hfYMl6YmcPM4sQRdOfgaZIkSZIkSbPNpN3WKBJHj6RIMr2u/nAkSZIkSZI0m7R62tpm4IMREZmZETEfOAoYycy7ZyRCSZIkSZJmSJ1P7t6wYQMAy5Yta3vdPgVbdWo1YPYJEfFj4M6I+G3gauDDwK0R8bKZCFCSJEmSpLmgt7eX3t7eTochTVtkVj9yPiKuB14K9ALXA8/IzA0RcThwSWY+fWbC3GlgYCCHhoZm+mMlSZIkSZLmrIi4NjMHmr3WaswjMvNHmfld4AeZuaEs+x7QM4UPPj4ibouIjRFxRsUyr4qIWyPiloi4qFWdkiRJkiRJmjmtnrYWETEvM8eAP2oonAfs0+KNPcB5wAuBTcD6iFiVmbc2LHME8BfAszPzFxGxeDf/DkmSJEmSJNWgVcujN1ImiTLzmobyw4BzW7z3GGBjZt6emQ8AFwMvn7DMHwHnZeYvys/YPNXAJUmSJEmSVL9Jk0eZuS4z/6dJ+feBVsPPHwzc0TC/qSxr9Hjg8RHx9YhYFxHHN6soIk6LiKGIGNqyZUuLj5UkSZIkSVK7tHra2ryIeGVEvDUijizLjo+IrwEfa1F3NCmbODr3fOAI4FjgNcDHImLhQ96UeX5mDmTmwKJFi1p8rCRJkiRJktql1ZhHHwMeA6wH/jkivkOR6PmLzLy0xXs3AYc2zB8C3NlkmXWZOQp8LyJuo0gmrZ9a+JIkSZIkSapTq+TRM4AnZ+aOiNgX+CnwuMz80RTqXg8cERGHAz8ETgZeO2GZz1C0OLogIg6i6MZ2+3T+AEmSJEmSJNWn1YDZ92fmDoDMvA+4bYqJIzJzO3A6sBbYAFySmbdExDkRsbRcbC3ws4i4FfgK8M7M/Nnu/CGSJEmSJElqv8icOAxRw4sR29g5MHYATyjnA8jMfGrtEU4wMDCQQ0NDM/2xkiRJkiRJc1ZEXJuZA81ea9Vt7Uk1xCNJkiRJkqQuMWnyKDO/CxARBwCPK4u/k5n31B2YJEmSJEmSOm/S5FFE7AMMAq8Cvk8xRtIhEXEJ8MflU9IkSZIkSZI0R7UaMPtMYH/gkMx8cmY+EegD9gP+qu7gJEmSJEmS1FmtkkcnAn+YmVvHCzLzbuCN5WuSJEmSJEmaw1olj2g2vlFm/hKofkybJEmSJEmS5oRWT1sbKwfLjiavmTySJEmSJEma41oljx4F3MKuyaMs500eSZIkSZIkzXGTJo8y85CZCkSSJEmSJEmzz6RjHkXEmxqm++sPR5IkSZIkSbNJqwGz/6hh+qI6A5EkSZIkSdLs0/Jpaw2aDZotSZIkSZKkOazVgNkLI+IEiiTTgRGxtPHFzFxVW2SSJEmSJEnquFbJo68DryqnvwG8suG1BEweSZIkSZIkzWGtnra2bKYCkSRJkiRJ0uzT6mlrJ0dE5VhHEbEkIp7V/rAkSZIkSZI0G7TqtnYwcH1EfBO4FtgCPBx4HHAssBX48zoDlCRJkiRJUue06rb2/oj4EPBC4NnAMcB9wAbgDZn5vfpDlCRJkiRJUqe0anlEZm4HPlf+kyRJkiRJ0l5k0jGPJEmSJEmStHczeSS1webNmznllFPYsmVLp0ORJEmSJKmtTB5JbTA4OMjQ0BCDg4OdDkWSJEmSpLaaUvIoIhZFxL9ExBXl/FERcWqtkUldYvPmzVx++eVkJpdddpmtjyRJkiRJc8pUWx5dAHwVOLSc/w7w9joCkrrN4OAgY2NjAIyNjdn6SJIkSZI0p0w1ebQ4My8CxgAycxTYUVtUUhdZvXo1o6OjAIyOjrJq1aoORyRJkiRJUvtMNXl0b0Q8EkiAiHg68MvaopK6yAknnMCCBQsAWLBgAUuXLu1wRJIkSZIktc9Uk0fvAFYDj4mIrwL/Dry5tqikLrJ8+XLmzSsOpXnz5rF8+fIORyRJkiRJUvu0TB5FxDygB3g+8DzgT4CjMvOGmmOTusLixYs58cQTiQhOOukkFi1a1OmQJEmSJElqm/mtFsjMsYj4UGY+E/jWDMQkdZ3ly5ezceNGWx1JkiRJkuacqXZb+0JEvLzWSKQutnjxYi688EJbHUmSJEmS5pyWLY9KpwOPiIj7gfuAADIzH1lbZJIkSZIkSeq4qbY8OghYAOwPLCrnWzaxiIjjI+K2iNgYEWc0ef3UiNgSETeU//5wOsFLkiRJkiSpXlNqeZSZOyLipcBvlkVXZebnJ3tPRPQA5wEvBDYB6yNiVWbeOmHRT2Xm6dOMW5IkSZIkSTNgSi2PImIF8GfA7eW/P4uId7d42zHAxsy8PTMfAC4GHDdJkiRJkiSpi0y129oJwAsy8/zMPB94EbC0xXsOBu5omN9Ulk10UkTcGBGXRsShzSqKiNMiYigihrZs2TLFkCVJkiRJkrSnppo8AjiwYfqAKSwfTcpywvxqYElmPhn4IvBvzSoqk1YDmTng06wkSZIkSZJmzlSftvY+4LqI+BJFUuhY4KwW79kENLYkOgS4s3GBzPxZw+xHgb+fYjySJEmSJEmaAVMdMPvCiPgK8AyK5NFZmfnDFm9bDxwREYcDPwROBl7buEBEPDozf1TOLgU2TCd4SZIkSZIk1WtKyaOIWAp8NTMvL+cXRsRvZ+YVVe/JzO0RcTqwFugBPp6Zt0TEOcBQZq4C3lLWvR34OXDqnv05kiRJkiRJaqfInDgMUZOFIm7IzKMnlF2fmU+pLbIKAwMDOTQ0NNMfK0mSJEmSNGdFxLWZOdDstakOmN1suamOlyRJkiRJkqQuNdXk0XUR8b6I6IuIwyLiXOD6OgOTJEmSJElS5001eXR6uexngdVl2fJaIpIkSZIkSdKsMdWnrd0DvAMgIg7IzF/WGpUkSZIkSZJmhUlbHkXEmRHRX07vExFXAndExE8i4rdmJEJJkiRJkiR1TKtua68Fbiunfw94OHAQ8FvAe2qMS5IkSZIkSbNAq+TRA5mZ5fTxwEWZuT0zbwEW1BuaJEmSJEmSOq1V8uj+iDgyIh5F0droyobXeusLS5IkSZIkSbNBqwGz3w6souiq9qHMvB0gIl4K3FhzbJIkSZIkSeqwSZNHmfl14Igm5WuANXUFJUmSJEmSpNmhVbc1SZIkSZIk7cVMHkmSJEmSJKnSlJJHEfGQ7m3NyiRJkiRJkjS3TLXl0TenWCZJkiRJkqQ5ZNLWQxGxGHg0sG9EPAmI8qUDgd6aY5MkSZIkSVKHtep69jLgD4BDgPPYmTz6JfDXNcYlSZIkSZKkWWDS5FFmfgL4RES8KjMvmaGYJEmSJEmSNEtMdcyjxRFxIEBEfCQivhkRL6gxLkmSJEmSJM0CU00enZaZWyPiRRRd2N4EvK++sCRJkiRJkjQbTDV5lOX/LwE+kZnXTuO9kiRJkiRJ6lJTTQB9KyLWACcAn4uI/dmZUJIkSZIkSdIc1eppa+N+H3gasDEzt0XEQcAb6gtLkiRJkiRJs8GUWh5l5g7gMRRjHQHsO9X3SpIkSZIkqXtNKQEUER8Gng+cUhbdC3ykrqAkSZIkSZI0O0y129qzMvOpEXE9QGb+PCL2qTEuSZIkSZIkzQJT7Xo2GhHzKAfJjohHAWO1RSVJkiRJkqRZYdLkUUSMt0w6D7gMWBQRfwNcDfx9zbFJkiRJkiSpw1p1W/sm8NTM/GREXAscBwTwysy8ufboJEmSJEmS1FGtkkcxPpGZtwC31BuOJEmSJEmSZpNWyaNFEfGnVS9m5gfaHI8kSZIkSZJmkVbJox5gfxpaIEmSJEmSJGnv0Sp59KPMPGd3K4+I44EPUSShPpaZ761Y7hXAp4GnZ+bQ7n6eJEmSJEmS2mvSp62xBy2OIqKH4iltLwGOAl4TEUc1We4A4C3Af+/uZ0mSJEmSJKkerZJHL9iDuo8BNmbm7Zn5AHAx8PImy/0t8D7gf/bgsyRJkiRJklSDSZNHmfnzPaj7YOCOhvlNZdmDIuIpwKGZecVkFUXEaRExFBFDW7Zs2YOQJEmSJEmSNB2tWh7tiWZd3vLBFyPmAR8E3t6qosw8PzMHMnNg0aJFbQxRkiRJkiRJk6kzebQJOLRh/hDgzob5A4AnAldFxPeBZwKrImKgxpgkSZIkSZI0DXUmj9YDR0TE4RGxD3AysGr8xcy8OzMPyswlmbkEWAcs9WlrkiRJkiRJs0dtyaPM3A6cDqwFNgCXZOYtEXFORCyt63MlSZIkSZLUPvPrrDwz1wBrJpSdVbHssXXGIkmSJEmSpOmrs9uaJEmSJEmSupzJI0mSJEmSJFUyeSRJkiRJkqRKJo8kSZIkSZJUyeSRJEmSJEmSKpk8kiRJkiRJUiWTR5IkSZIkSapk8kiSJEmSJEmVTB5JkiRJkiSpkskjSZIkSZIkVTJ5JEmSJEmSpEomjyRJkiRJklTJ5JEkSZIkSZIqmTySJEmSJElSJZNHkiRJkiRJqmTySJIkSZIkSZVMHkmSJEmSJKmSySNJkiRJkiRVMnkkSZIkSZKkSiaPJEmSJEmSVMnkkSRJkiRJkiqZPJIkSZIkSVIlk0eSJEmSJEmqZPJIkiRJkiRJlUweSZIkSZIkqZLJI0mSJEmSJFUyeSRJkiRJkqRKJo8kSZIkSZJUyeSRJEmSJEmSKpk8kiRJkiRJUiWTR5IkSZIkSapUa/IoIo6PiNsiYmNEnNHk9TdGxE0RcUNEXB0RR9UZjyRJkiRJkqantuRRRPQA5wEvAY4CXtMkOXRRZj4pM48G3gd8oK54JEmSJEmSNH11tjw6BtiYmbdn5gPAxcDLGxfIzK0Ns/sBWWM8kiRJkiRJmqb5NdZ9MHBHw/wm4BkTF4qIPwb+FNgH+K1mFUXEacBpAIcddljbA5UkSZIkSVJzdbY8iiZlD2lZlJnnZeZjgT8H/qpZRZl5fmYOZObAokWL2hymJEmSJEmSqtSZPNoEHNowfwhw5yTLXwz8To3xSJIkSZIkaZrqTB6tB46IiMMjYh/gZGBV4wIRcUTD7MuA79QYjyRJkiRJkqaptjGPMnN7RJwOrAV6gI9n5i0RcQ4wlJmrgNMj4jhgFPgF8Pq64pEkSZIkSdL01TlgNpm5BlgzoeyshmZlco8AABqQSURBVOk/qfPzJUmSJEmStGfq7LYmSZIkSZKkLmfySJIkSZIkSZVMHkmSJEmSJKmSySNJkiRJkiRVMnkkSZIkSZKkSiaPJEmSJEmSVMnkkSRJkiRJkiqZPJIkSZIkSVIlk0eSJEmSJEmqZPJIkiRJkiRJlUweSZIkSZIkqZLJI0mSJEmSJFUyeSRJkiRJkqRKJo8kSZIkSZJUyeSRJEmSJEmSKpk8kiRJkiRJUiWTR5IkSZIkSapk8kiSJEmSJEmVTB5JkiRJkiSpkskjSZIkSZIkVTJ5JEmSJEmSpEomjyRJkiRJklTJ5JEkSZIkSZIqmTySJEmSJElSJZNHkiRJkiRJqmTySJIkSZIkSZVMHkmSJEmSJKmSySNJkiRJkiRVMnkkSZIkSZKkSiaPJEmSJEmSVMnkkSRJkiRJkirVmjyKiOMj4raI2BgRZzR5/U8j4taIuDEivhQRfXXGI0mSJEmSpOmpLXkUET3AecBLgKOA10TEURMWux4YyMwnA5cC76srHkmSJEmSJE1fnS2PjgE2ZubtmfkAcDHw8sYFMvMrmbmtnF0HHFJjPJIkSZIkSZqmOpNHBwN3NMxvKsuqvAH4XLMXIuK0iBiKiKEtW7a0MURJkiRJkiRNps7kUTQpy6YLRpwCDADnNns9M8/PzIHMHFi0aFEbQ5QkSZIkSdJk5tdY9ybg0Ib5Q4A7Jy4UEccBZwLPy8z7a4xHkiRJkiRJ01Rny6P1wBERcXhE7AOcDKxqXCAingL8C7A0MzfXGIskSZIkSZJ2Q23Jo8zcDpwOrAU2AJdk5i0RcU5ELC0XOxfYH/h0RNwQEasqqpMkSZIkSVIH1NltjcxcA6yZUHZWw/RxdX6+JEmSJEmS9kyd3dYkSZIkSZLU5UweSZIkSZIkqZLJI0mSJEmSJFUyeSRJkiRJkqRKJo8kSZIkSZJUyeSRJEmSJEmSKpk8kiRJkiRJUiWTR5IkSZIkzYDNmzdzyimnsGXLlk6HIk2LySNJkiRJkmbA4OAgQ0NDDA4OdjoUaVpMHkmSJEmSVLPNmzdz+eWXk5lcdtlltj5SVzF5JEmSJElSzQYHBxkbGwNgbGzM1kfqKiaPJEmSJEmq2erVqxkdHQVgdHSUVatWdTgiaepMHkmSJEmSVLMTTjiBBQsWALBgwQKWLl3a4YikqTN5JEmSJElSzZYvX868ecVP8Hnz5rF8+fIORyRNnckjSZIkSZJqtnjxYk488UQigpNOOolFixZ1OiRpyuZ3OgBJkiRJkvYGy5cvZ+PGjbY6UtcxeSRJkiRJ0gxYvHgxF154YafDkKbNbmuSJEmSJEmqZPJIkiRJkiRJlUweSZIkSZIkqZLJI0mSJEmSJFUyeSRJkiRJkqRKJo8kSZIkSZJUyeSRJEmSJEmSKpk8kiRJkiRJUiWTR5IkSZIkSaoUmdnpGKYlIrYAI52Oo0YHAT/tdBDaLW677ub2625uv+7ltutubr/u5bbrbm6/7ub2615zfdv1ZeaiZi90XfJorouIocwc6HQcmj63XXdz+3U3t1/3ctt1N7df93LbdTe3X3dz+3WvvXnb2W1NkiRJkiRJlUweSZIkSZIkqZLJo9nn/E4HoN3mtutubr/u5vbrXm677ub2615uu+7m9utubr/utdduO8c8kiRJkiRJUiVbHkmSJEmSJKnSnEweRcQ9bajj1yLi0kleXxgRy6e6fJP3XxAR34uIGyLiWxHxgj2NuZ0i4o0R8XudjmOqImJHuS5vKdfnn0bEbu3fEXFORBw3yet7vG4i4kllvDdExM8b9oUv7km93aJhe90cEasjYmGb6l0SETe3o64J9Z4dET9s2GbvbfdnNHzW0RHx0rrqr0vDNv1WRFwXEc9qc/1/uRvvOTUiPjyh7PcbtuMDEXFT3dt0T0TEr0bERRFxe0RcGxHXRMTv7mGdZ0fEO8rpSc93Leqp3Fcj4tiIuLtctzdGxBcjYvGexD2h/iUR8dqG+YGI+Mc21Htmw/6xo2H6LXta92wQEYeW3zePLOd/pZzvi4gjIuKKiPhuua99JSJ+s1zu1IjY0vA9e2lE9LYxrq487820iPhfEXFxuY1ujYg1EfH4imUnXqcuiYj7Gs7T34iIJ7Qxtl0+T7uv8XdMRLw0Ir4TEYeV5+5tjefSCctmRLy/Yf4dEXH2jAUu4MHvkVvK774bIuJzEfGeCcscHREbyunvR8R/TXj9hjquZ7tNp/bpiLgqIh7yNLWyfKhhfiAirmpR1y7XK22MsZbfPK3MyeRRO2TmnZn5ikkWWQgsn8byzbwzM48G3gp8ZDfCfIiImN+OejLzI5n5yXbUNUPuy8yjM/PXgRcCLwXetTsVZeZZmVmZxGnHusnMm8p4jwZWUe4LmbnLj7h2bc9ZaHx7PRH4OfDHnQ5oCj44vs0y84ypvikieqb5OUdT7L/dZnyb/m/gL4D3tHrDNE07edRMZn6i4di7E3h+s206G469iAjgM8DXMvMxmfk04GTgkCbL7la8rc53LbTaV/+rXLdPBtbT3uN8CfDgxVhmDmXmHid4MnNFw/5xX8Mxv0tiajbsH7sjM+8A/hkYT5a+l2Lshp8A/wmcn5mPLfe1NwOPaXj7pxq+Zx8AXt3G0Lr1vDdjyvPBfwBXldvoKIrz4q9WvGWX69TSdxvO0/9Gm86rk3ye9kAUN5b/CTg+M39QFv8UeHvFW+4HToyIg2YiPj1URPwG8NvAU8vvvuMozrMTz5cnAxc1zB8QEYeWdRw5E7F2iVr26Sjsbh5kcUS8ZBrLL6HheqUdduO3RdvsNcmj8q7al8os8Jci4rCy/LERsS4i1pd3YO8pyx/M5kXEr0fENxvuoB5BcSJ4bFl27oTleyLiH6K4o31jRLy5RXjXAAc3xPq0iPhqeedvbUQ8uix/elnfNeVnjn/eqRHx6YhYDVxZlr2z/JtujIi/Kcv2i4j/LO843RwRry7L31vewboxIv6hLGu8M310uY5ujIj/iIhfKcuvioi/L9fNtyPiuW3YVHssMzcDpwGnlyeHnnJ9ja+P/zO+bET8WbmdvhVly4MoWoW9opye8XUTEcdFcZf+YuD6suz1Dfvg4PgJLyJeUu4P10XEpyJiv7asxJn14P4fEfuXx+d15XZ5eVm+JCI2RMRHo7ibc2VE7Fu+9rRy+11Dw4/TiHh4RHyirOf6iHh+WX5qRHwmihZP34uI06NoqXZ9uS0fOdXAI+IF5ftuioiPR8TDyvLvR8RZEXE18MoozjOfL4/p/4qI/nK5V5bH4rci4msRsQ9wDvDqclu388fZTDoQ+AU8+AV9bvl33tRw3qkqf3S5LsZbpj23PDb3Lcv+X7ncKQ3HxL9E+UUaRcuib0fEV4FnTyfoiHh3WdcXgE9ExPyI+ED5OTdGxB82LHtGQ/lZ7VhpTfwW8EBmPnhzITNHMvOfyhh2OfdXHT/lsmdGxG1RtG58QkN54/mu6rvnIeez6eyrERHAAezcJx5ZHoM3lsfck1uUPy92tgC6PiIOoPgOfm5Z9rYoWjpdUS5/dnk8XhVFi623NMTy1xExHBFfiIh/j/JcPhURcWFEvD8ivgL8Xbm+LyjXy/URcUK5XOV+M0t8EHhmRLwVeA7wfuB1wDWZuWp8ocy8OTMvmPjmKBJn+7Fze1ZdX1WVz9XzXt2eD4xOOB/cAFxfcdzvcp3apL7G83TV92VVecvr4vpWw94hiuvGjwIvy8zvNrz0cYpjpdm1ynaKZPDbZiBENfdo4KeZeT9AZv40M78K3BURz2hY7lXAxQ3zl7AzwfQa4N9nItguULlPR8SiiLgsit936yPi2WX5g7/Tyvmbo/gdMf5bYhC4Djg0Iv45Ioai+G3xN1OM6Vzgr5rEU/V7c+L1ypqG65vro7yGjIi/jYg/jEKz6+Njo2gRfBFw04TPfkxZ19On+Dfsvsycc/+Ae5qUrQZeX07/AfCZcvoK4DXl9BvH30uRJby5nP4n4HXl9D7Avo2vN1n+TcBlwPxy/pFN4rkAeEU5/TvAReX0AuAbwKJy/tXAx8vpm4FnldPvbfi8U4FN458DvIjiQAuKBOEVwG8CJwEfbYjhEcAjgdvgwcHTF5b/nw28o5y+EXheOX0O8H/L6auA95fTLwW+OMu2+S8o7sidBvxVWfYwYAg4HHhJua57G7fT+LaZqXXTuC+U88cB9wCHlfNPpGh9ML4/nU+RwV4MfLUh/jOBv+z08Ted7QX0AJ+muKsGMB84sJw+CNhY7sdLKL5Aji5fuwQ4pck2OLfhuHg78Ilyuh/4AfBwiuNlI8WP2UXA3cAby+U+CLy1SbxnAz8Ebij/vbis6w7g8eUynxx/L/B94M8a3v8l4Ihy+hnAl8vpm4CDJ+xfpwIf7vQ22o1tuqNcN8PlOn1aWX4S8IVyW/9quR0ePUn524EzG/aPAxr3mXL6SIpz+oJyfhD4vfL9Pyi36z7A1ydbl+V2Oqhh/t3AN4GHl/PLgTPK6YdRJHMPozimB9l5jv085bm5zev0LRQt3qpeP5Vdz/1Vx8/Tyn2tl+IH40Z2nsMuoDjfTfbdcxVNzmeT7avAseV+cAPFcTLcENs/Ae8qp38LuKFF+Wrg2eX0/uXfeSxwxYTPu6LheP1Guc0OAn5W/n0DZTz7Uhz/3xlfDxV/wz0T5i+kOBfPK+ffB5xcTv8K8G2K80LT/abTx+iEv+XFQAIvLOc/APxJi31tS7n+fgL8F9DTsH1eX043Xl9Vlc+Z894Mb7Om5wMm/96ceJ16X7kNvwv8iJ3XGVXfl1XlLa+L/bdH23qUolX2kyeUnw28AzgL+JuyrPG78R6Kc/z3Ka7x3wGc3em/Z2/6R/EddUP5fTDIzuvTd44fv8AzgfUN7/k+8HjgG+X89cBRHk+T79MULbeeU04fBmwop8+m4bud4vfzkvLfGPDMhtfGr596KK51nlzOXwUMNInnKopriS9TJPQHKFqDQvXvzWPZ9XrlDIqb3QdStMpeW5Z/heLmXtX18bHAvcDh5fJLyr/tCeU+c/RMbJO9puUR8BvsbB64kuJu23j5p8vpiya+qXQN8JcR8edAX2be1+KzjgM+kpnbATLz5xXLnRsRt1NckP5dWfYEimTBFyLiBorM5iFRjAlzQGZ+oyLWLzR8zovKf9dTZFb7gSMoLtiOi+IO8nMz825gK/A/wMci4kRgW2OlEfEIiou7r5ZF/0aRiBp3efn/tRQ78WwS5f8vAn6vXJ//DTyKYn0cR3FRtA2abqdOrptrcmcT5eOApwND5d/wPOCxwLMovly+UZa/bjc+p1P2LWP+GUWS7gtleVDc0b8R+CJFi6TxJvnfy+IuK5TrtMk2WNnwGc8Zn8/MYWCE4ssZ4CuZ+cvM3ELxA3d1WX4T1euwsdvaWopj9XuZ+e3y9Ynb/1NQtKai2FafLv/mf6H4EoAiuXFBRPwRxZdENxvv4tMPHA98MiKCYjv8e2buyMyfUCQ8nz5J+Xrg96Po0/6kzPxlk896AUVCZH25Tl9A0b3mGRRf4lsy8wHKbTBNn83M/ymnX1TGMn7uWEhx7ngRRfJ5/Bz7OHbuW7WJiPPK1hrrG4obz/1Vx89zgf/IzG2ZuZWiq+xETb97Gl7fnfPZeLe1Q4FPUCRbYNdj88vAo8pjuar868AHomhBtHD8u7WF/8zM+zPzp8BmivXwHIrte1+5X62etIbmPp2ZY+X0i4Azy/X1FYof1YdRvd/MJi+hSB48sdmLUbSkvTkiLm8o/lQW3fn+F8W58p1l+WTXV83K59J5bzaY7HtzovFua4+lGDJh/HHTVd+XVeXTvS7W9IxSJMDfUPH6PwKvj4gDJ75QnuM/SZFs1AzLzHsork9Oo0i4fyoiTqVoZfSKKHoOnMxDWxb9HPhFRJwMbGDCb4692ST79HHAh8vv2lXAgVG0TJ7MSGaua5h/VURcR3E99+sUv6um4t08tPVR1e/Nif6L4vfCcyi6i+8fxRiCSzLzNqqvjwG+mZnfa6hrEfBZihvqNzADurLPfpvklBfMvCgi/ht4GbA2iibot0/ylphi/e+kuCB/C8UPz6eV770lM39jlwrL7lCTuHfC578nM//lIYFFPI3izvF7IuLKzDwnIo6h+PF1MnA6xR3fqbq//H8Hs2h/iojHUMS0mWJ9vLn8wd+4zPFMsp0yc3sH183E7fnxzPzrxgWiGDT385m5bJp1zwb3ZebR5Q/DKygy8P9IkQBbRNFqZTQivk/xgwx2rk8o1um+TH6sRUX5xLrGGubHmPq2mqx+2LkN5wF3lT+6dpGZbyybMb8MuCEiHrJMN8rMa6Lon76I6vXUtDwzvxbFIL0vA1ZGxLn50DHGAvi3zPyLXQojfodpnNsrTDz2lmfmlyZ8zlLg3Zn5r3v4Wa3cQnEHCoDM/ONyvQ41LNMY72THT6v10vS7p8GenutXUbTIHf+sibKqPDPfGxH/SfHdtS6mNsD3xPPF/Ir6p2vi/vE7uWuXkvFueg/Zb2aL8jzzQoq731dH0UX6FhqS35n5u1EMFvoPE9+fmRlFV8k3s3PspF0WqfjoLN8/J897M+AWilaCE0123E9mFUVSF6Z/np7udbGmZ4yiW9MXI+IvM/PvGl/MzLvKritVY0z9X4obG5+oeF01yswdFC1UroqImyhaYF5QHpvPo/heb/Zd+yngPIqWmNpVs316HvAbE5PXEbGdXYfmaTwf3tuw3OEULZmenpm/iIgLmNq5k8z8ckT8LcX36INV0vz35rET3r6eosXS7RQ3zw8C/oji5tx4PVXunTB/N0Xr7mdTfEfUbm9qefQNigQAFF+0V5fT69h5cX7yxDfBg4mI27MYMHMV8GTglxTN3pu5EnhjlANqxiRjqJR3MD8EzIuIF1N0k1oUxYBrRMSCiPj1zPwF8MuIGN9Jm8ZaWgv8QdnigYg4OCIWR8SvAdsy80KKC8Knlss8IjPXUNyF2uUirmyd9IvYOWbPMooM6KwVEYsoBiD/cGYmxfp4U0QsKF9/fBRjA11JsZ56y/JHTqhntqybL1Jkxg8q43pUFGNHfAN4Xrl/jo9pNdvubk+qXIdvAd5Rbp9HAJvLC+DnA30t3n8XcHdEjN/Rfl3Dy18bn4/iaTSHURxf7TJM0frpceV80+1f3jH5XkS8sowlIuJ/l9OPzcz/zsyzKAbBPJTJzy1dIYoxnXooWpZ9jWJ8hp7y2PxNiq5hTcsjoo9iH/go8K/AU8tqR8ePYYpugK+I8okzUYyV00dxp+fY8hhZALxyD/+UtcDyhnP5E6IYa2st8IbyPEJEHBL1DFD6ZeDhEfGmhrLJnnBVdfx8DfjdiNi3vCt3QpP3Nv3uaRHfdPbV51B0lRmPZ/zYPJZifIitVeXlcXJTZv49ReKsf5qfPe5q4IQoxnHZn+KH755YS8Od0Ih4SkN5s/2m48rE1j9TdLH9AUVX33+gaCH07DIxOm6yfa1xe1ZdXzUtn6vnvRnwZeBhUbTYAiCK8S3Gz5kTj/tW67TqmGz8vmxavhvXxZqmslX8bwOvi4hmLZA+APwfmiTzy9aol1Ddckk1Kc/3jdfiR1O02IOitdEHKVoAbmry9v+gaKG7tslre7WKffpKihv7wIM3RqDo4vbUsuypFF3HmjmQIhlzd0T8KkWL3OlYAfxZw3zV781dzo1ly/g7KBLE6yhaIr2j/B+qr5ubeYBi+Jvfixqe6NbMrGkp0ma9EdF4UH6A4gLv4xHxTopmhL9fvvZW4MKIeDtF07G7m9T3auCUiBgFfgyck5k/j4ivRzFo9ecoMsXjPkbRrPfG8j0fBT48sdJx5V28d1OMkbI2isFL/zGKVhnzKbKtt1AcMB+NiHspMtrNYiUzr4xipP5riutE7gFOoehacW5EjFE0iX0Txc782Yh4OEWms9kge68HPlImWW5vWHezyXg3qAUUY+OspNjuUGyPJcB15YXzFoq7xZ8vTzRDEfEAsIZdnzwyK9ZNZt4UxSBuX4yiuesoxRg968sLik9FMeAoZfzfaXcM/7+9+3eJOo7jOP56i6P/QHME0Rq0NbUEBdZU4KAOSRBUS2bQUFM0lFJk1FRDUDREBQ1BBDYkBGdn0BidU5RBYFqJ9m54f7556n1OT03Nez7GzyH39e7z4/393Of9/v5L7j5iZmXFTcY9SU8tHoNZ1M9ZSrdibE9p/oI7qPhu3in6RJe7/0pjYi2u+6eZdSvS0VoVvyTknprYIemmmZ1X9NH7ksqK8bhD0b9epLYxSX2pP19y95WkXm2EYgxK8f90uvusmT1S/MJWVpw86HX3T3XaOyWdSXPnd0UtIynSK0bNrOTuHemzfF41Jk64+7BFuttrRUpOSatLi7mluFl6m/rNZ0nt7v4sbZANp/YJRR2y8VW81yJpbTgkqd/MehVz16Sks5k/qTl+3L1kZg9SW0VzAUr1e03XWXtyXqp+X92bXjPFelUUjr6gKEg+qjia37lE++l0Uzwr6b1izf0taSbNHXeUHi5QT5oznyj6XEWxEVVzHV2mi5IG0hzToqg1065Mv1nF+6ylY5LG3L1IFR5U/Mq9R3GjetXMBhR1jSYUR/MLRyw26lsUtba6Unsuvsq1b6V5b92k+eCwos/1KdLqPyrGzbUa4/5rjTh1e9WYnNbcmMytl7n2JeNidy/SGrFC6TPdL2nIzMYXvDae1tFccewrqrqxxrppk3TdotzIjGJd6EmvPVQcGKj5IKWUTn1ZktYqVt1iFvbpk5JupJihVbHpclxxyrlIH3ujqD+1iLuXzWxEEed8UKRUL1uKBb9UNdW831TUZv0br7h7vyIO2+fuU2b2SlEmoIjNcvHxzsx1TJrZQUXZgUl3f9zI/9GoohBw00o3/T/SonxUUTx7swR585hZW8qlVQoctrn7qQ2+LAAA/gvFOprW/iFJPe5e2ujrAgAA2Oy26smjRuxWFNsySd8UTwTZrA6Y2TnF91YRObEAADTitpntUtQ1uMvGEQAAwPI0/ckjAAAAAAAA5DVTwWwAAAAAAAA0iM0jAAAAAAAAZLF5BAAAAAAAgCw2jwAAAAAAAJDF5hEAAAAAAACy2DwCAAAAAABA1h+Uc+6Q0Glp1AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1440x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(20,6))\n",
    "ax = sns.boxplot(x='Model',y='Test', data=df, color='navy')\n",
    "\n",
    "# Select which box you want to change    \n",
    "mybox = ax.artists[9]\n",
    "\n",
    "# Change the appearance of that box\n",
    "mybox.set_facecolor('red')\n",
    "# mybox.set_edgecolor('black')\n",
    "# mybox.set_linewidth(3)\n",
    "plt.xlabel('')\n",
    "plt.ylabel('Test Score (FDR3%)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-05T04:14:07.740705Z",
     "start_time": "2022-05-05T04:14:07.048115Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(20,6))\n",
    "ax = sns.boxplot(x='Model',y='OOT', data=df, color='navy')\n",
    "\n",
    "# Select which box you want to change    \n",
    "mybox = ax.artists[9]\n",
    "\n",
    "# Change the appearance of that box\n",
    "mybox.set_facecolor('red')\n",
    "# mybox.set_edgecolor('black')\n",
    "# mybox.set_linewidth(3)\n",
    "plt.xlabel('')\n",
    "plt.ylabel('OOT Score (FDR3%)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-06T02:00:06.151447Z",
     "start_time": "2022-05-06T02:00:04.778546Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(20,6))\n",
    "# ax = sns.boxplot(x='Model',y='OOT', data=df, color='navy')\n",
    "ax = sns.boxplot(x='Model',y='Value',hue='Type', data=df_compare, palette=['navy','r'])\n",
    "# Select which box you want to change    \n",
    "mybox = ax.artists[9]\n",
    "\n",
    "# Change the appearance of that box\n",
    "# mybox.set_facecolor('red')\n",
    "# mybox.set_edgecolor('black')\n",
    "# mybox.set_linewidth(3)\n",
    "# plxlabelabel('')\n",
    "plt.ylabel('Score (FDR3%)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
